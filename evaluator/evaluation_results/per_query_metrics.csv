query,split,raw_gold-text,num_gold_texts,num_retrieved,retrieved_docs,text_recall,text_precision,text_f1,num_gold_texts_matched,ctx_precision,ctx_recall,ctx_f1,chunk_overlap,min_chunk_overlap,max_chunk_overlap,chunk_overlap_count,bleu,text_supported@1,text_recall@1,text_supported@3,text_recall@3
Explain the importance of ImageNet in the works alexnet and googlenet.,multi,"('We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 dif- ferent classes.', 'We propose a deep convolutional neural network architecture codenamed Incep- tion, which was responsible for setting the new state of the art for classiï¬cation and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC14).')",2,7,"['ference. Compared to the widely used ResNet-50 (He et al.,\n2016), our Efï¬cientNet-B4 improves the top-1 accuracy\nfrom 76.3% to 83.0% (+6.7%) with similar FLOPS. Besides\nImageNet, Efï¬cientNets also transfer well and achieve state-of-the-art accuracy on 5 out of 8 widely used datasets, while\nreducing parameters by up to 21x than existing ConvNets.\n2. Related Work\nConvNet Accuracy: Since AlexNet (Krizhevsky et al.,\n2012) won the 2012 ImageNet competition, ConvNets have', 'ImageNet only in the choice of data augmentation, the use of\na nonlinear head at the end of the network, and the loss func-\ntion. The strength of this simple framework suggests that,\ndespite a recent surge in interest, self-supervised learning\nremains undervalued.\nAcknowledgements\nWe would like to thank Xiaohua Zhai, Rafael MÃ¼ller and\nYani Ioannou for their feedback on the draft. We are also\ngrateful for general support from Google Research teams in\nToronto and elsewhere.\nReferences', 'composition of two transformations (applied sequentially). The\nlast column reï¬‚ects the average over the row.\nTo understand the effects of individual data augmentations\nand the importance of augmentation composition, we in-\nvestigate the performance of our framework when applying\naugmentations individually or in pairs. Since ImageNet\nimages are of different sizes, we always apply crop and re-\nsize images (Krizhevsky et al., 2012; Szegedy et al., 2015),\nwhich makes it difï¬cult to study other augmentations in', 'as belonging to the class indicated by the image label.\nE ImageNet\nImageNet is a dataset of millions of labeled images in thousands of categories. The images\nwere collected from the web and labelled by human labellers using Amazonâ€™s Mechanical Turk\ncrowd-sourcing tool. In 2010, a subset of roughly 1000 images in each of 1000 classes was the\nbasis of an object recognition competition, a part of the Pascal Visual Object Challenge. This', 'keeping the computational budget constant. To optimize quality, the architectural\ndecisions were based on the Hebbian principle and the intuition of multi-scale\nprocessing. One particular incarnation used in our submission for ILSVRC14 is\ncalled GoogLeNet, a 22 layers deep network, the quality of which is assessed in\nthe context of classiï¬cation and detection.\n1 Introduction\nIn the last three years, mainly due to the advances of deep learning, more concretely convolutional', 'ity. An ensemble of these residual nets achieves 3.57% error\non the ImageNet testset. This result won the 1st place on the\nILSVRC 2015 classiï¬cation task. We also present analysis\non CIFAR-10 with 100 and 1000 layers.\nThe depth of representations is of central importance\nfor many visual recognition tasks. Solely due to our ex-\ntremely deep representations, we obtain a 28% relative im-\nprovement on the COCO object detection dataset. Deep\nresidual nets are foundations of our submissions to ILSVRC', 'and there are 1000 categories instead of ten. Another difference is that the ImageNet images\noften contain multiple instances of ImageNet objects, simply due to the sheer number of object\nclasses. For this reason, even a human would have difï¬culty approaching perfect accuracy on\nthis dataset. For our experiments we resized all images to 256\x02256pixels.\nF Convolutional Neural Networks\nOur models for CIFAR-10 and ImageNet are deep, feed-forward convolutional neural networks', 'Krizhevsky et al., 2012) and machine translation (Sutskever et al., 2014; Bahdanau et al., 2015; Wu\net al., 2016). Along with this success is a paradigm shift from feature designing to architecture\ndesigning, i.e., from SIFT (Lowe, 1999), and HOG (Dalal & Triggs, 2005), to AlexNet (Krizhevsky\net al., 2012), VGGNet (Simonyan & Zisserman, 2014), GoogleNet (Szegedy et al., 2015), and\nResNet (He et al., 2016a). Although it has become easier, designing architectures still requires a', 'â€œmax-poolingâ€ layers that report the maximum activity in local pools of convolutional units.\nThese six layers were followed by one locally-connected layer (For details see Appendix D) .\nUsing dropout in the last hidden layer gives an error rate of 15.6%.\nImageNet is an extremely challenging object recognition dataset consisting of thousands of\nhigh-resolution images of thousands of classes of object ( 11). In 2010, a subset of 1000 classes', 'is not among the ï¬ve labels considered most probable by the model.\nImageNet consists of variable-resolution images, while our system requires a constant input dimen-\nsionality. Therefore, we down-sampled the images to a ï¬xed resolution of 256\x02256. Given a\nrectangular image, we ï¬rst rescaled the image such that the shorter side was of length 256, and then\ncropped out the central 256\x02256patch from the resulting image. We did not pre-process the images']",1.0,0.0,0.0,2,0.020833333333333332,0.5,0.039999999999999994,0.18130481280618665,0.07392197125256673,0.25366876310272535,10,3.632520053801081e-05,0.0,0.0,0.0,0.0
How does auto-differentiation work in these frameworks?,single,"TensorFlow is an interface for expressing machine learning algorithms and an implementation for executing such algorithms â€¢PyTorch  is a programming framework for tensor computation, deep learning, and auto differentiation 6Deep Learning Programming Framework7Aspect PyTorch TensorFlow JAX NumPy Primary Use Deep learning Deep learning numerical and ML computing numerical computing Programming Paradigm Dynamic (eager execution)Static (Graph mode, or Eager)Functional transformations Procedural Auto grad dynamic comp graphstatic comp graph Functional -based with grad/jit",1,4,"['torch.func  has auto-differentiation transforms (grad(f) returns a function that computes\nthe gradient of f), a vectorization/batching transform (vmap(f) returns a function that\ncomputes f over batches of inputs), and others.\nThese function transforms can compose with each other arbitrarily. For example, composing\nvmap(grad(f))  computes a quantity called per-sample-gradients that stock PyTorch cannot\nefficiently compute today.\nWhy composable function transforms?', 'Joel Andersson. A general-purpose software framework for dynamic optimization . PhD thesis, 2013.\nJoel A E Andersson, Joris Gillis, Greg Horn, James B Rawlings, and Moritz Diehl. CasADi â€“ A\nsoftware framework for nonlinear optimization and optimal control. Mathematical Programming\nComputation , In Press, 2018.\nAtilim Gunes Baydin, Barak A Pearlmutter, Alexey Andreyevich Radul, and Jeffrey Mark Siskind.\nAutomatic differentiation in machine learning: a survey. Journal of machine learning research , 18', '11868 LLM Systems\nAuto Differentiation\nLei Li\nâ€¢GPU is composed of \nostreaming processing units (SMs)\nâ–ªeach with four partitions of 32 cores\nâ–ªshared L1 cache \nomemory\noL2 cache: share with all SMs\nâ€¢Threads organized in\nogrid of thread blocks\noeach block is divided into warps \nrunning on one SM.\n2Recap\nGrid GPU\nWarp\n1Thread Block\nWarp\n1Warp\n2\nWarp\n3Warp\n4SM\npartition1â€¢Basic CUDA operations\nomemory allocation\nodata movement\nocreating threads and running on SMs', 'The results above extend those of Stapor et al. (2018, section 2.4.2). An extended version of\nAlgorithm 1 including derivatives w.r.t. t0andt1can be found in Appendix C. Detailed derivations\nare provided in Appendix B. Appendix D provides Python code which computes all derivatives for\nscipy.integrate.odeint by extending the autograd automatic differentiation package. This\ncode also supports all higher-order derivatives. We have since released a PyTorch (Paszke et al.,', 'Automatic differentiation in machine learning: a survey. Journal of machine learning research , 18\n(153):1â€“153, 2018.\nRianne van den Berg, Leonard Hasenclever, Jakub M Tomczak, and Max Welling. Sylvester\nnormalizing ï¬‚ows for variational inference. arXiv preprint arXiv:1803.05649 , 2018.\nBob Carpenter, Matthew D Hoffman, Marcus Brubaker, Daniel Lee, Peter Li, and Michael Betan-\ncourt. The Stan math library: Reverse-mode automatic differentiation in c++. arXiv preprint\narXiv:1509.07164 , 2015.', 'Differentiation (recall previous lecture)\n17Gradient Computation\ntrain_step  = tf.train.GradientDescentOptimizer (0.5).minimize( cross_entropy )â€¢How to design a deep learning framework\noDesign ideas in TensorFlow\nâ–ªAbadi et al., â€œTensorFlow: A System for Large -Scale Machine Learningâ€, \nOSDI 2016\noBasic Graph node types in Tensorflow /Pytorch\noOverall design principles\nâ€¢Hands -on practice to implement a mini -tensorflow\nâ€¢Execution in Tensorflow\n18Todayâ€™s Topic\nâ€¢All nodes return tensors', 'coverage over PyTorch operations.\nIf you have suggestions on the API or use-cases youÊ¼d like to be covered, please open a\nGitHub issue or reach out. WeÊ¼d love to hear about how youÊ¼re using the library.\nWhat are composable function transforms?\nA â€œfunction transformâ€ is a higher-order function that accepts a numerical function and returns\na new function that computes a different quantity.\ntorch.func  has auto-differentiation transforms (grad(f) returns a function that computes', 'Yiping Lu, Aoxiao Zhong, Quanzheng Li, and Bin Dong. Beyond ï¬nite layer neural networks:\nBridging deep architectures and numerical differential equations. arXiv preprint arXiv:1710.10121 ,\n2017.\nDougal Maclaurin, David Duvenaud, and Ryan P Adams. Autograd: Reverse-mode differentiation of\nnative Python. In ICML workshop on Automatic Machine Learning , 2015.\nHongyuan Mei and Jason M Eisner. The neural Hawkes process: A neurally self-modulating', 'through the integrator difï¬cult. We implement the adjoint sensitivity method in Pythonâ€™s autograd\nframework (Maclaurin et al., 2015). For the experiments in this section, we evaluated the hidden\nstate dynamics and their derivatives on the GPU using Tensorï¬‚ow, which were then called from the\nFortran ODE solvers, which were called from Python autograd code.\nTable 1: Performance on MNIST.yFrom LeCun\net al. (1998).\nTest Error # Params Memory Time\n1-Layer MLPy1.60% 0.24 M - -\nResNet 0.41% 0.60 M O(L)O(L)', 'oparents: the parent Nodes\nâ€¢More details in next lecture\n18Implementation\nhttps://github.com/mattjj/autodidact  â€¢Autogradâ€™s  NumPy module provides primitive ops which \nlook and feel like NumPy functions, but secretly build the \ncomputation graph.\n19Wrapper around Numpy\nâ€¢Learning algorithm for Neural Network\nâ€¢Computation Graph\nâ€¢Auto Differentiation\noconstructing the computation graph for calculating gradients\n20Today â€™s Topic\nâ€¢To learn a neural network, we need gradient of loss function \nw.r.t.  parameters. ']",0.0,0.0,0.0,0,0.12727272727272726,1.0,0.22580645161290322,0.23346147890241414,0.10119047619047619,0.30357142857142855,10,1.7055413557580346e-07,0.0,0.0,0.0,0.0
"What are FlashMLA, DeepEP, and DeepGEMM, and what problems do they each solve?",single,"Accelerating Transformer Layersâ€¢FlashMLA  (released 2/24/2025) FlashMLA  is an efficient MLA decoding kernel for Hopper GPUs, optimized for variable -length sequences serving. DeepEP  (released 2/25/2025) oa communication library tailored for Mixture -of-Experts ( MoE) and expert parallelism (EP). It provides high -throughput and low -latency all -to-all GPU kernels, which are also as known as MoE dispatch and combine. DeepGEMM  (released 2/26/2025) oDeepGEMM  is a library designed for clean and efficient FP8 General Matrix Multiplications (GEMMs) with fine -grained scaling 3Deepseek  opensource libraries",1,5,"['Reinforcement learning integration\nWeight sync, asynchronous algorithms, memory saver\nveRL  Integration ( link ),Areal, LlamaFactory\nLow latency optimizations\nSpeculative decoding (e.g., EAGLE 3 ), kernel optimizations\nThe full roadmap : https://github.com/sgl -project/sglang/issues/4042  \n28A case study of the DeepSeek  system\n29\nLoad balancer + prefill / decode disaggregation + speculative decoding + quantization + ', 'blocks are combined using the statistics to produce the ï¬nal output. FlashAttention instead incrementally\nupdates the output (Algorithm 1 line 12) after processing each block, so only one copy of the output is needed\n(instead ofğ¾copies forğ¾blocks). This means that FlashAttention has smaller total memory requirement\ncompared to Rabe and Staats [66].\nThe ï¬nal major diï¬€erence is the way the backward pass is computed. Rabe and Staats [66]uses gradient', 'https://gptpluginz.com/llm-agents/LLM agents can be utilized as personal assistants to assist users in breaking free from daily tasks and repetitive labor. They can analyze, plan, and solve problems independently, reducing the work pressure on individuals and enhancing task-solving efficiency.Single-agent applications\nhttps://github.com/langchain-ai/langchain', 'Standard attention (Algorithm 0) requires Î˜Â¹ğ‘ğ‘‘Â¸ğ‘2ÂºHBM accesses, while FlashAttention (Algorithm 1)\nrequiresÎ˜Â¹ğ‘2ğ‘‘2ğ‘€\x001ÂºHBM accesses.\nFor typical values of ğ‘‘(64-128) and ğ‘€(around 100KB), ğ‘‘2is many times smaller than ğ‘€, and thus\nFlashAttention requires many times fewer HBM accesses than standard implementation. This leads to\nboth faster execution and lower memory footprint, which we validate in Section 4.3.\nThe main idea of the proof is that given the SRAM size of ğ‘€, we can load blocks of K\x96Vof sizeÎ˜Â¹ğ‘€Âºeach', '(the number of memory reads/writes). As mentioned in Section 2, the amount of memory access is the\nprimary determining factor of runtime. Reducing memory accesses also necessarily reduces the total amount\nof memory required (e.g., if an operation incurs ğ´memory accesses, then its total memory requirement is at\nmostğ´). As a result, FlashAttention is faster than standard attention (2-4 \x02) while Rabe and Staats [66]', 'and sparse attention. We report a subset of baselines in the main body; Appendix E contains more baselines\nand full details.\n4Path-256 requires longer sequences but has relatively shorter paths than Path-X, so it is easier to obtain a higher accuracy.\n9Runtime. Figure 3 (left) reports the runtime in milliseconds of the forward + backward pass of FlashAt-\ntention and block-sparse FlashAttention compared to the baselines in exact, approximate, and sparse', 'A100 GPU. Fewer HBM accesses result in faster runtime, up to a point. Right: The runtime (for seq. length 4K) of\nblock-sparse FlashAttention is faster than FlashAttention by a factor proportional to the sparsity.\nasymptotically improve on HBM accesses over all SRAM sizes. Proofs are in Appendix C.\nTheorem2. Letğ‘be the sequence length, ğ‘‘be the head dimension, and ğ‘€be size of SRAM with ğ‘‘\x14ğ‘€\x14ğ‘ğ‘‘.\nStandard attention (Algorithm 0) requires Î˜Â¹ğ‘ğ‘‘Â¸ğ‘2ÂºHBM accesses, while FlashAttention (Algorithm 1)', 'and prefix-caching scenarios. For kernel evaluation, we com-\npare FLEXINFER against different versions of FlashAttention,\nPagedAttention from vLLM, and triton prefix-prefilling ker-\nnel from SGLang [63], which is adopted by vLLM, too. For\nconvenience, we take PagedAttention as vLLM, FlashAtten-\ntion with paged KV cache as FA_paged, our virtual-memory-\nenabled FlashAttention as FLEXINFER , native FlashAttention\nas FA_native and triton prefix-prefilling kernels as SGLang.', 'ğ‘–Âº\x001Â¹diagÂ¹â„“ğ‘–Âºğ‘’ğ‘šğ‘–\x00ğ‘šnew\nğ‘–Oğ‘–Â¸ğ‘’~ğ‘šğ‘–ğ‘—\x00ğ‘šnew\nğ‘–~Pğ‘–ğ‘—Vğ‘—Âºto HBM.\n13:Writeâ„“ğ‘– â„“new\nğ‘–,ğ‘šğ‘– ğ‘šnew\nğ‘–to HBM.\n14:end for\n15:end for\n16:Return O.\nWe show FlashAttention â€™s correctness, runtime, and memory requirement (proof in Appendix C).\nTheorem 1. Algorithm 1 returns O=softmaxÂ¹QK>ÂºVwithğ‘‚Â¹ğ‘2ğ‘‘ÂºFLOPs and requires ğ‘‚Â¹ğ‘Âºadditional\nmemory beyond inputs and output.\n3.2 Analysis: IO Complexity of FlashAttention\nWe analyze the IO complexity of FlashAttention , showing signiï¬cant reduction in HBM accesses compared', 'tutorial introduction.Computational Optimization and Applications1, 1 (01 Oct\n1992), 7â€“66. doi:10.1007/BF00247653\n[3]Davis Blalock, Jose Javier Gonzalez Ortiz, Jonathan Frankle, and John Guttag.\n2020. What is the state of neural network pruning?Proceedings of machine\nlearning and systems2 (2020), 129â€“146.\n[4]Hongrong Cheng, Miao Zhang, and Javen Qinfeng Shi. 2024. A Survey on Deep\nNeural Network Pruning: Taxonomy, Comparison, Analysis, and Recommen-']",0.0,0.0,0.0,0,0.0,0.0,0.0,0.19858009465402734,0.13924050632911392,0.2675438596491228,10,1.1666853603757757e-07,0.0,0.0,0.0,0.0
What are the challenges of theoretical distributed training (PipeDream) and what is the scaling efficiency of distributed real-world Trainium performance (HLAT)?,multi,"('Challenge 1: Stage Partitioningâ€¢How to partition model layers into the stages evenly? â€¢Throughput depends on the slowest stage in pipeline â€¢Solution: â€¢Proï¬le layer-wise perf and comm perf â€¢Allows a stage to be replicated (DP) â€¢Uses dynamic programming to ï¬nd optimal partition and layer replication', 'Challenge 2: Work Schedulingâ€¢How to schedule forward and backward computation on a worker? â€¢Solution: 1F1B-RR â€¢Run one forward and one backward â€¢Round-robin across replicated stages', 'Challenge 3: Weight Versioningâ€¢How to ensure the same minibatch uses the same weight version across workers for forward and backward? â€¢Otherwise computation will be far oï¬€ and training not able to converge â€¢Solution: Store multiple weight versions so that the backward and forward of the same minibatch â€¢Weights across workers can be diï¬€erent!', 'The scaling efficiency for Llama2 7b:â€¢87% on 32 nodes. MFU = 33.5%â€¢72% on 64 nodes. MFU = 27.9%Â© 2025, Amazon Web Services, Inc. or its affiliates. All rights reserved. Amazon Confidential and Trademark. Observations on training precision for 70B (preliminary)')",4,7,"['(parameter count) are further increased to an incredible 530 billion (Megatron-Turing-NLG Smith\net al. (2022)) and 540 billion (PaLM Chowdhery et al. (2022)), because the scaling law Henighan\net al. (2020) is still working.\nEfï¬cient Distributed Model Training. Scaling model training to tens of or hundreds of billion\nparameters is a complicated task, which requires a lot of algorithmic innovations and engineer-\ning optimization. One of the most critical challenges is that the model cannot ï¬t into one single', 'training large-scale models.\n2.2 Parallelization Strategies\nVarious parallelization strategies have been proposed to im-\nprove the efficiency of distributed training.\nData parallelism replicates the entire model on each GPU,\nand each GPU processes a different portion of the training\ndata. A key advantage is that it can be used with any model\narchitecture without requiring code changes, and the com-\nmunication overhead is relatively low. However, because', 'Background: distributed training on Trainium', 'training framework becomes increasingly imperative for applica-\ntions built on top of PyTorch. This section elucidates the trajectory\nof PyTorchâ€™s distributed training capabilities.\n2.1 Model Replication\nModel replication approaches are designed to tackle high-volume\ndatasets by scaling out and distributing computations across multi-\nple devices. DistributedDataParallel (DDP) [ 14] is the first end-to-end\ndistributed training feature in PyTorch that falls into this category.', 'computation/communication overlap, and memory usage\nlimits. PipeMoE (Shaohuai et al. 2023) utilizes parallelism\nbetween expert computation and AllToAll communication,\nemploying a micro chunk approach for hiding latency, and\npresents the optimal parallel degree for pipelining modeling.\nDeepSpeed is a framework developed by Microsoft specifi-\ncally designed for large-scale distributed training to improve\ntraining efficiency and reduce resource consumption. It', 'shows the training losses for both settings. Although some\noperators are non-deterministic and introduce subtle differ-\nences, the loss curves were closely aligned.\n6 Related Work\n6.1 Distributed Training Framework\nAs discussed in Section 2, various distributed training strate-\ngies have been proposed to scale deep learning models be-\nyond the capacity of a single GPU. For instance, tensor par-\nallelism, as introduced in Megatron-LM [ 25], splits large', 'patterns). These methods assume that workloads remain stable dur-\ning training. Consequently, they fail to handle the pipeline stalls\nintroduced by dynamic models, leading to reduced computational\nefficiency.\nInnovative designs of dynamic models aim to reduce compu-\ntational cost, but without effective load balancing, their benefits\npractically fail to translate into actual performance gains during\ndistributed training [ 4]. To address this gap, we introduce DynMo ,', 'tion, they often introduce significant workload imbalance across\nworkers. In many cases, this imbalance is severe enough to ren-\nder the techniques impractical for large-scale distributed training,\nlimiting their applicability to toy models due to poor efficiency.\nWe propose an autonomous dynamic load balancing solution,\nDynMo , which provably achieves maximum reduction in workload\nimbalance and adaptively equalizes compute loads across work-\ners in pipeline-parallel training. In addition, DynMo dynamically', 'effectively. Conventional distributed training methods include TP, DP, CP and PP. TP divides the\ncomputations of neural network layers across multiple devices, allowing for parallel processing\nof tensors within layers[ 31]. TP can significantly reduce the memory consumption of each model\nrank but introduces some intra-layer communication overhead. DP distributes batches of data\nacross replicas of the model on different devices, aggregating gradients during training[ 35]. Zero', 'Background: distributed training on Trainium\n4â€¢AWS Trainiumâ€¢Trn1.32xlarge contains 16 Trn accelerators, and 32 Neuron Coresâ€¢16GB memory per Neuron Coreâ€¢3040 TFLOPS in FP16/BF1â€¢Cost $21.50 vs. p4d.24xlarge $32.77â€¢Neuron Distributed Training Library (NDTL, also called NeuronX-distributed)â€¢Tensor, pipeline, data, and sequence parallelismâ€¢Zero-1 optimizerâ€¢Multiple training precision configurationsâ€¢Automatic fault recoveryâ€¢â€¦Â© 2025, Amazon Web Services, Inc. or its affiliates. All rights reserved. Amazon Confidential and Trademark.']",0.25,0.2,0.22222222222222224,1,0.06521739130434782,0.23076923076923078,0.10169491525423728,0.27785602990883357,0.18655097613882862,0.7045454545454546,10,0.03674858044083876,0.0,0.0,0.0,0.0
What are the three core components of the TinyServe system?,single,"The system is organized around three core components: (1)Query-Aware KV Retriever: Dynamically selects relevant key-value blocks at decode time based on the current query vector and page-level metadata, reducing unnecessary mem- ory access. (2)Modular Scheduling Pipeline: A dispatch loop handles incoming queries and routes them through configurable plug- ins (e.g., entropy-based early exit, token-level pruning, ap- proximate attention). This modular design allows experimen- tation with different sparsity strategies without modifying the core model. (3)Sparse Attention Executor: Efficiently computes attention over selected KV pages using fused CUDA kernels, with support for FP16/INT8 KV formats and multi-GPU dispatch.",1,2,"['3.1 System Overview: TinyServe\nTinyServe is a lightweight serving framework designed for serving\ntiny language models under tight memory and latency constraints.\nRather than acting as a benchmarking tool, TinyServe serves as a\nreal-time serving environment that enables sparsity-aware atten-\ntion, modular token selection, and efficient KV-cache reuse.\nThe system is organized around three core components:\n(1)Query-Aware KV Retriever: Dynamically selects relevant', 'the training process.\nFor distributed training scenarios, TinyServe supports asynchro-\nnous gradient synchronization with configurable communication\npatterns, allowing researchers to experiment with different paral-\nlelization strategies without modifying the core training loop. This\nis particularly valuable for exploring efficient training strategies on\nresource-constrained hardware.\n3.3 Inference Time Is Dominated by Decode\nStage\nLLM inference consists of two stages: prefill and decode. In the', 'pages); - Reduced HBM bandwidth pressure.\nSystem Implication. TinyServe enables dynamic query-aware\nsparsity without requiring architectural retraining. The modular\nimplementation integrates directly into TinyServeâ€™s kernel loop\nand allows hardware-sensitive scheduling: e.g., keeping hot pages\nin shared memory or limiting K to match tensor core granularity.\nThe kernel design for TinyServe can be found at algorithm 1.\n3.6 Memory Efficiency Analysis', 'The kernel design for TinyServe can be found at algorithm 1.\n3.6 Memory Efficiency Analysis\nTo quantify memory access savings under query-aware sparsity, we\nconstruct a probabilistic cost model that accounts for (1) metadata\noverhead, (2) selected KV tokens, and (3) cross-step reuse. This anal-\nysis provides theoretical bounds on the performance improvements\nachievable through our approach.\nLet: -ğ¿: total cache length (tokens); - ğ‘†: page size (tokens per', 'sity dynamicsâ€”emerge in small models under realistic serving work-\nloads. By emulating production serving scenarios with tiny LLMs,\nwe can approximate the performance trends and failure modes of\nlarge-scale deployments at a fraction of the cost.\nQuery-Aware Sparsity and Efficient KV Access. To demonstrate\nthe utility of TinyServe, we propose a query-aware token selection\nmechanism that leverages low-cost metadata to dynamically se-\nlect the most relevant parts of the KV cache for each query. This', 'and evaluation scripts will be made publicly available upon publi-\ncation. The codebase includes:\nâ€¢TinyServe framework implementation with modular plugin\nsystem\nâ€¢All baseline implementations (vLLM, TGI, TensorRT-LLM\nadapters)\nâ€¢Evaluation scripts for all experiments\nâ€¢Preprocessed datasets and model checkpointsMM â€™25, October 27â€“31, 2025, Dublin, Ireland Dong Liu and Yanxuan Yu\nAccuracyLatency\nThroughput\nKV Hit RateTinyLLaMA-125M\nFullCache\nStreamingLLM\nSoftPrune\nEntropyStop\nTinyServe\nAccuracyLatency\nThroughput', 'and unified memory management ( Â§3.4).\n3.1 System Overview\nFigure 3 illustrates the overall system architecture for PSA, which comprises three key components:\nthe batch scheduler, the model executor, and the KV cache manager.\nâ€¢The Batch Controller is responsible for grouping incoming requests into batches using dynamic\nbatching techniques [ 43] in a first-come-first-serve (FCFS) manner. These batches are subsequently', 'accuracy while achieving significant latency reduction across all\ntasks.\n4.8 KV Cache Efficiency and Access Breakdown\nWe visualize KV cache utilization over time and analyze memory\naccess patterns. Figure 6 shows cache reuse patterns, while Figure 7\nprovides detailed access breakdown. TinyServe preserves high-\nrelevance tokens and avoids cache flushing, resulting in higher\neffective reuse rate.\n4.9 Serving Synthetic Diagnostics\nTo validate behavioral consistency and stress-test our system under', 'We use a default page size of 16 for best tradeoff.\n4.12 Multi-GPU Scaling\nWe evaluate TinyServeâ€™s scalability from 1 to 8 A100 GPUs on 128\nconcurrent prompts. Results show near-linear scaling in throughput,\nvalidating kernel fusion and inter-GPU cache reuse.\n4.13 Reproducibility and Implementation\nDetails\n4.13.1 Hyperparameter Search. We conducted extensive hyperpa-', 'ğ‘–=1softmax(ğ‘âŠ¤\nğ‘¡ğ‘˜ğ‘–)Â·ğ‘£ğ‘–\nThis process is latency-critical during inference due to two bot-\ntlenecks:\nâ€¢Memory movement : loading allğ‘˜ğ‘–,ğ‘£ğ‘–from high-bandwidth\nmemory (HBM);\nâ€¢Unstructured access : attention requires full key scan with\nno cache prefetch pattern.\nTo address this, TinyServe introduces a structured memory\nlayout via token grouping into fixed-size pages . Letğ¾=Ãğ‘ƒ\nğ‘—=1Kğ‘—\nbe partitioned into ğ‘ƒ=âŒˆğ‘¡/ğ‘†âŒ‰pages of size ğ‘†. Each pageKğ‘—stores a\nsmall metadata summary ğœ™(Kğ‘—)that enables relevance estimation.']",0.0,0.2,0.0,0,0.046511627906976744,0.25,0.07843137254901962,0.24501041774014767,0.14092140921409213,0.424,10,0.03417852426735417,1.0,0.0,0.3333333333333333,0.0
What are the trade-offs between simple post-training quantization and GPTQ?,multi,"('8CUDA APIs for Half Precisionâ€¢Using lower precision oconverting parameters from FP32 to INT8 or INT4 operform all computation in lower prevision. Reduce model accuracy: oLoss of Precision â” accumulate quantization noise oRange mismatch â” values are clipped and lead to information loss oQuantization error â” rounding errors 9Direct Quantization Approachâ€¢Absmax  quant Zero -point quant 10Quantize a number', 'mixed strategy for 8 -bit quantization and 16 -bit (for outliers)2Recap of Basic Quantization Methodsâ€¢Absmax  quant Zero -point quant 4Quantize a Number to Int8 5GPTQ scale to GPT -size LLMs maintain accuracyâ€¢Revisit  layer -wise quantization of weight matrices argmin', 'layer -wise quantization + compensation for errors + precompute oaccurately compress some of the largest publicly -available models down to 3 and 4 bits, and bring end -to-end speedups')",3,4,"['28\nâ€¢Overview of Parameter Efficient Fine -Tuning\nâ€¢LoRA : Low -rank Adaptation (Counter -interference adapter, \nCIAT)\nâ€¢QLoRA : Quantization + Low -rank training\nâ€¢Code Walkthrough\n30Outline\nâ€¢GPTQ is Post -Training Quantization (PTQ): converting the \nweights of an already trained model to a lower precision \nwithout any retraining. \nâ€¢Quantization -Aware Training (QAT): integrates the weight \nconversion process during the training stage. often superior \nmodel performance. ( QLoRA ) ', 'error  incurred by quantizing a single weight\n6Overall idea of GPTQ\nOptimal Brain Compression: A framework for accurate post -training quantization and pruning (2022)Optimal Brain Surgeon and General Network Pruning (1993)GPTQ: Accurate Post -Training Quantization for Generative Pre -trained Transformers. Frantar  et al. ICLR 2023. 1.Pre-compute Cholesky decomposition of the Hessian \ninverse for input data X of current (Linear) layer\n2.Iteratively handle one batch of columns of weight matrix W', 'â€¢The scalability is verified up to 20B models (GPT -NeoX20B)\nâ€¢At 1.3B scale, computation time is ~3 hours\nobut slower than GPTQ (x100 larger in ~4 hours)\nâ€¢integrated in Deepspeed\n22ZeroQuant\nYao et al. ZeroQuant : Efficient and Affordable Post -Training Quantization for Large -Scale Transformers. Neurips  2022.â€¢Using 8 -bit quantization for \nmatrix multiplications\nâ€¢But, extreme outliers in \nfeatures (activation values)\noneed for wider numerical ranges \noQuantize all parameters without ', 'language models,â€ Advances in Neural Information Processing Systems ,\nvol. 35, pp. 17 402â€“17 414, 2022.\n[7] G. Xiao, J. Lin, M. Seznec, H. Wu, J. Demouth, and S. Han,\nâ€œSmoothquant: Accurate and efficient post-training quantization for\nlarge language models,â€ in International Conference on Machine\nLearning . PMLR, 2023, pp. 38 087â€“38 099.\n[8] E. Frantar, S. Ashkboos, T. Hoefler, and D. Alistarh, â€œGptq: Accu-\nrate post-training quantization for generative pre-trained transformers,â€', '19Insight of Arbitrary Update Order for OBQ\nGPTQ: Accurate Post -Training Quantization for Generative Pre -trained Transformers. Frantar  et al. ICLR 2023. â€¢NaÃ¯ve column update is not fast in practice \nolow compute -to-memory -access ratio\nocannot highly utilize GPUs compute.\nâ€¢Observation: \noRounding decisions for col i only affected \nby updates on this col\noUpdates to later columns are irrelevant at \nthis point in the process.\nâ€¢Efficient update\n20Lazy Batch Updates', '15Quantization   during training\npost trainingpreserve accuracy\nscale to large \nparametersBRECQ (Li et al., 2021)AdaQuant (Hubara et al., 2021)\nOBQ (Frantar et al., 2022)\nZeroQuant (Yao et al., 2022)\nLLM.int8() (Dettmers et al., 2022)Model Quantization Approaches\n16Quantization  during training\npost trainingpreserve \naccuracy\n(by quantizing each \nindividual / consecutive \nlayers)\nscale to large \nparametersBRECQ (Li et al., 2021)AdaQuant (Hubara et al., 2021)\nOBQ (Frantar et al., 2022)', 'P rec@5 85.7 % 5.6 % 75.7 %\n19Is Quantization Accurate?20Why is Quantizing LLMs Difficult?\nQuantization  during training\npost trainingpreserve accuracy\nscale to large \nparameters\n(by rounding weights to the \nnearest quantization level)BRECQ (Li et al., 2021)AdaQuant (Hubara et al., 2021)\nOBQ (Frantar et al., 2022)\nZeroQuant (Yao et al., 2022)\nLLM.int8() (Dettmers et al., 2022)accuracy loss when lower -bit \nprecision (ex. 3, 4 bits per \nparameter)â€¢Layer -by-layer knowledge distillation ', 'rate post-training quantization for generative pre-trained transformers,â€\narXiv preprint arXiv:2210.17323 , 2022.\n[9] Z. Liu, B. Oguz, C. Zhao, E. Chang, P. Stock, Y . Mehdad, Y . Shi, R. Kr-\nishnamoorthi, and V . Chandra, â€œLlm-qat: Data-free quantization aware\ntraining for large language models,â€ arXiv preprint arXiv:2305.17888 ,\n2023.\n[10] W. Wang, W. Chen, Y . Luo, Y . Long, Z. Lin, L. Zhang, B. Lin,\nD. Cai, and X. He, â€œModel compression and efficient inference for', '1.3B model - 3h\nDoes GPT -Q even work for extreme 2 -bit quantization?\n28\nHow is GPT -Qâ€™s perf on small models compared with \naccurate -but-expensive methods?\n29\n Fastest prior methodâ€¢https://github.com/qwopqwop200/GPTQ -for-LLaMa/  \nâ€¢GPTQ in \nohttps://github.com/qwopqwop200/GPTQ -for-\nLLaMa/blob/triton/gptq.py  \n31GPTQ for LLaMAGPTQ: Initialization\n32\nâ—Reshape weights from the \ninput layer\nâ—Initialize Hessian matrixGPTQ: Hessian Matrix Update\n33â—Update Hessian matrix with \ninformation from a new ', 'â€¢Key ideas: \n1.Quantizes one column -block of weights at a time\n2.Updates all the not -yet-quantized weights, to compensate for the \nerror  incurred by quantizing a single weight\n6Overall idea of GPTQ']",0.0,0.0,0.0,0,0.0,0.0,0.0,0.22510032498346483,0.19750519750519752,0.2865979381443299,10,4.0890867856657154e-05,0.0,0.0,0.0,0.0
What does â€œIO-awareâ€ mean in the context of FlashAttention?,single,"In this paper, we argue that a missing principle is making attention algorithms IO-aware [1]â€”that is, carefully accounting for reads and writes to diï¬€erent levels of fast and slow memory (e.g., between fast GPU on-chip SRAM and relatively slow GPU high bandwidth memory, or HBM [ 45], Figure 1 left).",1,1,"['computation back to HBM. Right:Speedup over the PyTorch implementation of attention on GPT-2.\nFlashAttention does not read and write the large ğ‘\x02ğ‘attention matrix to HBM, resulting in an 7.6 \x02\nspeedup on the attention computation.\nGPUs, compute speed has out-paced memory speed [ 61,62,63], and most operations in Transformers are\nbottlenecked by memory accesses [ 43]. IO-aware algorithms have been critical for similar memory-bound', 'FlashAttention : Fast and Memory-Eï¬ƒcient Exact Attention\nwith IO-Awareness\nTri Daoy, Daniel Y. Fuy, Stefano Ermony, Atri Rudraz, and Christopher RÃ©y\nyDepartment of Computer Science, Stanford University\nzDepartment of Computer Science and Engineering, University at Buï¬€alo, SUNY\n{trid,danfu}@cs.stanford.edu ,ermon@stanford.edu ,atri@buffalo.edu ,\nchrismre@cs.stanford.edu\nJune 24, 2022\nAbstract\nTransformers are slow and memory-hungry on long sequences, since the time and memory complexity', 'We analyze the IO complexity [ 1] ofFlashAttention , proving that it requires ğ‘‚Â¹ğ‘2ğ‘‘2ğ‘€\x001ÂºHBM\naccesses where ğ‘‘is the head dimension and ğ‘€is the size of SRAM, as compared to Î©Â¹ğ‘ğ‘‘Â¸ğ‘2Âºof standard\nattention. For typical values of ğ‘‘andğ‘€,FlashAttention requires many times fewer HBM accesses\ncompared to standard attention (up to 9 \x02fewer, as shown in Fig. 2). Moreover, we provide a lower bound,\nshowing that no exact attention algorithm can asymptotically improve on the number of HBM accesses over\nall SRAM sizes.', 'Proposition 4. Letğ‘be the sequence length, ğ‘‘be the head dimension, and ğ‘€be size of SRAM with\nğ‘‘\x14ğ‘€\x14ğ‘ğ‘‘. Block-sparse FlashAttention (Algorithm 5) requires Î˜Â¹ğ‘ğ‘‘Â¸ğ‘2ğ‘‘2ğ‘€\x001ğ‘ ÂºHBM accesses\nwhereğ‘ is the fraction of nonzero blocks in the block-sparsity mask.\nWe see that applying block-sparsity yields a direct improvement by the sparsity to the larger term in the\nIO complexity. For large sequence lengths ğ‘,ğ‘ is often set to ğ‘\x001\x9d2[11] orğ‘\x001logğ‘[3,17,92], resulting\ninÎ˜Â¹ğ‘p', '17:end if\n18:end for\n19:end for\n20:Return O\x96â„“\x96ğ‘š\x96R.\nD Extension Details\nD.1 Block-sparse FlashAttention\nWe describe the full block-sparse FlashAttention algorithm in Algorithm 5. The algorithm is identical\nto Algorithm 2, except that we skip zero blocks.\nWe prove the IO-complexity of block-sparse FlashAttention .\nProof of Proposition 4. The proof is very similar to the proof of Theorem 2. For the block-sparse case, notice', 'HBM, and needs to write the outputs of size ğ‘2orğ‘ğ‘‘to HBM. This incurs Î˜Â¹ğ‘ğ‘‘Â¸ğ‘2ÂºHBM accesses.\nWe now analyze the IO complexity of FlashAttention backward pass.\nSimilar to Theorem 2, we see that each element of KandVis loaded from HBM once. Each element of\ndKanddVis only written to HBM once. We make ğ‘‡ğ‘passes over Q\x96O\x96dO, each pass loading all of Q\x96O\x96dO\nto HBM. We also make ğ‘‡ğ‘passes over dQ, each pass reading/writing all of dQfrom/to HBM. Therefore the\nnumber of HBM accesses is Î˜Â¹ğ‘ğ‘‘Â¸ğ‘ğ‘‘ğ‘‡ğ‘Âº=Î˜Â¹ğ‘ğ‘‘ğ‘‡ğ‘Âº.', 'both methods avoid storing the large attention matrix in the forward pass and recompute it in the backward\npass.\nThe ï¬rst major diï¬€erence is that Rabe and Staats [66]focuses on the reducing the total memory footprint\n(maximum amount of GPU memory required) while FlashAttention focuses on reducing memory accesses\n(the number of memory reads/writes). As mentioned in Section 2, the amount of memory access is the', 'blocks are combined using the statistics to produce the ï¬nal output. FlashAttention instead incrementally\nupdates the output (Algorithm 1 line 12) after processing each block, so only one copy of the output is needed\n(instead ofğ¾copies forğ¾blocks). This means that FlashAttention has smaller total memory requirement\ncompared to Rabe and Staats [66].\nThe ï¬nal major diï¬€erence is the way the backward pass is computed. Rabe and Staats [66]uses gradient', '(the number of memory reads/writes). As mentioned in Section 2, the amount of memory access is the\nprimary determining factor of runtime. Reducing memory accesses also necessarily reduces the total amount\nof memory required (e.g., if an operation incurs ğ´memory accesses, then its total memory requirement is at\nmostğ´). As a result, FlashAttention is faster than standard attention (2-4 \x02) while Rabe and Staats [66]', 'Standard attention (Algorithm 0) requires Î˜Â¹ğ‘ğ‘‘Â¸ğ‘2ÂºHBM accesses, while FlashAttention (Algorithm 1)\nrequiresÎ˜Â¹ğ‘2ğ‘‘2ğ‘€\x001ÂºHBM accesses.\nFor typical values of ğ‘‘(64-128) and ğ‘€(around 100KB), ğ‘‘2is many times smaller than ğ‘€, and thus\nFlashAttention requires many times fewer HBM accesses than standard implementation. This leads to\nboth faster execution and lower memory footprint, which we validate in Section 4.3.\nThe main idea of the proof is that given the SRAM size of ğ‘€, we can load blocks of K\x96Vof sizeÎ˜Â¹ğ‘€Âºeach']",0.0,0.0,0.0,0,0.0,0.0,0.0,0.1861447783174603,0.13942307692307693,0.24271844660194175,10,1.0300185274803412e-07,0.0,0.0,0.0,0.0
"What is internal covariate shift, and how does it affect training?",single,"We deï¬ne Internal Covariate Shift as the change in the distribution of network activations due to the change in network parameters during training. To improve the training, we seek to reduce the internal covariate shift. By ï¬xing the distribution of the layer inputs xas the training progresses,we expect to improve the training speed.",1,4,"['ratesandcarefulparameterinitialization,andmakesitno -\ntoriously hard to train models with saturating nonlineari-\nties. We refer to this phenomenon as internal covariate\nshift, and address the problem by normalizing layer in-\nputs. Ourmethoddrawsitsstrengthfrommakingnormal-\nizationapartofthemodelarchitectureandperformingthe\nnormalization for each training mini-batch . Batch Nor-\nmalizationallowsustousemuchhigherlearningratesand\nbe less careful about initialization. It also acts as a regu-', 'that the distribution of nonlinearity inputs remains more\nstable as the network trains, then the optimizer would be\nless likely to get stuck in the saturated regime, and the\ntrainingwouldaccelerate.\nWe refer to the change in the distributions of internal\nnodes of a deep network, in the course of training, as In-\nternal Covariate Shift . Eliminating it offers a promise of\nfaster training. We propose a new mechanism, which we\ncallBatch Normalization , that takes a step towards re-', 'that we can match its performance using only 7% of the\ntraining steps, and can further exceed its accuracy by a\nsubstantial margin. Using an ensemble of such networks\ntrained with Batch Normalization, we achieve the top-5\nerror rate that improves upon the best known results on\nImageNetclassiï¬cation.2 Towards Reducing Internal\nCovariateShift\nWe deï¬ne Internal Covariate Shift as the change in the\ndistribution of network activations due to the change in\nnetworkparametersduringtraining. Toimprovethetrain-', 'callBatch Normalization , that takes a step towards re-\nducing internal covariate shift, and in doing so dramati-\ncally accelerates the training of deep neural nets. It ac-\ncomplishes this via a normalization step that ï¬xes the\nmeansandvariancesoflayerinputs. BatchNormalization\nalso has a beneï¬cial effect on the gradient ï¬‚ow through\nthe network, by reducing the dependence of gradients\non the scale of the parameters or of their initial values.\nThis allows us to use much higher learning rates with-', 'networkparametersduringtraining. Toimprovethetrain-\ning, we seek to reduce the internal covariate shift. By\nï¬xingthe distributionof the layer inputs xas the training\nprogresses,weexpecttoimprovethetrainingspeed. Ithas\nbeen long known (LeCunetal., 1998b; Wiesler &Ney,\n2011) that the network training convergesfaster if its in-\nputsarewhitenedâ€“i.e.,linearlytransformedtohavezero\nmeansandunitvariances,anddecorrelated. Aseachlayer\nobservestheinputsproducedbythelayersbelow,itwould', 'computation , 9(8):1735â€“1780, 1997.\n[16] S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep\nnetwork training by reducing internal covariate shift. In ICML , 2015.\n[17] H. Jegou, M. Douze, and C. Schmid. Product quantization for nearest\nneighbor search. TPAMI , 33, 2011.\n[18] H. Jegou, F. Perronnin, M. Douze, J. Sanchez, P. Perez, and\nC. Schmid. Aggregating local image descriptors into compact codes.\nTPAMI , 2012.\n[19] Y . Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick,', 'network training by reducing internal covariate shift. arXiv\npreprint arXiv:1502.03167 , 2015.\nJi, X., Henriques, J. F., and Vedaldi, A. Invariant information\nclustering for unsupervised image classiï¬cation and segmenta-\ntion. In Proceedings of the IEEE International Conference on\nComputer Vision , pp. 9865â€“9874, 2019.\nKingma, D. P. and Welling, M. Auto-encoding variational bayes.\narXiv preprint arXiv:1312.6114 , 2013.\nKolesnikov, A., Zhai, X., and Beyer, L. Revisiting self-supervised', 'ï¬tting,inabatch-normalizednetworkwefoundthatitcan\nbeeitherremovedorreducedinstrength.\n4 Experiments\n4.1 Activationsovertime\nTo verify the effects of internal covariate shift on train-\ning, and the ability of Batch Normalization to combat it,\nweconsideredtheproblemofpredictingthedigitclasson\ntheMNISTdataset(LeCunetal.,1998a). Weusedavery\nsimple network, with a 28x28binary image as input, and\n510K20K30K40K50K0.70.80.91\n  \nWithout BN\nWith BN\nâˆ’202\nâˆ’202\n(a) (b)WithoutBN (c)With BN', 'and Aaron van den Oord. Data-efï¬cient image recognition with contrastive predictive coding. In\nICML , 2020.\n10Published as a conference paper at ICLR 2021\nSergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by\nreducing internal covariate shift. 2015.\nDiederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR , 2015.\nAlexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan Puigcerver, Jessica Yung, Sylvain Gelly,', 'introduces normalized activations into the network. This\nensures that as the model is training, layers can continue\nlearningoninputdistributionsthatexhibitlessinternal co-\nvariate shift, thus accelerating the training. Furthermor e,\nthe learned afï¬ne transform applied to these normalized\nactivationsallowstheBNtransformtorepresenttheiden-\ntity transformationandpreservesthenetworkcapacity.\n3.1 Training and Inference with Batch-\nNormalizedNetworks\nToBatch-Normalize anetwork,wespecifyasubsetofac-']",1.0,0.2,0.33333333333333337,1,0.09876543209876543,0.6666666666666666,0.17204301075268816,0.2620677556625185,0.18962075848303392,0.4393305439330544,10,0.05371348238687446,0.0,0.0,0.3333333333333333,1.0
What is NVIDIA GPU Confidential Computing (CC) and how does it secure communication?,single,"NVIDIA CC ensures the confidentiality and integrity of communication between a CVM and a GPU via AES-GCM encryption [ 15]. A critical component of AES- GCM is the Initialization Vector (IV), a unique, non-repeating number (a nonce) required for each encryption session.",1,1,"['their memory, this encryption is separate from that used\nby NVIDIA CC. NVIDIA CC ensures the confidentiality and\nintegrity of communication between a CVM and a GPU via\nAES-GCM encryption [ 15]. A critical component of AES-\nGCM is the Initialization Vector (IV), a unique, non-repeating\nnumber (a nonce) required for each encryption session. As\nwe will show later (Â§4.1), managing IVs presents a significant\nchallenge.\nFigure 1 illustrates the workflow of data transfers of the', 'GPU to protect sensitive data and models from unauthorized\naccess. Moreover, the data communication between the CVM\nand the GPU enclave is encrypted, further reinforcing the\nsecurity of I/O operations.\nAlthough GPU confidential computing effectively enhances\nsecurity for traditional small-scale AI models, it significantly\nundermines the performance of LLMs in throughput and\nlatency. Our comprehensive experiments on NVIDIA H100\nGPUs reveal that the GPU enclave can incur up to a 52.8%', 'denote ciphertexts moved from the GPU back to the CPU. After the\ntransfers, the current IV of CPU and GPU is 3 and 7, respectively.\nread/write GPU memory and modify the control flow. Hard-\nware GPU confidential computing has low performance over-\nhead and is backward-compatible with existing applications.\nThis paper focuses on studying hardware GPU confidential\ncomputing.\nA closer look at NVIDIA CC. Although CVMs encrypt\ntheir memory, this encryption is separate from that used', 'tion of how NVIDIA Confidential Computing and PipeLLM\nexecute it.\nCPU\nGPU\nGPUCPUDecrypt Encrypt\nSaved\nTime1c 3c\n1t1c\n1t\nPipeLLMNVIDIA\nCC# 1. Swap\xa0 from GPU\n# - PCIe transfer\n# - CPU decryption\ndata = swap_from_gpu()\n# 2. GPU computation\nllm.compute()\n# 3. Load back to GPU\n# - CPU encryption\n# - PCIe transfer\nload_back_to_gpu(data)1t\n1c\n2\n3c\n3tTime\n3t3t3c\n2\nCompute2\nCompute\nFor transparency, NVIDIA Confidential Computing per-\nforms on-the-fly encryption and decryption (indicated by', 'of NVIDIA Confidential Computing, while preserving its (1)\nsecurity guarantees and (2) user transparency. By user trans-\nparency, we mean that PipeLLM applies to non-modified\nLLM applications, including LLM models, deep learning\nframeworks, and any other supporting code and data. Next,\nwe elaborate on PipeLLMâ€™s threat model.\nThreat model. NVIDIA Confidential Computing aims at\nprotecting the confidentiality and integrity of applications\nrunning on GPUs; for LLM applications, these are the model', 'commands, but this adds substantial performance overhead\ndue to extra runtime checks for indirect memory access,\nheavily used in systems like vLLM [25].\nUnlike software-based solutions, NVIDIA Confidential\nComputing relies on hardware: NVIDIA H100 GPU is the\nfirst commercial implementation with confidential comput-\ning capability [ 15]. Working with CVMs, H100 could build a\nGPU enclave, allowing users to have exclusive control over\nthe GPU and rejecting any access from the host, such as', 'guarantees of NVIDIA Confidential Computing.\nAlthough PipeLLM does not compromise confidentiality\nor integrity, its mis-speculation introduces side channels in\nNOP transfers compared to NVIDIA Confidential Comput-\ning, including (1) attackers can detect if the LLM system is\ncurrently swapping by observing NOPs, and (2) attackers\ncould profile the frequency of prediction failures, potentially\nrevealing the swapping patterns of applications. The security', 'gies such as Intel TDX [ 18], AMD SEV [ 2], and ARM CCA [ 3],\nserves as a prime example of this. Any software external to\na CVM is unable to access the code and data within it.\nRegarding machine learning workloads, people develop\nGPU enclaves to enhance security measures within GPUs [ 36,\n50]. A notable implementation of this is the NVIDIA H100\nGPU [ 36], which supports confidential computing inside the\nGPU to protect sensitive data and models from unauthorized', 'encrypted version on the CPU. This would allow the en-\ncrypted data to be transferred directly to the GPU during\nloading, thereby eliminating encryption overhead. While this\napproach improves performance, implementing it naively\ncompromises security guarantees. For example, reusing en-\ncrypted data enables attackers to identify data that matches\na previous transfer; more critically, it could make the system\nvulnerable to replay attacks [35].\nCurrently, NVIDIA Confidential Computing uses the AES-', 'their memory to guarantee privacy. CVMs generally incur\nminimal performance overhead, averaging around 4% [ 24].\nI/O operations however may introduce significant perfor-\nmance overhead due to data copying and encryption [27].\nConfidential Computing (CC) on GPUs. Beyond CPU-\nbased CVMs, confidential computing on GPUs secures GPU\ncomputations such as LLM serving and training. Soter [40],\ndesigned for edge computing, uses CPU-side confidential\ncomputing to eliminate the trust on GPU hardware. It em-']",1.0,0.1,0.18181818181818182,1,0.041666666666666664,1.0,0.07999999999999999,0.21023599525292286,0.07,0.5651260504201681,10,0.05541644715140558,1.0,1.0,0.3333333333333333,1.0
What is the difference between torch.disttibuted and torch.distributed.pipelining?,multi,"('While promising for scaling, pipelining is often difficult to implement because it needs to partition the execution of a model in addition to model weights. The partitioning of execution often requires intrusive code changes to your model. Another aspect of complexity comes from scheduling micro-batches in a distributed environment, with data flow dependency considered.', 'The torch.distributed  package provides PyTorch support and communication primitives for multiprocess parallelism across several computation nodes running on one or more machines.')",2,2,"['What is torch.distributed.pipelining ?\nWhile promising for scaling, pipelining is often difficult to implement because it needs to partition\nthe execution of a model in addition to model weights. The partitioning of execution often requires\nintrusive code changes to your model. Another aspect of complexity comes from scheduling\nmicro-batches in a distributed environment, with data flow dependency considered.\nThe pipelining  package provides a toolkit that does said things automatically which allows easy', 'splitting the module. (default: None)\nReturn type:\nA pipeline representation of class Pipe.\nclass torch.distributed.pipelining. Pipe (split_gm , num_stages ,\nhas_loss_and_backward , loss_spec )\ntorch.distributed.pipelining. pipe_split ( )\npipe_split is a special operator that is used to mark the boundary between stages in a module.\nIt is used to split the module into stages. It is a no-op if your annotated module is run eagerly.\nExample\nThe above example will be split into two stages.\nMicrobatch Utilities', 'torch.distributed.ProcessGroupNCCL.Options . Learn more about them using help (e.g.\nhelp(torch.distributed.ProcessGroupNCCL.NCCLConfig) ) in the interpreter.\nBasics\nThe torch.distributed  package provides PyTorch support and communication primitives for\nmultiprocess parallelism across several computation nodes running on one or more machines. The\nclass torch.nn.parallel.DistributedDataParallel()  builds on this functionality to provide', 'model to be partitioned such that multiple micro-batches can execute different parts of the mode\ncode concurrently. Pipeline parallelism can be an effective technique for:\nlarge-scale training\nbandwidth-limited clusters\nlarge model inference\nThe above scenarios share a commonality that the computation per device cannot hide the\ncommunication of conventional parallelism, for example, the weight all-gather of FSDP.\nWhat is torch.distributed.pipelining ?', 'TORCH_GLOO_LAZY_INIT  - establishes connections on demand rather than using a full mesh\nwhich can greatly improve initialization time for non all2all operations.\nPost-Initialization\nOnce torch.distributed.init_process_group()  was run, the following functions can be used.\nTo check whether the process group has already been initialized use\ntorch.distributed.is_initialized() .\nclass torch.distributed. Backend (name )\nAn enum-like class for backends.', 'during initialization that can lead to hangs.\ntorch.distributed. is_available ( )\nReturn True if the distributed package is available.\nOtherwise, torch.distributed  does not expose any other APIs. Currently,\ntorch.distributed  is available on Linux, MacOS and Windows. Set USE_DISTRIBUTED=1  to\nenable it when building PyTorch from source. Currently, the default value is\nUSE_DISTRIBUTED=1  for Linux and Windows, USE_DISTRIBUTED=0  for MacOS.\nReturn type:\nbool', 'class\ntorch.distributed.pipelining.schedules. PipelineScheduleSingle (stage,\nn_microbatches , loss_fn=None, args_chunk_spec =None, kwargs_chunk_spec =None\noutput_merge_spec =None, scale_grads =True )\nBase class for single-stage schedules. Implements the step method. Derived classes should\nimplement _step_microbatches.\nGradients are scaled by num_microbatches depending on the scale_grads argument, defaultin\nto True. This setting should match the configuration of your loss_fn, which may either average', 'forward function. :ivar END: Represents adding a split point after the execution of a certain\nsubmodule in the forward function.\ntorch.distributed.pipelining. pipeline (module, mb_args, mb_kwargs =None,\nsplit_spec =None, split_policy =None )\nSplit a module based on a specification.\nSee Pipe for more details.\nParameters:\nmodule (Module) â€“ The module to be split.\nmb_args (tuple[Any, ...]) â€“ Example positional inputs, in micro-batch form.', 'components.\nInitialization\nThe package needs to be initialized using the torch.distributed.init_process_group()  or\ntorch.distributed.device_mesh.init_device_mesh()  function before calling any other\nmethods. Both block until all processes have joined.\nInitialization is not thread-safe. Process group creation should be performed from a single\nthread, to prevent inconsistent â€˜UUIDÊ¼ assignment across ranks, and to prevent races\nduring initialization that can lead to hangs.\ntorch.distributed. is_available ( )', 'Pipeline Parallelism\nCreated On: Jun 16, 2025 | Last Updated On: Jun 16, 2025\ntorch.distributed.pipelining  is currently in alpha state and under development. API\nchanges may be possible. It was migrated from the PiPPy project.\nWhy Pipeline Parallel?\nPipeline Parallelism is one of the primitive parallelism for deep learning. It allows the execution of \nmodel to be partitioned such that multiple micro-batches can execute different parts of the mode']",1.0,0.2,0.33333333333333337,2,0.19672131147540983,1.0,0.3287671232876712,0.2644633503442417,0.15079365079365079,0.7337278106508875,10,0.12297747844607611,1.0,0.5,0.6666666666666666,1.0
What optimizations does FlashAttention introduce compared to standard attention kernels implemented in PyTorch?,multi,"('FMHA targets BERT models, it only supports head dimension 64, and only runs on A100 GPUs. FMHA fuses the attention computation dropoutÂ¹softmaxÂ¹maskÂ¹QK>ÂºÂºÂºVinto one CUDA kernel. In the forward pass, it stores the attention matrix softmaxÂ¹maskÂ¹QKğ‘‡ÂºÂºto HBM to be used in gradient computation. As a result, it does not oï¬€er substantial memory saving (though for shorter sequences memory footprint is often not a primary concern).', 'This module contains functions and classes that alter the behavior of torch.nn.functional.scaled_dot_product_attention', 'flex_attention This module implements the user facing API for flex_attention in PyTorch. bias Defines bias subclasses that work with scaled_dot_product_attention')",3,3,"['tention and block-sparse FlashAttention compared to the baselines in exact, approximate, and sparse\nattention (exact numbers in Appendix E). Runtime grows quadratically with sequence length, but FlashAt-\ntention runs signiï¬cantly faster than exact attention baselines, up to 3 \x02faster than the PyTorch\nimplementation. The runtimes of many approximate/sparse attention mechanisms grow linearly with se-\nquence length, but FlashAttention still runs faster than approximate and sparse attention for short', 'computation back to HBM. Right:Speedup over the PyTorch implementation of attention on GPT-2.\nFlashAttention does not read and write the large ğ‘\x02ğ‘attention matrix to HBM, resulting in an 7.6 \x02\nspeedup on the attention computation.\nGPUs, compute speed has out-paced memory speed [ 61,62,63], and most operations in Transformers are\nbottlenecked by memory accesses [ 43]. IO-aware algorithms have been critical for similar memory-bound', 'time of all models. Each task has a diï¬€erent sequence length varying between 1024 and 4096. We follow the\nimplementation and experimental setting in Tay et al. [80]and Xiong et al. [90].3Table 3 shows that FlashAt-\ntention achieves up 2.4\x02speed-up compared to standard attention. Block-sparse FlashAttention is\nfaster than all of the approximate attention methods that we have tested.\nTable 3: The performance of standard attention, FlashAttention , block-sparse FlashAttention , and approximate', 'wall-clock time. We analyze its IO complexity, showing that our method requires much fewer HBM accesses\ncompared to standard attention. We further show that FlashAttention can serve as a useful primitive by\nextending it to handle block-sparse attention.\nWe focus here on the forward pass for ease of exposition; Appendix B contains details for the backward.\n3.1 An Eï¬ƒcient Attention Algorithm With Tiling and Recomputation', 'We analyze the IO complexity [ 1] ofFlashAttention , proving that it requires ğ‘‚Â¹ğ‘2ğ‘‘2ğ‘€\x001ÂºHBM\naccesses where ğ‘‘is the head dimension and ğ‘€is the size of SRAM, as compared to Î©Â¹ğ‘ğ‘‘Â¸ğ‘2Âºof standard\nattention. For typical values of ğ‘‘andğ‘€,FlashAttention requires many times fewer HBM accesses\ncompared to standard attention (up to 9 \x02fewer, as shown in Fig. 2). Moreover, we provide a lower bound,\nshowing that no exact attention algorithm can asymptotically improve on the number of HBM accesses over\nall SRAM sizes.', 'In Fig. 2 (left), we see that even though FlashAttention has higher FLOP count compared to standard\nattention (due to recomputation in the backward pass), it has much fewer HBM accesses, resulting in much\nfaster runtime. In Fig. 2 (middle), we vary the block size ğµğ‘ofFlashAttention , which results in diï¬€erent\namounts of HBM accesses, and measure the runtime of the forward pass. As block size increases, the number', 'solely from using a longer sequence length (16K). Block-sparse FlashAttention enables a Transformer\nto scale to even longer sequences (64K), resulting in the ï¬rst model that can achieve better-than-chance\nperformance on Path-256.\nâ€¢Benchmarking Attention. FlashAttention is up to 3\x02faster than the standard attention implemen-\ntation across common sequence lengths from 128 to 2K and scales up to 64K. Up to sequence length of 512,', 'optimizations such as operator reordering, fusion, and mem-\nory reuse.\nMajor frameworks have been integrating compiler capa-\nbilities into their ecosystems. For example, PyTorch now in-\ncludes its own compiler infrastructure to enable graph-based\noptimizations [ 2]. Since these compilers can be applied to\nthe vast number of existing model implementations, they\nare now widely used in practice.\n2.4 Motivating Example\nThe fully sharded approach, as implemented in systems like', 'can be scheduled as long as the required KV blocks for executing a single iteration of progressive\nattention can be accommodated within the GPU memory.\nCompatibility with FlashAttention [ 10].FlashAttention is an attention backend widely used in\nmodern LLM serving system to accelerate attention computation. It avoids writing attention weights\nto HBM through kernel fusion, thus significantly reducing data movement between on-chip memory', 'The proof is in Appendix C.\n21B.5 Comparison with Rabe and Staats [66]\nWe describe here some similarities and diï¬€erences between our FlashAttention algorithm and the algorithm\nof Rabe and Staats [66].\nConceptually, both FlashAttention and Rabe and Staats [66]operate on blocks of the attention matrix\nusing the well-established technique of tiling (or softmax scaling) [ 51,60]. To reduce the memory footprint,']",0.0,0.0,0.0,0,0.06521739130434782,0.42857142857142855,0.11320754716981131,0.2407186814147413,0.20545073375262055,0.275609756097561,10,5.598323518359448e-05,0.0,0.0,0.0,0.0
What problem does the Model Context Protocol (MCP) solve?,single,MCP (Model Context Protocol) Connecting (N) LLMs to (M) external tools/resources used to be a NxM problem MCP standardizes the LLM-tool communication into a N->1->M process Build with a client-server model MCP client: the agent that needs to call tool/data MCP server: a service to expose external tools and data sources,1,7,"['https://lilianweng.github.io/posts/2023-06-23-agent/\nMCP (Model Context Protocol)â—Connecting (N) LLMs to (M) external tools/resources used to be a NxM problem â—MCP standardizes the LLM-tool communication into a N->1->M process â—Build with a client-server model â—MCP client: the agent that needs to call tool/data â—MCP server: a service to expose external tools and data sources', 'LLMs + training for tool use: ToolformerMCP (Model Context Protocol)â—Connecting (N) LLMs to (M) external tools/resources used to be a NxM problem â—MCP standardizes the LLM-tool communication into a N->1->M process â—Build with a client-server model â—MCP client: the agent that needs to call tool/data â—MCP server: a service to expose external tools and data sources', 'superior performance. (2) ETP in the MoE layer introduces substantially higher communication\noverhead compared to EP, with this effect being particularly pronounced in fine-grained MoE models.\n(3) Fine-grained MoE models exhibit notably lower computation-to-communication ratios. When\nETPxEP exceeds 8, necessitating inter-node communication, communication overhead dominates,\nFigure 4: Context-scaling experiments by increasing context length and number of GPUs up to 128K\nand 1024.\n9(a) Mixtral 8x22B model', 'AI Agent/Workflow Frameworksâ—Frameworks initially proposed to standardize AI workflows, provide some out-of-box design patterns and abstractions â—Some examples â—LangChain: Came out the earliest, probably the most popular and hardest to use â—LlamaIndex: Good RAG support â—CrewAI and Camel: multi-agent framework for more complex tasks â—But a lot of unnecessary, added complexity for agents, harder to customize â—My experience of whatâ€™s the easiest and sufficient for many tasks â—No framework (pure Python) â—No MCP (can just write your own functions or hooks) â—No A2A (no need for multi-agent)What is AI Agent Infra?â—Agent testing and evaluation â—‹Unit + e2e test, metrics, benchmarks, human-in-the-loop â—Agent autotuning and optimization â—‹Automated prompt tuning, model selection, tool selection, workflow optimization â—Agent hosting â—‹Serverless or long-running?  â—‹Stateful or stateless? â—Tooling, memory, dataDemo Time: Eigent Computer-Use Agent performing a Discord summarization taskOutlineâ—Transformer primer â—Introduction oriented for LLM infra (perf problems), not the theory â—LLM performanceSelf Attention', '(b)Backward Computation.\nFigure 11: Communication Conflict\nmunication, conflicts caused by asynchronous communica-\ntion between EP and DP/PP can be reduced by setting priori-\nties for different communication primitives: EPÂ¿PPÂ¿CPÂ¿DP,\nensuring the efficiency of EP AllToAll communication.\nCluster Expansion\nThis section analyzes the cluster expansion handling meth-\nods for long context MoE training in MoNTA implemen-\ntations. In the training of the MoE model, it typically in-', 'Equipping Agents: The Power of Toolingâ€¢Tools: external functions, APIs, or even another (utility) agent (e.g., MCP server)\nâ€¢Agents can often decide when to call tools and what tools to call\nâ€¢Common tools\nâ€¢Web search + crawling\nâ€¢Browser\nâ€¢Social media, email hooks\nâ€¢Code + CLI executionAgent Memory: Knowledge, History, Stateâ€¢LLMs only have short-term memory (i.e., context window)\nâ€¢Many agents needs long-term memory and/or internal/external knowledge', 'AI Agent/Workflow Frameworksâ—Frameworks initially proposed to standardize AI workflows, provide some out-of-box design patterns and abstractions â—Some examples â—LangChain: Came out the earliest, probably the most popular and hardest to use â—LlamaIndex: Good RAG support â—CrewAI and Camel: multi-agent framework for more complex tasks â—But a lot of unnecessary, added complexity for agents, harder to customize â—My experience of whatâ€™s the easiest and sufficient for many tasks â—No framework (pure Python) â—No MCP (can just write your own functions or hooks) â—No A2A (no need for multi-agent)What is AI Agent Infra?â—Agent testing and evaluation â—‹Unit + e2e test, metrics, benchmarks, human-in-the-loop â—Agent autotuning and optimization â—‹Automated prompt tuning, model selection, tool selection, workflow optimization â—Agent hosting â—‹Serverless or long-running?  â—‹Stateful or stateless? â—Tooling, memory, dataDemo Time: Eigent Computer-Use Agent performing a Discord summarization task', 'the work handle h, which MCR-DL uses internally to wait on\nthe prior event e.\nThis scheme is similar to PyTorchâ€™s distributed module, but\nthere are a few key implementation details that enable greater\nperformance: (1): The use of multiple streams enables con-\ncurrent small-message operations (concurrent large-message\noperations are bandwidth-bound and show no beneï¬t), (2):\nInstead of having an overall communication stream, each back-\nend contains its own stream for overlap across backends. This', 'Efficiency is the Bottleneck for Modeling Long Sequences with Attention\nHow to efficiently scale models  to longer sequences?\n3Context length: how many other \nelements in the sequence does \nthe current element interact with.\n2xâ†“Increasing context length slows down (or stops) trainingBackground: Attention is the Heart of Transformers\n4\nBackground: Attention Mechanism\nO = Softmax (QKT)V\n5Q\n(N x d)K\n(N x d)\nxV\n(N x d)\nxO\n(N x d)\n=\nQuery Key Similarity \nScoreAttention prob \n= row -wise normalized ', 'https://lilianweng.github.io/posts/2023-06-23-agent/\nEquipping Agents: The Power of Toolingâ—Tools: external functions, APIs, or even another (utility) agent (e.g., MCP server) â—Agents can often decide when to call tools and what tools to call â—Common tools â—‹Web search + crawling â—‹Browser â—‹Social media, email hooks â—‹Code + CLI executionhttps://lilianweng.github.io/posts/2023-06-23-agent/']",1.0,0.2,0.33333333333333337,1,0.08,1.0,0.14814814814814814,0.29710978428651835,0.09828674481514878,0.8763736263736264,10,0.046695363832149415,1.0,1.0,0.6666666666666666,1.0
What search algorithm does AlphaZero use instead of alpha-beta search?,single,"Instead of an alpha-beta search with domain-speciï¬c enhancements, AlphaZero uses a general-purpose Monte-Carlo tree search (MCTS) algorithm. Each search consists of a series of simulated games of self-play that traverse a tree from root srootto leaf. Each simulation proceeds by selecting in each state sa moveawith low visit count, high move probability and high value (averaged over the leaf states of simulations that selected afroms) according to the current neural network.",1,1,"['v\x19E[zjs].AlphaZero learns these move probabilities and value estimates entirely from self-\nplay; these are then used to guide its search.\nInstead of an alpha-beta search with domain-speciï¬c enhancements, AlphaZero uses a general-\npurpose Monte-Carlo tree search (MCTS) algorithm. Each search consists of a series of simu-\nlated games of self-play that traverse a tree from root srootto leaf. Each simulation proceeds by\nselecting in each state sa moveawith low visit count, high move probability and high value', 'out when evaluating a large subtree. In contrast, alpha-beta search computes an explicit mini-\nmax, which propagates the biggest approximation errors to the root of the subtree. Using MCTS\nmay allow AlphaZero to effectively combine its neural network representations with a powerful,\ndomain-independent search.\nDomain Knowledge\n1. The input features describing the position, and the output features describing the move,\nare structured as a set of planes; i.e. the neural network architecture is matched to the', 'An approach based on training dual policy and value networks using AlphaZero -like policy\niteration was successfully applied to improve on the state-of-the-art in Hex ( 3).\n11MCTS and Alpha-Beta Search\nFor at least four decades the strongest computer chess programs have used alpha-beta search\n(18, 23 ).AlphaZero uses a markedly different approach that averages over the position evalu-\nations within a subtree, rather than computing the minimax evaluation of that subtree. How-', '(see Table 1).\nWe also analysed the relative performance of AlphaZero â€™s MCTS search compared to the\nstate-of-the-art alpha-beta search engines used by Stockï¬sh andElmo .AlphaZero searches just\n80 thousand positions per second in chess and 40 thousand in shogi, compared to 70 million\nforStockï¬sh and 35 million for Elmo .AlphaZero compensates for the lower number of evalu-\nations by using its deep neural network to focus much more selectively on the most promising', 'ations within a subtree, rather than computing the minimax evaluation of that subtree. How-\never, chess programs using traditional MCTS were much weaker than alpha-beta search pro-\ngrams, ( 4, 24 ); while alpha-beta programs based on neural networks have previously been un-\nable to compete with faster, handcrafted evaluation functions.\nAlphaZero evaluates positions using non-linear function approximation based on a deep', 'AlphaZero evaluates positions using non-linear function approximation based on a deep\nneural network, rather than the linear function approximation used in typical chess programs.\nThis provides a much more powerful representation, but may also introduce spurious approxi-\nmation errors. MCTS averages over these approximation errors, which therefore tend to cancel\nout when evaluating a large subtree. In contrast, alpha-beta search computes an explicit mini-', 'and a tabula rasa reinforcement learning algorithm.\nInstead of a handcrafted evaluation function and move ordering heuristics, AlphaZero utilises\na deep neural network (p;v) =f\x12(s)with parameters \x12. This neural network takes the board po-\nsitionsas an input and outputs a vector of move probabilities pwith components pa=Pr(ajs)\n2for each action a, and a scalar value vestimating the expected outcome zfrom position s,\nv\x19E[zjs].AlphaZero learns these move probabilities and value estimates entirely from self-', 'losses respectively,\n(p;v) =f\x12(s); l = (z\x00v)2\x00\x19\x19\x19>logp+cjj\x12jj2(1)\nwherecis a parameter controlling the level of L2weight regularisation. The updated parameters\nare used in subsequent games of self-play.\nThe AlphaZero algorithm described in this paper differs from the original AlphaGo Zero\nalgorithm in several respects.\nAlphaGo Zero estimates and optimises the probability of winning, assuming binary win/loss\noutcomes. AlphaZero instead estimates and optimises the expected outcome, taking account of', 'the-art programs are based on powerful engines that search many millions of positions, leverag-\ning handcrafted domain expertise and sophisticated domain adaptations. AlphaZero is a generic\nreinforcement learning algorithm â€“ originally devised for the game of Go â€“ that achieved su-\nperior results within a few hours, searching a thousand times fewer positions, given no domain\n3The prevalence of draws in high-level chess tends to compress the Elo scale, compared to shogi or Go.', 'ons ( 5). These programs use a similar algorithm to computer chess programs, again based on a\nhighly optimised alpha-beta search engine with many domain-speciï¬c adaptations.\nGo is well suited to the neural network architecture used in AlphaGo because the rules of\nthe game are translationally invariant (matching the weight sharing structure of convolutional\nnetworks), are deï¬ned in terms of liberties corresponding to the adjacencies between points']",1.0,0.1,0.18181818181818182,1,0.08571428571428572,1.0,0.15789473684210528,0.27867317487152155,0.09803921568627451,0.7235294117647059,10,0.07259078172285663,1.0,1.0,0.3333333333333333,1.0
Why can cProfile and the CPU-mode autograd profiler give incorrect timings on CUDA code?,single,"Due to the asynchronous nature of CUDA kernels, when running against CUDA code, the cProfile output and CPU-mode autograd profilers may not show correct timings: the reported CPU time includes only the time to launch kernels but not their GPU execution time unless synchronization occurs.",1,4,"['torch.utils.bottleneck  -h for more usage instructions.\nBecause your script will be profiled, please ensure that it exits in a finite amount of time.\nDue to the asynchronous nature of CUDA kernels, when running against CUDA code, the\ncProfile output and CPU-mode autograd profilers may not show correct timings: the\nreported CPU time reports the amount of time used to launch the kernels but does not\ninclude the time the kernel spent executing on a GPU unless the operation does a', 'include the time the kernel spent executing on a GPU unless the operation does a\nsynchronize. Ops that do synchronize appear to be extremely expensive under regular\nCPU-mode profilers. In these case where timings are incorrect, the CUDA-mode autograd\nprofiler may be helpful.python  -m  torch .utils .bottleneck  /path /to /source /script .py  [args]\nWarning âš \nWarning âš \nTo analyze traffic and optimize your experience, we serve cookies on this site. By clicking or', 'Similarly, IntelÂ® VTuneâ„¢ Profiler  helps to analyze performance on Intel platforms\nfurther with torch.autograd.profiler.emit_itt() .\nIf you are profiling CUDA code, the first profiler that bottleneck  runs (cProfile) will\ninclude the CUDA startup time (CUDA buffer allocation cost) in its time reporting. This\nshould not matter if your bottlenecks result in code much slower than the CUDA startup\ntime.\nFor more complicated uses of the profilers (like in a multi-GPU case), please see', 'should first check if your script is CPU-bound (â€œCPU total time is much greater than CUDA\ntotal timeâ€). If it is CPU-bound, looking at the results of the CPU-mode autograd profiler\nwill help. If on the other hand your script spends most of its time executing on the GPU,\nthen it makes sense to start looking for responsible CUDA operators in the output of the\nCUDA-mode autograd profiler.\nOf course the reality is much more complicated and your script might not be in one of', 'Consequently, if any checkpointed functions involve randomness, this may result in\nincorrect gradients. (Note that if CUDA devices are among the devices detected, it will be\nprioritized; otherwise, the first device encountered will be selected.) If there are no CPU-\ntensors, the default device type state (default value is cuda, and it could be set to other\ndevice by DefaultDeviceType) will be saved and restored. However, the logic has no way', 'To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or\nnavigating, you agree to allow our usage of cookies. As the current maintainers of this site,\nFacebookÊ¼s Cookies Policy applies. Learn more, including about available controls: Cookies\nPolicy.10/10/25, 3:03 PM torch.utils.bottleneck â€” PyTorch 2.8 documentation\nhttps://docs.pytorch.org/docs/stable/bottleneck.html 1/3To decide which (CPU-only-mode or CUDA-mode) autograd profiler output to look at, you', 'fast CPU thread aggressively allocates GPU memory blocks and\ncauses defragmentations. If it is difficult to identify with certainty\nfrom latency measurements or profiled traces, CUDA malloc retry\ncan serve as a helpful indicator, which can be obtained from the\nnum_alloc_retries key in the torch.cuda.memory_stats() dictionary.\nThe experiments conducted with T5 models have demonstrated\nthat the rate limiter technique can greatly benefit training efficiency,611M 2.28B 11.3B', 'down. The next feature in line will be autograd that will properly create the autograd graph and then\nredispatch down. Finally, we reach the backend kernel for CUDA which will launch the right CUDA\nkernel and return the final result. On the way out, autograd will attach the graph to the output and,\nfinally, autocast will have a chance to do any update it needs on exit.\nOne configuration of the dispatcher is the order in which all these feature and backend keys are', 'Of course the reality is much more complicated and your script might not be in one of\nthose two extremes depending on the part of the model youÊ¼re evaluating. If the profiler\noutputs donÊ¼t help, you could try looking at the result of\ntorch.autograd.profiler.emit_nvtx()  with nvprof . However, please take into\naccount that the NVTX overhead is very high and often gives a heavily skewed timeline.\nSimilarly, IntelÂ® VTuneâ„¢ Profiler  helps to analyze performance on Intel platforms', 'Specifically, the caching allocator requests CUDA memory blocks\nand internally determines how to split and reuse the blocks without\nreturning them to CUDA with the goal being to reach a steady state\nwithout further calls to cudaMalloc and cudaFree .\nThe caching allocator runs from the CPU thread, meaning that it\nmust decide which caching allocator block to use for an allocation\nwhen the CPU thread processes the allocation request. It cannot\nwait until the GPU kernel needing the allocation actually runs,']",1.0,0.1,0.18181818181818182,1,0.08108108108108109,1.0,0.15,0.2373787731364812,0.15730337078651685,0.5333333333333333,10,0.04038893010681695,1.0,1.0,0.3333333333333333,1.0
Why canâ€™t you perform data-dependent operations on meta tensors?,single,"Because meta tensors do not have real data, you cannot perform data-dependent operations like torch.nonzero()  or item() . In some cases, not all device types (e.g., CPU and CUDA) have exactly the same output metadata for an operation",1,5,"['represent the actual tensors. Because meta tensors do not have real data, you cannot perform\ndata-dependent operations like torch.nonzero()  or item() . In some cases, not all device\ntypes (e.g., CPU and CUDA) have exactly the same output metadata for an operation; we\ntypically prefer representing the CUDA behavior faithfully in this situation.\nAlthough in principle meta tensor computation should always be faster than an equivalent', 'make transformations on the model before you load the actual data.\nMost operations can be performed on meta tensors, producing new meta tensors that describe\nwhat the result would have been if you performed the operation on a real tensor. You can use\nthis to perform abstract analysis without needing to spend time on compute or space to\nrepresent the actual tensors. Because meta tensors do not have real data, you cannot perform', 'a device for initialization:\nYou cannot convert a meta tensor directly to a CPU/CUDA tensor, because the meta tensor stores\nno data and we do not know what the correct data values for your new tensor are:\nUse a factory function like torch.empty_like()  to explicitly specify how you would like the\nmissing data to be filled in.\nNN modules have a convenience method torch.nn.Module.to_empty()  that allows you to move', 'Meta device\nCreated On: Jun 17, 2025 | Last Updated On: Jun 17, 2025\nThe â€œmetaâ€ device is an abstract device which denotes a tensor which records only metadata, but\nno actual data. Meta tensors have two primary use cases:\nModels can be loaded on the meta device, allowing you to load a representation of the model\nwithout actually loading the actual parameters into memory. This can be helpful if you need to\nmake transformations on the model before you load the actual data.', 'ing a rich set of data manipulation operations. Every Tensor object\nhas an associated storage that is allocated on a specific device.\nWhen Tensor s only represent simple transformations such as reshape\nand split , they can share the same underlying storage. Each Module\ndescribes a transformation from input to output values, and its\nbehavior during the forward pass is specified by its forward member\nfunction. Such a module may feature Tensor objects as parameters,', ""Although in principle meta tensor computation should always be faster than an equivalent\nCPU/CUDA computation, many meta tensor implementations are implemented in Python\nand have not been ported to C++ for speed, so you may find that you get lower absolute\nframework latency with small CPU tensors.\nIdioms for working with meta tensors\nAn object can be loaded with torch.load()  onto meta device by specifying\nmap_location='meta' :\n>>> torch.save(torch.randn(2), 'foo.pt' )"", 'Policy.10/10/25, 3:03 PM Meta device â€” PyTorch 2.8 documentation\nhttps://docs.pytorch.org/docs/stable/meta.html 1/4If you have some arbitrary code which performs some tensor construction without explicitly\nspecifying a device, you can override it to instead construct on meta device by using the\ntorch.device()  context manager:\nThis is especially helpful NN module construction, where you often are not able to explicitly pass in\na device for initialization:', 'torch\nCreated On: Dec 23, 2016 | Last Updated On: Mar 10, 2025\nThe torch package contains data structures for multi-dimensional tensors and defines mathematic\noperations over these tensors. Additionally, it provides many utilities for efficient serialization of\nTensors and arbitrary types, and other useful utilities.\nIt has a CUDA counterpart, that enables you to run your tensor computations on an NVIDIA GPU\nwith compute capability >= 3.0.\nTensors\nis_tensorReturns True if obj is a PyTorch tensor.', 'data structures and will only consider tensors that are direct arguments to the call. You can\nreturn either a single Tensor output, or a tuple of tensors if there are multiple outputs. Also\nplease refer to the docs of Function  to find descriptions of useful methods that can be called\nonly from forward().\nsetup_context()  (optional). One can either write a â€œcombinedâ€ forward()  that accepts a', 'has thrown an error, this value() method will also throw an error.\nReturn type:\nT\nwait ( )\nBlock until the value of this Future is ready.\nIf the value contains tensors that reside on GPUs, then an additional synchronization is\nperformed with the kernels (executing on the device) which may be asynchronously\npopulating those tensors. Such sync is non-blocking, which means that wait() will inser\nthe necessary instructions in the current streams to ensure that further operations']",1.0,0.2,0.33333333333333337,1,0.08823529411764706,1.0,0.1621621621621622,0.2007771663086802,0.06638115631691649,0.5368663594470046,10,0.05001046998936093,1.0,1.0,0.3333333333333333,1.0
