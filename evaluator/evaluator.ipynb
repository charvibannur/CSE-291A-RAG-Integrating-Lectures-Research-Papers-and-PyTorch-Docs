{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "nIftlvy53PxN"
      },
      "outputs": [],
      "source": [
        "# evaluator_textonly_fixed.py\n",
        "\n",
        "import os\n",
        "import json\n",
        "import ast\n",
        "import re\n",
        "import string\n",
        "import unicodedata\n",
        "from math import log2\n",
        "from typing import List, Dict, Tuple\n",
        "import difflib\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# -------------------- utilities --------------------\n",
        "\n",
        "def _exists_any(paths: List[str]) -> str:\n",
        "    for p in paths:\n",
        "        if p and os.path.exists(p):\n",
        "            return p\n",
        "    return \"\"\n",
        "\n",
        "def _pick_col(df: pd.DataFrame, candidates: List[str]) -> str:\n",
        "    for c in candidates:\n",
        "        if c in df.columns:\n",
        "            return c\n",
        "    raise KeyError(f\"None of the columns {candidates} found. Available: {list(df.columns)}\")\n",
        "\n",
        "# -------------------- evaluator --------------------\n",
        "\n",
        "class TextOnlyRAGEvaluator:\n",
        "    \"\"\"\n",
        "    Text-only evaluator for RAG retrieval where gold supervision is *only* gold-text snippets.\n",
        "\n",
        "    Ground truth CSVs (both are used and merged by query):\n",
        "      - multi_file_retrieval_queries.csv\n",
        "      - single_file_retrieval_queries.csv  (or singlefile_retrieval_queries.csv)\n",
        "\n",
        "    Required columns (auto-detected names):\n",
        "      - query\n",
        "      - gold-text (or gold_text / gold / goldtext)  -> stored as raw_gold-text in outputs\n",
        "\n",
        "    Retrieval results JSON:\n",
        "      - rag_results.json with per-query 'results[*].match' (or 'text') and optional 'filename'.\n",
        "    \"\"\"\n",
        "\n",
        "    _SENT_SPLIT = re.compile(r'(?<=[\\.\\?\\!])\\s+')\n",
        "    _STOP = set(\"\"\"\n",
        "        a an the and or of in on to for from with as at by is are was were be been being\n",
        "        this that these those it its their his her we you they i vs v et al\n",
        "    \"\"\".split())\n",
        "    _EXT_RE = re.compile(r'\\.(pdf|txt|md|docx|pptx|html|htm|csv|json)$', re.I)\n",
        "\n",
        "    # ---------------- canonicalization ----------------\n",
        "    def _canon_query(self, s: str) -> str:\n",
        "        _PUNCT_TO_STRIP = string.punctuation + \"“”‘’´`•·•–—-\"  # add common unicode punct/dashes\n",
        "        _ZWSP_LIKE = \"\".join([\n",
        "            \"\\u200b\",  # zero width space\n",
        "            \"\\u200c\",  # zero width non-joiner\n",
        "            \"\\u200d\",  # zero width joiner\n",
        "            \"\\ufeff\",  # zero width no-break space (BOM)\n",
        "            \"\\u00a0\",  # non-breaking space\n",
        "        ])\n",
        "        # 1) Unicode normalize\n",
        "        s = unicodedata.normalize(\"NFKC\", str(s))\n",
        "\n",
        "        # 2) Remove zero-width / NBSPs\n",
        "        s = s.translate({ord(c): None for c in _ZWSP_LIKE})\n",
        "\n",
        "        # 3) Normalize fancy quotes/dashes first, then strip all punctuation\n",
        "        s = (s.replace(\"“\", '\"').replace(\"”\", '\"')\n",
        "              .replace(\"‘\", \"'\").replace(\"’\", \"'\")\n",
        "              .replace(\"–\", \"-\").replace(\"—\", \"-\").replace(\"-\", \"-\"))\n",
        "        s = s.translate(str.maketrans(\"\", \"\", _PUNCT_TO_STRIP))\n",
        "\n",
        "        # 4) Lowercase and collapse whitespace\n",
        "        s = s.lower()\n",
        "        s = re.sub(r\"\\s+\", \" \", s).strip()\n",
        "        return s\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        multi_csv: str,\n",
        "        single_csv: str,\n",
        "        results_json: str,\n",
        "        out_dir: str = \"./eval_out\",\n",
        "        topk: List[int] = (1, 3, 5, 10),\n",
        "        query_col_candidates: List[str] = (\"query\", \"question\"),\n",
        "        gold_text_col_candidates: List[str] = (\"gold-text\", \"gold_text\", \"gold\", \"goldtext\"),\n",
        "    ):\n",
        "        # Allow path variants for single_csv\n",
        "        single_csv = _exists_any([single_csv, single_csv.replace(\"single_file_\", \"singlefile_\"),\n",
        "                                  single_csv.replace(\"singlefile_\", \"single_file_\")]) or single_csv\n",
        "\n",
        "        if not os.path.exists(multi_csv):\n",
        "            raise FileNotFoundError(f\"multi_csv not found: {multi_csv}\")\n",
        "        if not os.path.exists(single_csv):\n",
        "            print(f\"[warn] single_csv not found: {single_csv} — continuing with multi only.\")\n",
        "\n",
        "        self.multi_csv = multi_csv\n",
        "        self.single_csv = single_csv if os.path.exists(single_csv) else \"\"\n",
        "        self.results_json = results_json\n",
        "        self.out_dir = out_dir\n",
        "        self.topk = list(topk)\n",
        "\n",
        "        os.makedirs(self.out_dir, exist_ok=True)\n",
        "\n",
        "        # Load ground truth CSVs\n",
        "        self.multi_df = pd.read_csv(self.multi_csv)\n",
        "        if self.single_csv:\n",
        "            self.single_df = pd.read_csv(self.single_csv)\n",
        "        else:\n",
        "            self.single_df = pd.DataFrame(columns=list(self.multi_df.columns))\n",
        "\n",
        "        # Detect column names\n",
        "        self.query_col = _pick_col(self.multi_df if len(self.multi_df) else self.single_df, list(query_col_candidates))\n",
        "        self.gold_text_col = _pick_col(self.multi_df if len(self.multi_df) else self.single_df, list(gold_text_col_candidates))\n",
        "\n",
        "        # If the other split uses different casing, fix too\n",
        "        for df in (self.multi_df, self.single_df):\n",
        "            if self.query_col not in df.columns and len(df.columns):\n",
        "                alt = _pick_col(df, list(query_col_candidates))\n",
        "                df.rename(columns={alt: self.query_col}, inplace=True)\n",
        "            if self.gold_text_col not in df.columns and len(df.columns):\n",
        "                alt = _pick_col(df, list(gold_text_col_candidates))\n",
        "                df.rename(columns={alt: self.gold_text_col}, inplace=True)\n",
        "\n",
        "        # Load results\n",
        "        with open(self.results_json, \"r\", encoding=\"utf-8\") as f:\n",
        "            self.results_data = json.load(f)\n",
        "\n",
        "        # Build gold lookup and query->split map (keys are canonicalized)\n",
        "        self.gold_lookup, self.query_split, self.display_lookup = self._build_gold_textonly()\n",
        "\n",
        "        # Build results map (canonicalized) and merge display names\n",
        "        self.results_map, results_display = self._build_results_map()\n",
        "        # Prefer gold display text when available; otherwise use results' original text\n",
        "        for k, disp in results_display.items():\n",
        "            self.display_lookup.setdefault(k, disp)\n",
        "\n",
        "    # --------------- parsing helpers ---------------\n",
        "    def _chunk_overlap_ratio(self, rt: str, gold_texts: List[str]) -> float:\n",
        "        \"\"\"\n",
        "        Portion of a retrieved chunk's characters that overlap with the BEST-matching gold snippet.\n",
        "        Uses character-level alignment (SequenceMatcher) on normalized strings.\n",
        "        Returns a value in [0, 1].\n",
        "        \"\"\"\n",
        "        rt_n = self._normalize_text(rt)\n",
        "        if not rt_n:\n",
        "            return 0.0\n",
        "\n",
        "        best = 0.0\n",
        "        for gt in gold_texts:\n",
        "            gt_n = self._normalize_text(gt)\n",
        "            if not gt_n:\n",
        "                continue\n",
        "            sm = difflib.SequenceMatcher(None, rt_n, gt_n, autojunk=False)\n",
        "            # Sum sizes for all exact-matching blocks\n",
        "            equal_chars = sum(m.size for m in sm.get_matching_blocks())\n",
        "            # Note: get_matching_blocks() ends with a size-0 sentinel—adds 0, safe to include.\n",
        "            ratio = equal_chars / max(1, len(rt_n))\n",
        "            if ratio > best:\n",
        "                best = ratio\n",
        "        return best\n",
        "\n",
        "\n",
        "    def chunk_overlap_metrics(self, retrieved_texts: List[str], gold_texts: List[str]) -> Dict[str, float]:\n",
        "        \"\"\"\n",
        "        Computes the average per-chunk overlap ratio across all retrieved chunks for a query.\n",
        "        Returns:\n",
        "          - chunk_overlap: mean fraction of retrieved chunk content that overlaps with gold\n",
        "          - min_chunk_overlap / max_chunk_overlap: extremes across retrieved chunks\n",
        "          - chunk_overlap_count: number of retrieved chunks considered\n",
        "        \"\"\"\n",
        "        r_clean = [r for r in retrieved_texts if str(r).strip()]\n",
        "        g_clean = [g for g in gold_texts if str(g).strip()]\n",
        "\n",
        "        if not r_clean:\n",
        "            return {\n",
        "                'chunk_overlap': 0.0,\n",
        "                'min_chunk_overlap': 0.0,\n",
        "                'max_chunk_overlap': 0.0,\n",
        "                'chunk_overlap_count': 0,\n",
        "            }\n",
        "\n",
        "        overlaps = [self._chunk_overlap_ratio(rt, g_clean) for rt in r_clean]\n",
        "        return {\n",
        "            'chunk_overlap': float(np.mean(overlaps)) if overlaps else 0.0,\n",
        "            'min_chunk_overlap': float(np.min(overlaps)) if overlaps else 0.0,\n",
        "            'max_chunk_overlap': float(np.max(overlaps)) if overlaps else 0.0,\n",
        "            'chunk_overlap_count': int(len(overlaps)),\n",
        "        }\n",
        "\n",
        "\n",
        "    def _parse_gold_texts(self, v) -> List[str]:\n",
        "        \"\"\"\n",
        "        Parse gold-text into a list. We do NOT split on commas.\n",
        "        Accepts: Python/JSON list/tuple, or '||' / ';' delimiters; else a single string.\n",
        "        \"\"\"\n",
        "        if v is None or (isinstance(v, float) and np.isnan(v)):\n",
        "            return []\n",
        "        s = str(v).strip()\n",
        "        if not s:\n",
        "            return []\n",
        "        # Python/JSON list-like\n",
        "        if (s.startswith('[') and s.endswith(']')) or (s.startswith('(') and s.endswith(')')):\n",
        "            try:\n",
        "                parsed = ast.literal_eval(s)\n",
        "                if isinstance(parsed, (list, tuple)):\n",
        "                    return [str(x) for x in parsed if str(x).strip()]\n",
        "                return [str(parsed)]\n",
        "            except Exception:\n",
        "                pass\n",
        "        # Explicit delimiters\n",
        "        if '||' in s:\n",
        "            return [t.strip() for t in s.split('||') if t.strip()]\n",
        "        if ';' in s:\n",
        "            return [t.strip() for t in s.split(';') if t.strip()]\n",
        "        return [s]\n",
        "\n",
        "    def _build_gold_for_df(self, df: pd.DataFrame) -> Tuple[Dict[str, Dict], Dict[str, str]]:\n",
        "        \"\"\"\n",
        "        Returns:\n",
        "          out: canon_query -> {\"texts\": [...], \"raw_text\": str}\n",
        "          display: canon_query -> first seen original query (for pretty printing)\n",
        "        \"\"\"\n",
        "        out: Dict[str, Dict] = {}\n",
        "        display: Dict[str, str] = {}\n",
        "        for _, row in df.iterrows():\n",
        "            q_orig = str(row[self.query_col])\n",
        "            q = self._canon_query(q_orig)\n",
        "            raw_txt = \"\" if row.get(self.gold_text_col) is None else str(row.get(self.gold_text_col))\n",
        "            texts = self._parse_gold_texts(row.get(self.gold_text_col, \"\"))\n",
        "\n",
        "            if q not in out:\n",
        "                out[q] = {\"texts\": [], \"raw_text\": raw_txt}\n",
        "                display[q] = q_orig  # preserve a human-readable version\n",
        "            else:\n",
        "                if not out[q].get(\"raw_text\"):\n",
        "                    out[q][\"raw_text\"] = raw_txt\n",
        "\n",
        "            out[q][\"texts\"].extend(texts)\n",
        "\n",
        "        # Dedup texts while preserving order\n",
        "        for qk in out:\n",
        "            seen, uniq = set(), []\n",
        "            for t in out[qk][\"texts\"]:\n",
        "                if t not in seen:\n",
        "                    seen.add(t)\n",
        "                    uniq.append(t)\n",
        "            out[qk][\"texts\"] = uniq\n",
        "        return out, display\n",
        "\n",
        "    def _build_gold_textonly(self) -> Tuple[Dict[str, Dict], Dict[str, str], Dict[str, str]]:\n",
        "        gold: Dict[str, Dict] = {}\n",
        "        split_map: Dict[str, str] = {}\n",
        "        display: Dict[str, str] = {}\n",
        "\n",
        "        multi, multi_disp = self._build_gold_for_df(self.multi_df)\n",
        "        single, single_disp = self._build_gold_for_df(self.single_df)\n",
        "\n",
        "        for q, v in multi.items():\n",
        "            gold[q] = {\"texts\": list(v[\"texts\"]), \"raw_text\": v.get(\"raw_text\", \"\")}\n",
        "            split_map[q] = \"multi\"\n",
        "            display[q] = multi_disp.get(q, q)\n",
        "\n",
        "        for q, v in single.items():\n",
        "            if q in gold:\n",
        "                # merge texts (dedup)\n",
        "                for t in v[\"texts\"]:\n",
        "                    if t not in gold[q][\"texts\"]:\n",
        "                        gold[q][\"texts\"].append(t)\n",
        "                if not gold[q].get(\"raw_text\"):\n",
        "                    gold[q][\"raw_text\"] = v.get(\"raw_text\", \"\")\n",
        "                split_map[q] = \"both\"\n",
        "                # keep existing display (prefer multi), or set if missing\n",
        "                display.setdefault(q, single_disp.get(q, q))\n",
        "            else:\n",
        "                gold[q] = {\"texts\": list(v[\"texts\"]), \"raw_text\": v.get(\"raw_text\", \"\")}\n",
        "                split_map[q] = \"single\"\n",
        "                display[q] = single_disp.get(q, q)\n",
        "\n",
        "        return gold, split_map, display\n",
        "\n",
        "    # --------------- results mapping ---------------\n",
        "\n",
        "    def _build_results_map(self) -> Tuple[Dict[str, Dict[str, List[str]]], Dict[str, str]]:\n",
        "        mapping: Dict[str, Dict[str, List[str]]] = {}\n",
        "        display: Dict[str, str] = {}\n",
        "        for item in self.results_data.get(\"queries\", []):\n",
        "            q_orig = str(item.get(\"query\", \"\"))\n",
        "            q = self._canon_query(q_orig)\n",
        "            docs, texts = [], []\n",
        "            for r in item.get(\"results\", []):\n",
        "                fn = r.get(\"filename\") or r.get(\"doc\") or r.get(\"document\") or r.get(\"id\") or \"\"\n",
        "                txt = r.get(\"match\") or r.get(\"text\") or \"\"\n",
        "                if fn:\n",
        "                    docs.append(str(fn))\n",
        "                if txt is not None and str(txt).strip():\n",
        "                    texts.append(str(txt))\n",
        "            # Dedup docs keeping order\n",
        "            seen, uniq_docs = set(), []\n",
        "            for d in docs:\n",
        "                if d not in seen:\n",
        "                    seen.add(d)\n",
        "                    uniq_docs.append(d)\n",
        "            mapping[q] = {\"docs\": uniq_docs, \"texts\": texts}\n",
        "            display.setdefault(q, q_orig)\n",
        "        return mapping, display\n",
        "\n",
        "    # --------------- normalization / tokenization ---------------\n",
        "\n",
        "    def _normalize_text(self, s: str) -> str:\n",
        "        return ' '.join(str(s).lower().split())\n",
        "\n",
        "    def _tok(self, text: str) -> List[str]:\n",
        "        t = str(text).lower()\n",
        "        t = t.translate(str.maketrans('', '', string.punctuation))\n",
        "        toks = [w for w in t.split() if w and w not in self._STOP]\n",
        "        return toks\n",
        "\n",
        "    def _tok_all(self, text: str) -> List[str]:\n",
        "        \"\"\"Tokenizer for BLEU (keeps stopwords).\"\"\"\n",
        "        t = str(text).lower()\n",
        "        t = t.translate(str.maketrans('', '', string.punctuation))\n",
        "        toks = [w for w in t.split() if w]\n",
        "        return toks\n",
        "\n",
        "    def _split_sents(self, text: str) -> List[str]:\n",
        "        return [s.strip() for s in self._SENT_SPLIT.split(str(text).strip()) if s.strip()]\n",
        "\n",
        "    # --------------- BLEU (corpus-style over concatenated texts) ---------------\n",
        "\n",
        "    def _count_ngrams(self, tokens: List[str], n: int) -> Dict[Tuple[str, ...], int]:\n",
        "        counts: Dict[Tuple[str, ...], int] = {}\n",
        "        if len(tokens) < n:\n",
        "            return counts\n",
        "        for i in range(len(tokens) - n + 1):\n",
        "            ng = tuple(tokens[i:i+n])\n",
        "            counts[ng] = counts.get(ng, 0) + 1\n",
        "        return counts\n",
        "\n",
        "    def bleu_score(self, retrieved_texts: List[str], gold_texts: List[str], max_n: int = 4, smoothing: float = 1e-9) -> float:\n",
        "        \"\"\"\n",
        "        Corpus BLEU between concatenated retrieved_texts (candidate) and gold_texts (reference).\n",
        "        - Uniform weights over n=1..max_n\n",
        "        - Modified precision with clipping\n",
        "        - Brevity penalty\n",
        "        - Light smoothing to avoid log(0)\n",
        "        \"\"\"\n",
        "        cand = \" \".join(str(x) for x in retrieved_texts if str(x).strip())\n",
        "        ref  = \" \".join(str(x) for x in gold_texts      if str(x).strip())\n",
        "        if not cand or not ref:\n",
        "            return 0.0\n",
        "\n",
        "        cand_tok = self._tok_all(cand)\n",
        "        ref_tok  = self._tok_all(ref)\n",
        "\n",
        "        c_len = len(cand_tok)\n",
        "        r_len = len(ref_tok)\n",
        "\n",
        "        precisions = []\n",
        "        for n in range(1, max_n + 1):\n",
        "            cand_counts = self._count_ngrams(cand_tok, n)\n",
        "            ref_counts  = self._count_ngrams(ref_tok, n)\n",
        "\n",
        "            if not cand_counts:\n",
        "                precisions.append(0.0)\n",
        "                continue\n",
        "\n",
        "            # clipped counts\n",
        "            match = 0\n",
        "            total = 0\n",
        "            for ng, cnt in cand_counts.items():\n",
        "                total += cnt\n",
        "                match += min(cnt, ref_counts.get(ng, 0))\n",
        "\n",
        "            # smoothing to avoid zero precision nuking the whole score\n",
        "            p_n = (match + smoothing) / (total + smoothing)\n",
        "            precisions.append(p_n)\n",
        "\n",
        "        # Brevity penalty\n",
        "        if c_len == 0:\n",
        "            return 0.0\n",
        "        bp = 1.0 if c_len > r_len else float(np.exp(1 - (r_len / max(1, c_len))))\n",
        "\n",
        "        # geometric mean of precisions\n",
        "        gm = float(np.exp(np.mean([np.log(p) for p in precisions])))\n",
        "\n",
        "        return bp * gm\n",
        "\n",
        "    # --------------- robust overlap (tiered) ---------------\n",
        "\n",
        "    def _substring_containment(self, a: str, b: str) -> float:\n",
        "        \"\"\"\n",
        "        Character containment of a in b, after normalization.\n",
        "        returns |a| / |b| if a is substring of b, else 0.\n",
        "        We check both directions externally.\n",
        "        \"\"\"\n",
        "        a_n = self._normalize_text(a)\n",
        "        b_n = self._normalize_text(b)\n",
        "        if not a_n or not b_n:\n",
        "            return 0.0\n",
        "        if a_n in b_n:\n",
        "            return len(a_n) / max(1, len(b_n))\n",
        "        return 0.0\n",
        "\n",
        "    def _char_ngram_containment(self, a: str, b: str, n: int = 5) -> float:\n",
        "        \"\"\"\n",
        "        Character n-gram containment: |ngrams(a) ∩ ngrams(b)| / |ngrams(a)|.\n",
        "        Helps when wording changes slightly.\n",
        "        \"\"\"\n",
        "        a_n = self._normalize_text(a)\n",
        "        b_n = self._normalize_text(b)\n",
        "        if len(a_n) < n or len(b_n) < n:\n",
        "            return 0.0\n",
        "        def grams(s): return {s[i:i+n] for i in range(len(s)-n+1)}\n",
        "        A, B = grams(a_n), grams(b_n)\n",
        "        if not A:\n",
        "            return 0.0\n",
        "        return len(A & B) / len(A)\n",
        "\n",
        "    def _token_jaccard(self, a: str, b: str) -> float:\n",
        "        A, B = set(self._tok(a)), set(self._tok(b))\n",
        "        if not A or not B:\n",
        "            return 0.0\n",
        "        return len(A & B) / len(A | B)\n",
        "\n",
        "    def _match_score(self, gold: str, cand: str) -> float:\n",
        "        \"\"\"\n",
        "        Tiered score in [0,1]: max over\n",
        "         - exact/substring containment (both directions)\n",
        "         - token Jaccard\n",
        "         - char n-gram containment\n",
        "        \"\"\"\n",
        "        # substring containment (both directions)\n",
        "        s1 = self._substring_containment(gold, cand)\n",
        "        s2 = self._substring_containment(cand, gold)\n",
        "        # token Jaccard\n",
        "        sj = self._token_jaccard(gold, cand)\n",
        "        # char n-gram containment\n",
        "        sc = self._char_ngram_containment(gold, cand, n=5)\n",
        "        return max(s1, s2, sj, sc)\n",
        "\n",
        "    # --------------- text metrics ---------------\n",
        "\n",
        "    def text_overlap_metrics(\n",
        "        self,\n",
        "        retrieved_texts: List[str],\n",
        "        gold_texts: List[str],\n",
        "        recall_thresh: float = 0.30,     # gentler default\n",
        "        precision_thresh: float = 0.20,  # gentler default\n",
        "    ) -> Dict[str, float]:\n",
        "        \"\"\"\n",
        "        Use the tiered _match_score to compute:\n",
        "          - text_recall: fraction of gold snippets matched by any retrieved\n",
        "          - text_precision: fraction of retrieved snippets that match some gold\n",
        "        \"\"\"\n",
        "        if not gold_texts or all(not str(gt).strip() for gt in gold_texts):\n",
        "            return {'text_recall': 0.0, 'text_precision': 0.0, 'text_f1': 0.0,\n",
        "                    'num_gold_texts_matched': 0}\n",
        "\n",
        "        g_clean = [g for g in gold_texts if str(g).strip()]\n",
        "        r_clean = [r for r in retrieved_texts if str(r).strip()]\n",
        "\n",
        "        # Recall\n",
        "        found = 0\n",
        "        for g in g_clean:\n",
        "            if any(self._match_score(g, r) >= recall_thresh for r in r_clean):\n",
        "                found += 1\n",
        "        text_recall = found / len(g_clean) if g_clean else 0.0\n",
        "\n",
        "        # Precision\n",
        "        rel_ret = 0\n",
        "        for r in r_clean:\n",
        "            if any(self._match_score(r, g) >= precision_thresh for g in g_clean):\n",
        "                rel_ret += 1\n",
        "        text_precision = rel_ret / len(r_clean) if r_clean else 0.0\n",
        "\n",
        "        text_f1 = (2 * text_precision * text_recall / (text_precision + text_recall)\n",
        "                   if (text_precision + text_recall) > 0 else 0.0)\n",
        "\n",
        "        return {\n",
        "            'text_recall': text_recall,\n",
        "            'text_precision': text_precision,\n",
        "            'text_f1': text_f1,\n",
        "            'num_gold_texts_matched': found,\n",
        "        }\n",
        "\n",
        "    def context_sentence_metrics(self, gold_texts: List[str], retrieved_texts: List[str], thresh: float = 0.35):\n",
        "        \"\"\"Sentence-level context precision/recall/F1 using the same tiered _match_score.\"\"\"\n",
        "        gold_sents = [s for t in gold_texts for s in self._split_sents(t)]\n",
        "        ret_sents = [s for t in retrieved_texts for s in self._split_sents(t)]\n",
        "        if not gold_sents or not ret_sents:\n",
        "            return {'ctx_precision': 0.0, 'ctx_recall': 0.0, 'ctx_f1': 0.0}\n",
        "\n",
        "        matched_gold, matched_ret = set(), set()\n",
        "        for i, rt in enumerate(ret_sents):\n",
        "            for j, gt in enumerate(gold_sents):\n",
        "                if self._match_score(rt, gt) >= thresh:\n",
        "                    matched_gold.add(j)\n",
        "                    matched_ret.add(i)\n",
        "                    break\n",
        "\n",
        "        prec = len(matched_ret) / len(ret_sents) if ret_sents else 0.0\n",
        "        rec  = len(matched_gold) / len(gold_sents) if gold_sents else 0.0\n",
        "        f1   = 2 * prec * rec / (prec + rec) if (prec + rec) > 0 else 0.0\n",
        "        return {'ctx_precision': prec, 'ctx_recall': rec, 'ctx_f1': f1}\n",
        "\n",
        "   # rank-aware text metrics\n",
        "    def _text_supported_flags(self, retrieved_texts: List[str], gold_texts: List[str], thresh=0.30) -> List[int]:\n",
        "        g_clean = [g for g in gold_texts if str(g).strip()]\n",
        "        r_clean = [r for r in retrieved_texts if str(r).strip()]\n",
        "        flags = []\n",
        "        for r in r_clean:\n",
        "            hit = any(self._match_score(r, g) >= thresh for g in g_clean)\n",
        "            flags.append(1 if hit else 0)\n",
        "        return flags\n",
        "\n",
        "    def _text_recall_at_k(self, retrieved_texts: List[str], gold_texts: List[str], k: int, thresh=0.30) -> float:\n",
        "        g_clean = [g for g in gold_texts if str(g).strip()]\n",
        "        r_clean = [r for r in retrieved_texts[:k] if str(r).strip()]\n",
        "        if not g_clean:\n",
        "            return 0.0\n",
        "        found = 0\n",
        "        for g in g_clean:\n",
        "            if any(self._match_score(g, r) >= thresh for r in r_clean):\n",
        "                found += 1\n",
        "        return float(found / len(g_clean))\n",
        "\n",
        "    # --------------- per-query evaluation ---------------\n",
        "\n",
        "    def evaluate_query(self, canon_query_key: str) -> Dict:\n",
        "        # canon_query_key is already canonicalized in evaluate()\n",
        "        gold = self.gold_lookup.get(canon_query_key, {\"texts\": [], \"raw_text\": \"\"})\n",
        "        gtexts: List[str] = gold[\"texts\"]\n",
        "\n",
        "        res = self.results_map.get(canon_query_key, {\"docs\": [], \"texts\": []})\n",
        "        rdocs: List[str] = res[\"docs\"]\n",
        "        rtexts: List[str] = res[\"texts\"]\n",
        "\n",
        "        display_q = self.display_lookup.get(canon_query_key, canon_query_key)\n",
        "\n",
        "        out = {\n",
        "            \"query\": display_q,\n",
        "            \"split\": self.query_split.get(canon_query_key, \"unknown\"),\n",
        "            \"raw_gold-text\": gold.get(\"raw_text\", \"\"),\n",
        "            \"num_gold_texts\": len(gtexts),\n",
        "            \"num_retrieved\": len(rdocs),\n",
        "            \"retrieved_docs\": rtexts[:10],\n",
        "        }\n",
        "\n",
        "        # Text overlap (robust)\n",
        "        out.update(self.text_overlap_metrics(rtexts, gtexts))\n",
        "\n",
        "        # Sentence-level context metrics\n",
        "        out.update(self.context_sentence_metrics(gtexts, rtexts))\n",
        "\n",
        "        # Chunk-level overlap metrics\n",
        "        out.update(self.chunk_overlap_metrics(rtexts, gtexts))\n",
        "\n",
        "        # BLEU (corpus-style over concatenated retrieved vs gold)\n",
        "        out[\"bleu\"] = float(self.bleu_score(rtexts, gtexts))\n",
        "\n",
        "        # Rank-aware text metrics\n",
        "        flags = self._text_supported_flags(rtexts, gtexts, thresh=0.30)\n",
        "        for k in (1, 3):\n",
        "            if k in self.topk:\n",
        "                top_flags = flags[:k]\n",
        "                out[f\"text_supported@{k}\"] = float(sum(top_flags) / k) if k > 0 and top_flags else 0.0\n",
        "                out[f\"text_recall@{k}\"] = self._text_recall_at_k(rtexts, gtexts, k, thresh=0.30)\n",
        "\n",
        "        return out\n",
        "\n",
        "    # --------------- evaluate all / aggregate ---------------\n",
        "\n",
        "    def evaluate(self) -> Tuple[Dict, pd.DataFrame]:\n",
        "        gold_qs = set(self.gold_lookup.keys())\n",
        "        res_qs = set(self.results_map.keys())\n",
        "        all_qs = sorted(list(gold_qs | res_qs))  # canonicalized keys\n",
        "\n",
        "        rows = [self.evaluate_query(qk) for qk in all_qs]\n",
        "        df = pd.DataFrame(rows)\n",
        "        agg = self._aggregate(df)\n",
        "        return agg, df\n",
        "\n",
        "    def _aggregate(self, df: pd.DataFrame) -> Dict:\n",
        "        if df.empty:\n",
        "            return {\"total_queries\": 0}\n",
        "\n",
        "        agg: Dict[str, float] = {\n",
        "            \"total_queries\": int(df.shape[0]),\n",
        "            \"queries_with_multiple_texts\": int((df[\"num_gold_texts\"] > 1).sum()),\n",
        "            \"avg_gold_texts_per_query\": float(df[\"num_gold_texts\"].mean()),\n",
        "        }\n",
        "\n",
        "        to_stat = [\n",
        "            \"text_recall\", \"text_precision\", \"text_f1\",\n",
        "            \"ctx_precision\", \"ctx_recall\", \"ctx_f1\",\n",
        "            \"chunk_overlap\",                # NEW\n",
        "            \"min_chunk_overlap\",            # optional, keep if you want distro stats\n",
        "            \"max_chunk_overlap\",            # optional\n",
        "            *(f\"text_supported@{k}\" for k in self.topk),\n",
        "            *(f\"text_recall@{k}\" for k in self.topk),\n",
        "            \"bleu\",                         # BLEU added\n",
        "        ]\n",
        "\n",
        "        for m in to_stat:\n",
        "            vals = df[m].fillna(0.0).astype(float).values if m in df else np.array([0.0])\n",
        "            agg[f\"mean_{m}\"] = float(np.mean(vals))\n",
        "            agg[f\"std_{m}\"]  = float(np.std(vals))\n",
        "            agg[f\"min_{m}\"]  = float(np.min(vals))\n",
        "            agg[f\"max_{m}\"]  = float(np.max(vals))\n",
        "\n",
        "        for split in [\"multi\", \"single\", \"both\"]:\n",
        "            sdf = df[df[\"split\"] == split]\n",
        "            agg[f\"{split}_queries\"] = int(sdf.shape[0])\n",
        "            for m in [\"text_f1\", \"ctx_f1\", \"text_recall@3\", \"text_recall@5\"]:\n",
        "                agg[f\"{split}_mean_{m}\"] = float(sdf[m].fillna(0.0).mean()) if m in sdf else 0.0\n",
        "        return agg\n",
        "\n",
        "    # --------------- outputs ---------------\n",
        "\n",
        "    def write_outputs(self, agg: Dict, df: pd.DataFrame):\n",
        "        os.makedirs(self.out_dir, exist_ok=True)\n",
        "        per_query_csv = os.path.join(self.out_dir, \"per_query_metrics.csv\")\n",
        "        df.to_csv(per_query_csv, index=False)\n",
        "\n",
        "        overall_json = os.path.join(self.out_dir, \"summary.json\")\n",
        "        with open(overall_json, \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(agg, f, indent=2)\n",
        "\n",
        "        report_path = os.path.join(self.out_dir, \"report.txt\")\n",
        "        with open(report_path, \"w\", encoding=\"utf-8\") as f:\n",
        "            f.write(\"=\" * 90 + \"\\n\")\n",
        "            f.write(\"CS RAG (TEXT-ONLY) RETRIEVAL EVALUATION — robust matching\\n\")\n",
        "            f.write(\"=\" * 90 + \"\\n\\n\")\n",
        "            f.write(\"DATASET STATS\\n\")\n",
        "            f.write(\"-\" * 90 + \"\\n\")\n",
        "            f.write(f\"Total Queries: {agg['total_queries']}\\n\")\n",
        "            f.write(f\"Queries with Multiple Gold Texts: {agg['queries_with_multiple_texts']}\\n\")\n",
        "            f.write(f\"Avg Gold Texts / Query: {agg['avg_gold_texts_per_query']:.2f}\\n\\n\")\n",
        "\n",
        "            f.write(\"GLOBAL METRICS (means ± stdev)\\n\")\n",
        "            f.write(\"-\" * 90 + \"\\n\")\n",
        "            for m in [\"text_recall\", \"text_precision\", \"text_f1\",\n",
        "                \"ctx_precision\", \"ctx_recall\", \"ctx_f1\",\n",
        "                \"chunk_overlap\",\n",
        "                \"text_supported@3\", \"text_recall@3\",\n",
        "                \"bleu\"]:\n",
        "                if f\"mean_{m}\" in agg:\n",
        "                    f.write(f\"{m:>18}: {agg[f'mean_{m}']:.4f} ± {agg[f'std_{m}']:.4f} \"\n",
        "                            f\"(min {agg[f'min_{m}']:.4f}, max {agg[f'max_{m}']:.4f})\\n\")\n",
        "\n",
        "            f.write(\"\\nSPLIT-SPECIFIC (means)\\n\")\n",
        "            f.write(\"-\" * 90 + \"\\n\")\n",
        "            for split in [\"multi\", \"single\", \"both\"]:\n",
        "                f.write(f\"{split.upper():<6} | n={agg.get(f'{split}_queries',0):<3d} | \"\n",
        "                        f\"tF1={agg.get(f'{split}_mean_text_f1',0.0):.4f}  \"\n",
        "                        f\"cF1={agg.get(f'{split}_mean_ctx_f1',0.0):.4f}  \"\n",
        "                        f\"tR@3={agg.get(f'{split}_mean_text_recall@3',0.0):.4f}\\n \")\n",
        "        print(f\"[✓] Wrote:\\n - {per_query_csv}\\n - {overall_json}\\n - {report_path}\")\n",
        "\n",
        "# ------------- Colab-friendly wrapper -------------\n",
        "\n",
        "def evaluate_cs_rag_textonly(\n",
        "    multi_csv=\"../queries/multi_file_retrieval_queries.csv\",\n",
        "    single_csv=\"../queries/single_file_retrieval_queries.csv\",  # auto-detects singlefile_ variant\n",
        "    results_json=\"../results/rag_results.json\",\n",
        "    out_dir=\"./eval_out_textonly\",\n",
        "    topk=(1, 3),\n",
        "    return_df=True,\n",
        "):\n",
        "    ev = TextOnlyRAGEvaluator(\n",
        "        multi_csv=multi_csv,\n",
        "        single_csv=single_csv,\n",
        "        results_json=results_json,\n",
        "        out_dir=out_dir,\n",
        "        topk=list(topk),\n",
        "    )\n",
        "    agg, df = ev.evaluate()\n",
        "    ev.write_outputs(agg, df)\n",
        "    if return_df:\n",
        "        try:\n",
        "            from IPython.display import display\n",
        "            print(\"=== Per-query metrics (head) ===\")\n",
        "            display(df.head(10))\n",
        "        except Exception:\n",
        "            print(df.head(10))\n",
        "    print(\"\\n=== Summary (Text-Only) ===\")\n",
        "    print(f\"TextF1: {agg.get('mean_text_f1', 0.0):.4f}  \"\n",
        "          f\"CtxF1: {agg.get('mean_ctx_f1', 0.0):.4f}  \"\n",
        "          f\"tR@3: {agg.get('mean_text_recall@3', 0.0):.4f}  \"\n",
        "          f\"ChunkOverlap: {agg.get('mean_chunk_overlap', 0.0):.4f}  \"\n",
        "          f\"BLEU: {agg.get('mean_bleu', 0.0):.4f}\")\n",
        "    print(f\"\\nWrote outputs to: {out_dir}\\n - per_query_metrics.csv\\n - summary.json\\n - report.txt\")\n",
        "    return agg, df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "GsR9T8m5LFft",
        "outputId": "284f6c4e-5dfa-403c-bd27-453b65d829f7"
      },
      "outputs": [
        {
          "ename": "KeyError",
          "evalue": "\"None of the columns ['gold-text', 'gold_text', 'gold', 'goldtext'] found. Available: ['filename', 'query', 'gold-doc']\"",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m agg, df = \u001b[43mevaluate_cs_rag_textonly\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmulti_csv\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m../queries/multi_file_retrieval_queries.csv\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43msingle_csv\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m../queries/single_file_retrieval_queries.csv\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresults_json\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m../results/semantic_rag_results.json\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mout_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m./eval_out\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m      6\u001b[39m \u001b[43m)\u001b[49m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 675\u001b[39m, in \u001b[36mevaluate_cs_rag_textonly\u001b[39m\u001b[34m(multi_csv, single_csv, results_json, out_dir, topk, return_df)\u001b[39m\n\u001b[32m    667\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mevaluate_cs_rag_textonly\u001b[39m(\n\u001b[32m    668\u001b[39m     multi_csv=\u001b[33m\"\u001b[39m\u001b[33m../queries/multi_file_retrieval_queries.csv\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    669\u001b[39m     single_csv=\u001b[33m\"\u001b[39m\u001b[33m../queries/single_file_retrieval_queries.csv\u001b[39m\u001b[33m\"\u001b[39m,  \u001b[38;5;66;03m# auto-detects singlefile_ variant\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    673\u001b[39m     return_df=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    674\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m675\u001b[39m     ev = \u001b[43mTextOnlyRAGEvaluator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    676\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmulti_csv\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmulti_csv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    677\u001b[39m \u001b[43m        \u001b[49m\u001b[43msingle_csv\u001b[49m\u001b[43m=\u001b[49m\u001b[43msingle_csv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    678\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresults_json\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresults_json\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    679\u001b[39m \u001b[43m        \u001b[49m\u001b[43mout_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mout_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    680\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtopk\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtopk\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    681\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    682\u001b[39m     agg, df = ev.evaluate()\n\u001b[32m    683\u001b[39m     ev.write_outputs(agg, df)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 126\u001b[39m, in \u001b[36mTextOnlyRAGEvaluator.__init__\u001b[39m\u001b[34m(self, multi_csv, single_csv, results_json, out_dir, topk, query_col_candidates, gold_text_col_candidates)\u001b[39m\n\u001b[32m    124\u001b[39m         df.rename(columns={alt: \u001b[38;5;28mself\u001b[39m.query_col}, inplace=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    125\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.gold_text_col \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m df.columns \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(df.columns):\n\u001b[32m--> \u001b[39m\u001b[32m126\u001b[39m         alt = \u001b[43m_pick_col\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mgold_text_col_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    127\u001b[39m         df.rename(columns={alt: \u001b[38;5;28mself\u001b[39m.gold_text_col}, inplace=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    129\u001b[39m \u001b[38;5;66;03m# Load results\u001b[39;00m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 28\u001b[39m, in \u001b[36m_pick_col\u001b[39m\u001b[34m(df, candidates)\u001b[39m\n\u001b[32m     26\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m df.columns:\n\u001b[32m     27\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m c\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNone of the columns \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcandidates\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m found. Available: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(df.columns)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
            "\u001b[31mKeyError\u001b[39m: \"None of the columns ['gold-text', 'gold_text', 'gold', 'goldtext'] found. Available: ['filename', 'query', 'gold-doc']\""
          ]
        }
      ],
      "source": [
        "agg, df = evaluate_cs_rag_textonly(\n",
        "    multi_csv=\"../queries/multi_file_retrieval_queries.csv\",\n",
        "    single_csv=\"../queries/single_file_retrieval_queries.csv\",\n",
        "    results_json=\"../results/semantic_rag_results.json\",\n",
        "    out_dir=\"./eval_out\"\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
