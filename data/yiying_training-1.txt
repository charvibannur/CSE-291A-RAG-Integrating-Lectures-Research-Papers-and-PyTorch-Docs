Distributed Model TrainingYiying ZhangMIT 6.S965: TinyML and Eﬃcient Deep Learning Computinghttps://eﬃcientml.aiSong HanLecture 13 Distributed Training
songhan@mit.eduNeural Network Training•Feedforward and back propagation gradient descent 
source: https://www.programmersought.com/article/45312377380/source: https://arxiv.org/abs/1805.04829Batch Gradient Descent
source: https://www.baeldung.com/cs/epoch-vs-batch-vs-mini-batchStochastic Gradient Descent
source: https://www.baeldung.com/cs/epoch-vs-batch-vs-mini-batchMinibatch-Based SGD
source: https://www.baeldung.com/cs/epoch-vs-batch-vs-mini-batchModel Training CostModelTraining endChip typeTFLOP/s (max)Chip countWall clock time (days)Total time (hours)Total time (years)Retail cost ($US)GPT-3Apr/2020V10013010,00015 days3,552,000405 years$9MLlama 1Jan/2023A1003122,04821 days1,032,192118 years$4MLlama 2Jun/2023A1003122,04835 days1,720,320196 years$7MTitanApr/2023A10031213,76048 days11,558,4001,319 years$45MGPT-4Aug/2022A10031225,00095 days57,000,0006,503 years$224MGeminiNov/2023TPUv427557,000100 days136,800,00015,606 years$440MLlama 3 70BApr/2024H10098924,57611 days6,300,000719 years$7MLlama 3 405BApr/2024H10098924,57650 days29,491,2003,364 years$125MGPT-5Mar/2024H10098950,000120 days144,000,00016,428 years$612MGrok 2Jun/2024H10098920,00050 days57,600,0006,571 years$245MGrok 3Dec/2024H100989100,00050 days288,000,00032,855 years$1.2BSource data from lifearchitect.aiMIT 6.S965: TinyML and Eﬃcient Deep Learning Computinghttps://eﬃcientml.aiDistributed Training is Necessary
8•Developers / Researchers’ time are more valuable than hardware .
•If a training takes 10 GPU days
•Parallelize with distributed training
•1024 GPUs can ﬁnish in 14 minutes (ideally)!
•The develop and research cycle will be greatly boostedLet’s see a use case of distributed training!MIT 6.S965: TinyML and Eﬃcient Deep Learning Computinghttps://eﬃcientml.aiParallelism in Distributed Training•Data Parallelism  •Model Parallelism 
•Compare the Advantages and Disadvantages of Two Parallelism
12MIT 6.S965: TinyML and Eﬃcient Deep Learning Computinghttps://eﬃcientml.aiIntroduction to Distributed TrainingData Parallelism
13ML Model
Training DatasetData ParallelismGPU 1
GPU 2
GPU N
…MIT 6.S965: TinyML and Eﬃcient Deep Learning Computinghttps://eﬃcientml.aiIntroduction to Distributed TrainingData Parallelism
14ML Model
Training DatasetGPU 1
GPU 2
GPU N
…
Split the dataMIT 6.S965: TinyML and Eﬃcient Deep Learning Computinghttps://eﬃcientml.aiIntroduction to Distributed TrainingData Parallelism
15ML Model
Training DatasetGPU 1
GPU 2
GPU N
…
Same model across devicesMIT 6.S965: TinyML and Eﬃcient Deep Learning Computinghttps://eﬃcientml.aiDive into Data ParallelismScaling Distributed Machine Learning with the Parameter Server
25Scaling Distributed Machine Learning with the Parameter Server. Mu Li et al. 2014
Figure credits from: Deep Gradient Compression. Lin et al. 2018…/u1D6AB/u1D47EDataDataDataDataParameter Server The central controller of the whole training processWorker nodes The hardware accelerators and dataset storage.Two diﬀerent roles in framework:
•Parameter Server: receive gradients from workers and send back the aggregated results
•Workers: compute gradients using splitted dataset and send to parameter server
MIT 6.S965: TinyML and Eﬃcient Deep Learning Computinghttps://eﬃcientml.aiProblems with Parameter Server
36……/u1D6AB/u1D47EDataDataData……DataDataData……DataDataData……DataDataData……DataDataData……DataDataData……DataDataDataWhen number of workers is small 
! When number of workers is large 
" 
The bandwidth requirement of parameter server grows linearly w.r.t number of workers.OverloadedMIT 6.S965: TinyML and Eﬃcient Deep Learning Computinghttps://eﬃcientml.aiDistributed CommunicationPoint-to-Point: Send and Recv
39Point-to-point communication: transfer data from one process to another
• Send & Receive are the most common distributed communication schemes.
• Implemented in Socket / MPI / Gloo / NCCLSend: n0 -> n30123Recv: n0 -> n30123MIT 6.S965: TinyML and Eﬃcient Deep Learning Computinghttps://eﬃcientml.aiDistributed CommunicationNaive All-Reduce Implementation - Parallel Reduce 
50Perform ALL reduce operations simultaneously. 
Pseudocode: —————————————————Parallel for i:=0 to N:Allreduce(work[i])01230123Step 1Time: O(1) <- improved Bandwidth: O(N^2) <- worseMIT 6.S965: TinyML and Eﬃcient Deep Learning Computinghttps://eﬃcientml.aiRecursive Halving All Reduce Distributed Communication
60Step 1 - Each node exchanges with neighbors with offset 1Step 2 - Each node exchanges with neighbors with offset 2
[1] Thakur, Rajeev, Rolf Rabenseifner, and William Gropp. "Optimization of collective communication operations in MPICH." Step 3 - Each node exchanges with neighbors with offset 412345678
For N workers, AllReduce ﬁnish in log(N) steps.NodeCombination of Data from nodes of corresponding colorMIT 6.S965: TinyML and Eﬃcient Deep Learning Computinghttps://eﬃcientml.aiAll Reduce Implementations ComparisonDistributed Communication
61AllReduce with proper implementations reduce the peak bandwidth from O(N) to O(1) with little time overhead.TimePeak Node BandwidthTotal BandwidthParameter ServerO(1)O(N)O(N)All-Reduce - SequentialO(N)O(N)O(N)All-Reduce - RingO(N)O(1)O(N)All-Reduce - ParallelO(1)O(N)O(N^2)All-Reduce - Recursive HalvingO(lgN)O(1)O(N)MIT 6.S965: TinyML and Eﬃcient Deep Learning Computinghttps://eﬃcientml.aiParallelism in Distributed Training•Data Parallelism  •Model Parallelism  •Compare the Advantages and Disadvantages of Two Parallelism
16MIT 6.S965: TinyML and Eﬃcient Deep Learning Computinghttps://eﬃcientml.aiData Parallelism Cannot Train Large Models
63GPU 1
GPU 2
Though model parallelism has better device utilization, if train a super-large model (e.g., GPT-3)
Nvidia A100 80GB175B * 16 Bits 
= 350GB Memory
>Even the best GPU CANNOT ﬁt the model into memory!MIT 6.S965: TinyML and Eﬃcient Deep Learning Computinghttps://eﬃcientml.aiIntroduction to Distributed TrainingModel Parallelism
17Figures credit from CMU 15-849 [Jia 2022]ML Model
GPU 1GPU 2
…GPU N
Training DatasetMIT 6.S965: TinyML and Eﬃcient Deep Learning Computinghttps://eﬃcientml.aiModel Parallelism Designed for Large Model Training
64In order to ﬁt training into hardware, instead of splitting the data, model parallelism split the model
350GB / 8 cards = 43.75G < 80G
With model parallelism, large ML models can be placed and trained on GPUs.GPU 1GPU 2
GPU 3
GPU 8…
MIT 6.S965: TinyML and Eﬃcient Deep Learning Computinghttps://eﬃcientml.aiIntroduction to Distributed TrainingModel Parallelism
18ML Model
GPU 1GPU 2
…GPU N
Training Dataset
Single copy of data
Figures credit from CMU 15-849 [Jia 2022]MIT 6.S965: TinyML and Eﬃcient Deep Learning Computinghttps://eﬃcientml.aiIntroduction to Distributed TrainingModel Parallelism
19ML Model
GPU 1GPU 2
…GPU N
Training Dataset
Split the model
Figures credit from CMU 15-849 [Jia 2022]MIT 6.S965: TinyML and Eﬃcient Deep Learning Computinghttps://eﬃcientml.aiIntroduction to Distributed TrainingModel Parallelism
20ML Model
GPU 1GPU 2
…GPU N
Training Dataset
Split the model
Figures credit from CMU 15-849 [Jia 2022]MIT 6.S965: TinyML and Eﬃcient Deep Learning Computinghttps://eﬃcientml.aiIntroduction to Distributed TrainingModel Parallelism
21ML Model
GPU 1GPU 2
…GPU N
Training Dataset
Split the model
Figures credit from CMU 15-849 [Jia 2022]MIT 6.S965: TinyML and Eﬃcient Deep Learning Computinghttps://eﬃcientml.aiModel Parallelism WorkflowNaive Implementation
65GPipe: Eﬃcient Training of Giant Neural Networks using Pipeline Parallelism [Huang et al. 2018]
Figure 2: (a) An example neural network with sequential layers is partitioned across four accelerators.Fkis the composite forward computation function of thek-th cell.Bkis the back-propagationfunction, which depends on bothBk+1from the upper layer andFk. (b) The naive model parallelismstrategy leads to severe under-utilization due to the sequential dependency of the network. (c) Pipelineparallelism divides the input mini-batch into smaller micro-batches, enabling different accelerators towork on different micro-batches simultaneously. Gradients are applied synchronously at the end.
(b)
(a)
(c)2.1 InterfaceAny deep neural network can be deﬁned as a sequence ofLlayers. Each layerLiis composed ofa forward computation functionfi, and a corresponding set of parameterswi. GPipe additionallyallows the user to specify an optional computation cost estimation function,ci. With a given numberof partitionsK, the sequence ofLlayers can be partitioned intoKcomposite layers, or cells. Letpkconsist of consecutive layers between layersiandj. The set of parameters corresponding topkisequivalent to the union ofwi,wi+1,...,wj, and its forward function would beFk=fj ... fi+1 fi.The corresponding back-propagation functionBkcan be computed fromFkusing automatic symbolicdifferentiation. The cost estimator,Ck, is set to⌃jl=icl.The GPipe interface is extremely simple and intuitive, requiring the user to specify: (i) the number ofmodel partitionsK, (ii) the number of micro-batchesM, and (iii) the sequence and deﬁnitions ofLlayers that deﬁne the model. Please refer to supplementary material for examples.2.2 AlgorithmOnce the user deﬁnes the sequence of layers in their network in terms of model parameterswi, forwardcomputation functionfi, and the cost estimation functionci, GPipe partitions the network intoKcells and places thek-th cell on thek-th accelerator. Communication primitives are automaticallyinserted at partition boundaries to allow data transfer between neighboring partitions. The partitioningalgorithm minimizes the variance in the estimated costs of all cells in order to maximize the efﬁciencyof the pipeline by syncing the computation time across all partitions.During the forward pass, GPipe ﬁrst divides every mini-batch of sizeNintoMequal micro-batches,which are pipelined through theKaccelerators. During the backward pass, gradients for eachmicro-batch are computed based on the same model parameters used for the forward pass. At the endof each mini-batch, gradients from allMmicro-batches are accumulated and applied to update themodel parameters across all accelerators. This sequence of operations is illustrated in Figure 2c.If batch normalization [20] is used in the network, the sufﬁcient statistics of inputs during trainingare computed over eachmicro-batchand over replicas if necessary [21]. We also track the movingaverage of the sufﬁcient statistics over the entiremini-batchto be used during evaluation.3F: Forward  B: Backward. Train a 4 layer network with model parallelism. Model parallelism is needed for training a bigger DNN model on accelerators by dividing the model into partitions and assigning diﬀerent partitions to diﬀerent accelerators.(a). Training data ﬂow(b). Training timelineF0F1F2F3B0B1B2B3UpdateUpdateUpdateUpdateMIT 6.S965: TinyML and Eﬃcient Deep Learning Computinghttps://eﬃcientml.aiModel Parallelism Workflow
66Figures from GPipe: Easy Scaling with Micro-Batch Pipeline Parallelism Figure 2: (a) An example neural network with sequential layers is partitioned across four accelerators.Fkis the composite forward computation function of thek-th cell.Bkis the back-propagationfunction, which depends on bothBk+1from the upper layer andFk. (b) The naive model parallelismstrategy leads to severe under-utilization due to the sequential dependency of the network. (c) Pipelineparallelism divides the input mini-batch into smaller micro-batches, enabling different accelerators towork on different micro-batches simultaneously. Gradients are applied synchronously at the end.
(b)
(a)
(c)2.1 InterfaceAny deep neural network can be deﬁned as a sequence ofLlayers. Each layerLiis composed ofa forward computation functionfi, and a corresponding set of parameterswi. GPipe additionallyallows the user to specify an optional computation cost estimation function,ci. With a given numberof partitionsK, the sequence ofLlayers can be partitioned intoKcomposite layers, or cells. Letpkconsist of consecutive layers between layersiandj. The set of parameters corresponding topkisequivalent to the union ofwi,wi+1,...,wj, and its forward function would beFk=fj ... fi+1 fi.The corresponding back-propagation functionBkcan be computed fromFkusing automatic symbolicdifferentiation. The cost estimator,Ck, is set to⌃jl=icl.The GPipe interface is extremely simple and intuitive, requiring the user to specify: (i) the number ofmodel partitionsK, (ii) the number of micro-batchesM, and (iii) the sequence and deﬁnitions ofLlayers that deﬁne the model. Please refer to supplementary material for examples.2.2 AlgorithmOnce the user deﬁnes the sequence of layers in their network in terms of model parameterswi, forwardcomputation functionfi, and the cost estimation functionci, GPipe partitions the network intoKcells and places thek-th cell on thek-th accelerator. Communication primitives are automaticallyinserted at partition boundaries to allow data transfer between neighboring partitions. The partitioningalgorithm minimizes the variance in the estimated costs of all cells in order to maximize the efﬁciencyof the pipeline by syncing the computation time across all partitions.During the forward pass, GPipe ﬁrst divides every mini-batch of sizeNintoMequal micro-batches,which are pipelined through theKaccelerators. During the backward pass, gradients for eachmicro-batch are computed based on the same model parameters used for the forward pass. At the endof each mini-batch, gradients from allMmicro-batches are accumulated and applied to update themodel parameters across all accelerators. This sequence of operations is illustrated in Figure 2c.If batch normalization [20] is used in the network, the sufﬁcient statistics of inputs during trainingare computed over eachmicro-batchand over replicas if necessary [21]. We also track the movingaverage of the sufﬁcient statistics over the entiremini-batchto be used during evaluation.3F: Forward  B: Backward. Train a 4 layer network with model parallelism. Model parallelism is needed for training a bigger DNN model on accelerators by dividing the model into partitions and assigning diﬀerent partitions to diﬀerent accelerators.(a). Training data ﬂow(b). Training timelineNaive ImplementationF0F1F2F3B0B1B2B3UpdateUpdateUpdateUpdateMIT 6.S965: TinyML and Eﬃcient Deep Learning Computinghttps://eﬃcientml.aiF0F1F2F3B0B1B2B3UpdateUpdateUpdateUpdateModel Parallelism Workflow
67Figures from GPipe: Easy Scaling with Micro-Batch Pipeline Parallelism Figure 2: (a) An example neural network with sequential layers is partitioned across four accelerators.Fkis the composite forward computation function of thek-th cell.Bkis the back-propagationfunction, which depends on bothBk+1from the upper layer andFk. (b) The naive model parallelismstrategy leads to severe under-utilization due to the sequential dependency of the network. (c) Pipelineparallelism divides the input mini-batch into smaller micro-batches, enabling different accelerators towork on different micro-batches simultaneously. Gradients are applied synchronously at the end.
(b)
(a)
(c)2.1 InterfaceAny deep neural network can be deﬁned as a sequence ofLlayers. Each layerLiis composed ofa forward computation functionfi, and a corresponding set of parameterswi. GPipe additionallyallows the user to specify an optional computation cost estimation function,ci. With a given numberof partitionsK, the sequence ofLlayers can be partitioned intoKcomposite layers, or cells. Letpkconsist of consecutive layers between layersiandj. The set of parameters corresponding topkisequivalent to the union ofwi,wi+1,...,wj, and its forward function would beFk=fj ... fi+1 fi.The corresponding back-propagation functionBkcan be computed fromFkusing automatic symbolicdifferentiation. The cost estimator,Ck, is set to⌃jl=icl.The GPipe interface is extremely simple and intuitive, requiring the user to specify: (i) the number ofmodel partitionsK, (ii) the number of micro-batchesM, and (iii) the sequence and deﬁnitions ofLlayers that deﬁne the model. Please refer to supplementary material for examples.2.2 AlgorithmOnce the user deﬁnes the sequence of layers in their network in terms of model parameterswi, forwardcomputation functionfi, and the cost estimation functionci, GPipe partitions the network intoKcells and places thek-th cell on thek-th accelerator. Communication primitives are automaticallyinserted at partition boundaries to allow data transfer between neighboring partitions. The partitioningalgorithm minimizes the variance in the estimated costs of all cells in order to maximize the efﬁciencyof the pipeline by syncing the computation time across all partitions.During the forward pass, GPipe ﬁrst divides every mini-batch of sizeNintoMequal micro-batches,which are pipelined through theKaccelerators. During the backward pass, gradients for eachmicro-batch are computed based on the same model parameters used for the forward pass. At the endof each mini-batch, gradients from allMmicro-batches are accumulated and applied to update themodel parameters across all accelerators. This sequence of operations is illustrated in Figure 2c.If batch normalization [20] is used in the network, the sufﬁcient statistics of inputs during trainingare computed over eachmicro-batchand over replicas if necessary [21]. We also track the movingaverage of the sufﬁcient statistics over the entiremini-batchto be used during evaluation.3F: Forward  B: Backward. Train a 4 layer network with model parallelism. Model parallelism is needed for training a bigger DNN model on accelerators by dividing the model into partitions and assigning diﬀerent partitions to diﬀerent accelerators.(a). Training data ﬂow(b). Training timelineNaive ImplementationMIT 6.S965: TinyML and Eﬃcient Deep Learning Computinghttps://eﬃcientml.aiF0F1F2F3B0B1B2B3UpdateUpdateUpdateUpdateModel Parallelism Workflow
68Figures from GPipe: Easy Scaling with Micro-Batch Pipeline Parallelism Figure 2: (a) An example neural network with sequential layers is partitioned across four accelerators.Fkis the composite forward computation function of thek-th cell.Bkis the back-propagationfunction, which depends on bothBk+1from the upper layer andFk. (b) The naive model parallelismstrategy leads to severe under-utilization due to the sequential dependency of the network. (c) Pipelineparallelism divides the input mini-batch into smaller micro-batches, enabling different accelerators towork on different micro-batches simultaneously. Gradients are applied synchronously at the end.
(b)
(a)
(c)2.1 InterfaceAny deep neural network can be deﬁned as a sequence ofLlayers. Each layerLiis composed ofa forward computation functionfi, and a corresponding set of parameterswi. GPipe additionallyallows the user to specify an optional computation cost estimation function,ci. With a given numberof partitionsK, the sequence ofLlayers can be partitioned intoKcomposite layers, or cells. Letpkconsist of consecutive layers between layersiandj. The set of parameters corresponding topkisequivalent to the union ofwi,wi+1,...,wj, and its forward function would beFk=fj ... fi+1 fi.The corresponding back-propagation functionBkcan be computed fromFkusing automatic symbolicdifferentiation. The cost estimator,Ck, is set to⌃jl=icl.The GPipe interface is extremely simple and intuitive, requiring the user to specify: (i) the number ofmodel partitionsK, (ii) the number of micro-batchesM, and (iii) the sequence and deﬁnitions ofLlayers that deﬁne the model. Please refer to supplementary material for examples.2.2 AlgorithmOnce the user deﬁnes the sequence of layers in their network in terms of model parameterswi, forwardcomputation functionfi, and the cost estimation functionci, GPipe partitions the network intoKcells and places thek-th cell on thek-th accelerator. Communication primitives are automaticallyinserted at partition boundaries to allow data transfer between neighboring partitions. The partitioningalgorithm minimizes the variance in the estimated costs of all cells in order to maximize the efﬁciencyof the pipeline by syncing the computation time across all partitions.During the forward pass, GPipe ﬁrst divides every mini-batch of sizeNintoMequal micro-batches,which are pipelined through theKaccelerators. During the backward pass, gradients for eachmicro-batch are computed based on the same model parameters used for the forward pass. At the endof each mini-batch, gradients from allMmicro-batches are accumulated and applied to update themodel parameters across all accelerators. This sequence of operations is illustrated in Figure 2c.If batch normalization [20] is used in the network, the sufﬁcient statistics of inputs during trainingare computed over eachmicro-batchand over replicas if necessary [21]. We also track the movingaverage of the sufﬁcient statistics over the entiremini-batchto be used during evaluation.3F: Forward  B: Backward. Train a 4 layer network with model parallelism. Model parallelism is needed for training a bigger DNN model on accelerators by dividing the model into partitions and assigning diﬀerent partitions to diﬀerent accelerators.(a). Training data ﬂow(b). Training timelineNaive ImplementationMIT 6.S965: TinyML and Eﬃcient Deep Learning Computinghttps://eﬃcientml.aiF0F1F2F3B0B1B2B3UpdateUpdateUpdateUpdateModel Parallelism Workflow
69Figures from GPipe: Easy Scaling with Micro-Batch Pipeline Parallelism Figure 2: (a) An example neural network with sequential layers is partitioned across four accelerators.Fkis the composite forward computation function of thek-th cell.Bkis the back-propagationfunction, which depends on bothBk+1from the upper layer andFk. (b) The naive model parallelismstrategy leads to severe under-utilization due to the sequential dependency of the network. (c) Pipelineparallelism divides the input mini-batch into smaller micro-batches, enabling different accelerators towork on different micro-batches simultaneously. Gradients are applied synchronously at the end.
(b)
(a)
(c)2.1 InterfaceAny deep neural network can be deﬁned as a sequence ofLlayers. Each layerLiis composed ofa forward computation functionfi, and a corresponding set of parameterswi. GPipe additionallyallows the user to specify an optional computation cost estimation function,ci. With a given numberof partitionsK, the sequence ofLlayers can be partitioned intoKcomposite layers, or cells. Letpkconsist of consecutive layers between layersiandj. The set of parameters corresponding topkisequivalent to the union ofwi,wi+1,...,wj, and its forward function would beFk=fj ... fi+1 fi.The corresponding back-propagation functionBkcan be computed fromFkusing automatic symbolicdifferentiation. The cost estimator,Ck, is set to⌃jl=icl.The GPipe interface is extremely simple and intuitive, requiring the user to specify: (i) the number ofmodel partitionsK, (ii) the number of micro-batchesM, and (iii) the sequence and deﬁnitions ofLlayers that deﬁne the model. Please refer to supplementary material for examples.2.2 AlgorithmOnce the user deﬁnes the sequence of layers in their network in terms of model parameterswi, forwardcomputation functionfi, and the cost estimation functionci, GPipe partitions the network intoKcells and places thek-th cell on thek-th accelerator. Communication primitives are automaticallyinserted at partition boundaries to allow data transfer between neighboring partitions. The partitioningalgorithm minimizes the variance in the estimated costs of all cells in order to maximize the efﬁciencyof the pipeline by syncing the computation time across all partitions.During the forward pass, GPipe ﬁrst divides every mini-batch of sizeNintoMequal micro-batches,which are pipelined through theKaccelerators. During the backward pass, gradients for eachmicro-batch are computed based on the same model parameters used for the forward pass. At the endof each mini-batch, gradients from allMmicro-batches are accumulated and applied to update themodel parameters across all accelerators. This sequence of operations is illustrated in Figure 2c.If batch normalization [20] is used in the network, the sufﬁcient statistics of inputs during trainingare computed over eachmicro-batchand over replicas if necessary [21]. We also track the movingaverage of the sufﬁcient statistics over the entiremini-batchto be used during evaluation.3F: Forward  B: Backward. Train a 4 layer network with model parallelism. Model parallelism is needed for training a bigger DNN model on accelerators by dividing the model into partitions and assigning diﬀerent partitions to diﬀerent accelerators.(a). Training data ﬂow(b). Training timelineNaive ImplementationMIT 6.S965: TinyML and Eﬃcient Deep Learning Computinghttps://eﬃcientml.aiF0F1F2F3B0B1B2B3UpdateUpdateUpdateUpdateModel Parallelism Workflow
70Figures from GPipe: Easy Scaling with Micro-Batch Pipeline Parallelism Figure 2: (a) An example neural network with sequential layers is partitioned across four accelerators.Fkis the composite forward computation function of thek-th cell.Bkis the back-propagationfunction, which depends on bothBk+1from the upper layer andFk. (b) The naive model parallelismstrategy leads to severe under-utilization due to the sequential dependency of the network. (c) Pipelineparallelism divides the input mini-batch into smaller micro-batches, enabling different accelerators towork on different micro-batches simultaneously. Gradients are applied synchronously at the end.
(b)
(a)
(c)2.1 InterfaceAny deep neural network can be deﬁned as a sequence ofLlayers. Each layerLiis composed ofa forward computation functionfi, and a corresponding set of parameterswi. GPipe additionallyallows the user to specify an optional computation cost estimation function,ci. With a given numberof partitionsK, the sequence ofLlayers can be partitioned intoKcomposite layers, or cells. Letpkconsist of consecutive layers between layersiandj. The set of parameters corresponding topkisequivalent to the union ofwi,wi+1,...,wj, and its forward function would beFk=fj ... fi+1 fi.The corresponding back-propagation functionBkcan be computed fromFkusing automatic symbolicdifferentiation. The cost estimator,Ck, is set to⌃jl=icl.The GPipe interface is extremely simple and intuitive, requiring the user to specify: (i) the number ofmodel partitionsK, (ii) the number of micro-batchesM, and (iii) the sequence and deﬁnitions ofLlayers that deﬁne the model. Please refer to supplementary material for examples.2.2 AlgorithmOnce the user deﬁnes the sequence of layers in their network in terms of model parameterswi, forwardcomputation functionfi, and the cost estimation functionci, GPipe partitions the network intoKcells and places thek-th cell on thek-th accelerator. Communication primitives are automaticallyinserted at partition boundaries to allow data transfer between neighboring partitions. The partitioningalgorithm minimizes the variance in the estimated costs of all cells in order to maximize the efﬁciencyof the pipeline by syncing the computation time across all partitions.During the forward pass, GPipe ﬁrst divides every mini-batch of sizeNintoMequal micro-batches,which are pipelined through theKaccelerators. During the backward pass, gradients for eachmicro-batch are computed based on the same model parameters used for the forward pass. At the endof each mini-batch, gradients from allMmicro-batches are accumulated and applied to update themodel parameters across all accelerators. This sequence of operations is illustrated in Figure 2c.If batch normalization [20] is used in the network, the sufﬁcient statistics of inputs during trainingare computed over eachmicro-batchand over replicas if necessary [21]. We also track the movingaverage of the sufﬁcient statistics over the entiremini-batchto be used during evaluation.3F: Forward  B: Backward. Train a 4 layer network with model parallelism. Model parallelism is needed for training a bigger DNN model on accelerators by dividing the model into partitions and assigning diﬀerent partitions to diﬀerent accelerators.(a). Training data ﬂow(b). Training timelineNaive ImplementationMIT 6.S965: TinyML and Eﬃcient Deep Learning Computinghttps://eﬃcientml.aiModel Parallelism Workflow
71Figures from GPipe: Easy Scaling with Micro-Batch Pipeline Parallelism Figure 2: (a) An example neural network with sequential layers is partitioned across four accelerators.Fkis the composite forward computation function of thek-th cell.Bkis the back-propagationfunction, which depends on bothBk+1from the upper layer andFk. (b) The naive model parallelismstrategy leads to severe under-utilization due to the sequential dependency of the network. (c) Pipelineparallelism divides the input mini-batch into smaller micro-batches, enabling different accelerators towork on different micro-batches simultaneously. Gradients are applied synchronously at the end.
(b)
(a)
(c)2.1 InterfaceAny deep neural network can be deﬁned as a sequence ofLlayers. Each layerLiis composed ofa forward computation functionfi, and a corresponding set of parameterswi. GPipe additionallyallows the user to specify an optional computation cost estimation function,ci. With a given numberof partitionsK, the sequence ofLlayers can be partitioned intoKcomposite layers, or cells. Letpkconsist of consecutive layers between layersiandj. The set of parameters corresponding topkisequivalent to the union ofwi,wi+1,...,wj, and its forward function would beFk=fj ... fi+1 fi.The corresponding back-propagation functionBkcan be computed fromFkusing automatic symbolicdifferentiation. The cost estimator,Ck, is set to⌃jl=icl.The GPipe interface is extremely simple and intuitive, requiring the user to specify: (i) the number ofmodel partitionsK, (ii) the number of micro-batchesM, and (iii) the sequence and deﬁnitions ofLlayers that deﬁne the model. Please refer to supplementary material for examples.2.2 AlgorithmOnce the user deﬁnes the sequence of layers in their network in terms of model parameterswi, forwardcomputation functionfi, and the cost estimation functionci, GPipe partitions the network intoKcells and places thek-th cell on thek-th accelerator. Communication primitives are automaticallyinserted at partition boundaries to allow data transfer between neighboring partitions. The partitioningalgorithm minimizes the variance in the estimated costs of all cells in order to maximize the efﬁciencyof the pipeline by syncing the computation time across all partitions.During the forward pass, GPipe ﬁrst divides every mini-batch of sizeNintoMequal micro-batches,which are pipelined through theKaccelerators. During the backward pass, gradients for eachmicro-batch are computed based on the same model parameters used for the forward pass. At the endof each mini-batch, gradients from allMmicro-batches are accumulated and applied to update themodel parameters across all accelerators. This sequence of operations is illustrated in Figure 2c.If batch normalization [20] is used in the network, the sufﬁcient statistics of inputs during trainingare computed over eachmicro-batchand over replicas if necessary [21]. We also track the movingaverage of the sufﬁcient statistics over the entiremini-batchto be used during evaluation.3F: Forward  B: Backward. Train a 4 layer network with model parallelism. Model parallelism is needed for training a bigger DNN model on accelerators by dividing the model into partitions and assigning diﬀerent partitions to diﬀerent accelerators.(a). Training data ﬂow(b). Training timelineNaive ImplementationF0F1F2F3B0B1B2B3UpdateUpdateUpdateUpdateMIT 6.S965: TinyML and Eﬃcient Deep Learning Computinghttps://eﬃcientml.aiModel Parallelism Workflow
72Figures from GPipe: Easy Scaling with Micro-Batch Pipeline Parallelism Figure 2: (a) An example neural network with sequential layers is partitioned across four accelerators.Fkis the composite forward computation function of thek-th cell.Bkis the back-propagationfunction, which depends on bothBk+1from the upper layer andFk. (b) The naive model parallelismstrategy leads to severe under-utilization due to the sequential dependency of the network. (c) Pipelineparallelism divides the input mini-batch into smaller micro-batches, enabling different accelerators towork on different micro-batches simultaneously. Gradients are applied synchronously at the end.
(b)
(a)
(c)2.1 InterfaceAny deep neural network can be deﬁned as a sequence ofLlayers. Each layerLiis composed ofa forward computation functionfi, and a corresponding set of parameterswi. GPipe additionallyallows the user to specify an optional computation cost estimation function,ci. With a given numberof partitionsK, the sequence ofLlayers can be partitioned intoKcomposite layers, or cells. Letpkconsist of consecutive layers between layersiandj. The set of parameters corresponding topkisequivalent to the union ofwi,wi+1,...,wj, and its forward function would beFk=fj ... fi+1 fi.The corresponding back-propagation functionBkcan be computed fromFkusing automatic symbolicdifferentiation. The cost estimator,Ck, is set to⌃jl=icl.The GPipe interface is extremely simple and intuitive, requiring the user to specify: (i) the number ofmodel partitionsK, (ii) the number of micro-batchesM, and (iii) the sequence and deﬁnitions ofLlayers that deﬁne the model. Please refer to supplementary material for examples.2.2 AlgorithmOnce the user deﬁnes the sequence of layers in their network in terms of model parameterswi, forwardcomputation functionfi, and the cost estimation functionci, GPipe partitions the network intoKcells and places thek-th cell on thek-th accelerator. Communication primitives are automaticallyinserted at partition boundaries to allow data transfer between neighboring partitions. The partitioningalgorithm minimizes the variance in the estimated costs of all cells in order to maximize the efﬁciencyof the pipeline by syncing the computation time across all partitions.During the forward pass, GPipe ﬁrst divides every mini-batch of sizeNintoMequal micro-batches,which are pipelined through theKaccelerators. During the backward pass, gradients for eachmicro-batch are computed based on the same model parameters used for the forward pass. At the endof each mini-batch, gradients from allMmicro-batches are accumulated and applied to update themodel parameters across all accelerators. This sequence of operations is illustrated in Figure 2c.If batch normalization [20] is used in the network, the sufﬁcient statistics of inputs during trainingare computed over eachmicro-batchand over replicas if necessary [21]. We also track the movingaverage of the sufﬁcient statistics over the entiremini-batchto be used during evaluation.3F: Forward  B: Backward. Train a 4 layer network with model parallelism. Model parallelism is needed for training a bigger DNN model on accelerators by dividing the model into partitions and assigning diﬀerent partitions to diﬀerent accelerators.(a). Training data ﬂow(b). Training timelineNaive ImplementationF0F1F2F3B0B1B2B3UpdateUpdateUpdateUpdateMIT 6.S965: TinyML and Eﬃcient Deep Learning Computinghttps://eﬃcientml.aiF0F1F2F3B0B1B2B3UpdateUpdateUpdateUpdateModel Parallelism Workflow
73Figures from GPipe: Easy Scaling with Micro-Batch Pipeline Parallelism Figure 2: (a) An example neural network with sequential layers is partitioned across four accelerators.Fkis the composite forward computation function of thek-th cell.Bkis the back-propagationfunction, which depends on bothBk+1from the upper layer andFk. (b) The naive model parallelismstrategy leads to severe under-utilization due to the sequential dependency of the network. (c) Pipelineparallelism divides the input mini-batch into smaller micro-batches, enabling different accelerators towork on different micro-batches simultaneously. Gradients are applied synchronously at the end.
(b)
(a)
(c)2.1 InterfaceAny deep neural network can be deﬁned as a sequence ofLlayers. Each layerLiis composed ofa forward computation functionfi, and a corresponding set of parameterswi. GPipe additionallyallows the user to specify an optional computation cost estimation function,ci. With a given numberof partitionsK, the sequence ofLlayers can be partitioned intoKcomposite layers, or cells. Letpkconsist of consecutive layers between layersiandj. The set of parameters corresponding topkisequivalent to the union ofwi,wi+1,...,wj, and its forward function would beFk=fj ... fi+1 fi.The corresponding back-propagation functionBkcan be computed fromFkusing automatic symbolicdifferentiation. The cost estimator,Ck, is set to⌃jl=icl.The GPipe interface is extremely simple and intuitive, requiring the user to specify: (i) the number ofmodel partitionsK, (ii) the number of micro-batchesM, and (iii) the sequence and deﬁnitions ofLlayers that deﬁne the model. Please refer to supplementary material for examples.2.2 AlgorithmOnce the user deﬁnes the sequence of layers in their network in terms of model parameterswi, forwardcomputation functionfi, and the cost estimation functionci, GPipe partitions the network intoKcells and places thek-th cell on thek-th accelerator. Communication primitives are automaticallyinserted at partition boundaries to allow data transfer between neighboring partitions. The partitioningalgorithm minimizes the variance in the estimated costs of all cells in order to maximize the efﬁciencyof the pipeline by syncing the computation time across all partitions.During the forward pass, GPipe ﬁrst divides every mini-batch of sizeNintoMequal micro-batches,which are pipelined through theKaccelerators. During the backward pass, gradients for eachmicro-batch are computed based on the same model parameters used for the forward pass. At the endof each mini-batch, gradients from allMmicro-batches are accumulated and applied to update themodel parameters across all accelerators. This sequence of operations is illustrated in Figure 2c.If batch normalization [20] is used in the network, the sufﬁcient statistics of inputs during trainingare computed over eachmicro-batchand over replicas if necessary [21]. We also track the movingaverage of the sufﬁcient statistics over the entiremini-batchto be used during evaluation.3F: Forward  B: Backward. Train a 4 layer network with model parallelism. Model parallelism is needed for training a bigger DNN model on accelerators by dividing the model into partitions and assigning diﬀerent partitions to diﬀerent accelerators.(a). Training data ﬂow(b). Training timelineNaive ImplementationMIT 6.S965: TinyML and Eﬃcient Deep Learning Computinghttps://eﬃcientml.aiF0F1F2F3B0B1B2B3UpdateUpdateUpdateUpdateModel Parallelism Workflow
74GPipe: Eﬃcient Training of Giant Neural Networks using Pipeline Parallelism [Huang et al. 2018]
Figure 2: (a) An example neural network with sequential layers is partitioned across four accelerators.Fkis the composite forward computation function of thek-th cell.Bkis the back-propagationfunction, which depends on bothBk+1from the upper layer andFk. (b) The naive model parallelismstrategy leads to severe under-utilization due to the sequential dependency of the network. (c) Pipelineparallelism divides the input mini-batch into smaller micro-batches, enabling different accelerators towork on different micro-batches simultaneously. Gradients are applied synchronously at the end.
(b)
(a)
(c)2.1 InterfaceAny deep neural network can be deﬁned as a sequence ofLlayers. Each layerLiis composed ofa forward computation functionfi, and a corresponding set of parameterswi. GPipe additionallyallows the user to specify an optional computation cost estimation function,ci. With a given numberof partitionsK, the sequence ofLlayers can be partitioned intoKcomposite layers, or cells. Letpkconsist of consecutive layers between layersiandj. The set of parameters corresponding topkisequivalent to the union ofwi,wi+1,...,wj, and its forward function would beFk=fj ... fi+1 fi.The corresponding back-propagation functionBkcan be computed fromFkusing automatic symbolicdifferentiation. The cost estimator,Ck, is set to⌃jl=icl.The GPipe interface is extremely simple and intuitive, requiring the user to specify: (i) the number ofmodel partitionsK, (ii) the number of micro-batchesM, and (iii) the sequence and deﬁnitions ofLlayers that deﬁne the model. Please refer to supplementary material for examples.2.2 AlgorithmOnce the user deﬁnes the sequence of layers in their network in terms of model parameterswi, forwardcomputation functionfi, and the cost estimation functionci, GPipe partitions the network intoKcells and places thek-th cell on thek-th accelerator. Communication primitives are automaticallyinserted at partition boundaries to allow data transfer between neighboring partitions. The partitioningalgorithm minimizes the variance in the estimated costs of all cells in order to maximize the efﬁciencyof the pipeline by syncing the computation time across all partitions.During the forward pass, GPipe ﬁrst divides every mini-batch of sizeNintoMequal micro-batches,which are pipelined through theKaccelerators. During the backward pass, gradients for eachmicro-batch are computed based on the same model parameters used for the forward pass. At the endof each mini-batch, gradients from allMmicro-batches are accumulated and applied to update themodel parameters across all accelerators. This sequence of operations is illustrated in Figure 2c.If batch normalization [20] is used in the network, the sufﬁcient statistics of inputs during trainingare computed over eachmicro-batchand over replicas if necessary [21]. We also track the movingaverage of the sufﬁcient statistics over the entiremini-batchto be used during evaluation.3F: Forward  B: Backward. Train a 4 layer network with model parallelism. Model parallelism is needed for training a bigger DNN model on accelerators by dividing the model into partitions and assigning diﬀerent partitions to diﬀerent accelerators.IdleIdleIdle
But accelerators are signiﬁcantly under-utilized during model parallelism!(a). Training data ﬂow(b). Training timelineMIT 6.S965: TinyML and Eﬃcient Deep Learning Computinghttps://eﬃcientml.aiF0F1F2F3B0B1B2B3UpdateUpdateUpdateUpdateNaive Model Parallelism Suffers from Utilization
75GPipe: Eﬃcient Training of Giant Neural Networks using Pipeline Parallelism [Huang et al. 2018]
Only one device is computing at a time and others are waiting for it.
Theoretical utilization: 25% (low!)
Usual data parallelism utilization: ~75%IdleIdleIdleMIT 6.S965: TinyML and Eﬃcient Deep Learning Computinghttps://eﬃcientml.aiPipeline Parallelism
76GPipe: Eﬃcient Training of Giant Neural Networks using Pipeline Parallelism [Huang et al. 2018]•Split a single batch to micro batches
•[16, 10, 512] -> 
•[4, 10, 512]
•[4, 10, 512] 
•[4, 10, 512]
•[4, 10, 512]
•Motivation: model parameters are not changed during computation within a batch, thus we can pipeline computation and communication (a). Naive model parallelism
(b). Pipeline parallelismGpipe: Easy Scaling with Micro-Batch Pipeline ParallelismF0F1F2F3B0B1B2B3UpdateUpdateUpdateUpdate
F0,0F1,0F2,0F3,0UpdateUpdateUpdateUpdateF0,1F0,2F0,3F1,1F1,2F1,3F2,1F2,2F2,3F3,1F3,2F3,3B0,0B1,0B2,0B3,0B0,1B0,2B0,3B1,1B1,2B1,3B2,1B2,2B2,3B3,1B3,2B3,3MIT 6.S965: TinyML and Eﬃcient Deep Learning Computinghttps://eﬃcientml.aiF0,0F1,0F2,0F3,0UpdateUpdateUpdateUpdateF0,1F0,2F0,3F1,1F1,2F1,3F2,1F2,2F2,3F3,1F3,2F3,3B0,0B1,0B2,0B3,0B0,1B0,2B0,3B1,1B1,2B1,3B2,1B2,2B2,3B3,1B3,2B3,3Pipeline Parallelism
77GPipe: Eﬃcient Training of Giant Neural Networks using Pipeline Parallelism [Huang et al. 2018](a). Naive model parallelism
(b). Pipeline parallelismMicro-batch improvs the device utilizationUtilization: (25%)
Utilization: 57% (2.5x improvement)The more chunks (#num of micro batches), the higher of device utilization.F0F1F2F3B0B1B2B3UpdateUpdateUpdateUpdateGPipe Pipeline Bubbles•GPipe synchronizes weights at each mini batch
•Pro: maintain the original maths => potentially more eﬃcient SGD
•Con: pipeline bubble => ineﬃcient GPU use
Image from Narayanan eval. “PipeDream: Generalized Pipeline Parallelism for DNN Training” [SOSP 2019]PipeDream: Basic Idea•Partition layers into multi stages and process mini-batches (like GPipe)
•No synchronization points (unlike GPipe)
•Workers alternate between forward and backward passes on minibatches
•Gradients used to update model immediately
Image from Narayanan eval. “PipeDream: Generalized Pipeline Parallelism for DNN Training” [SOSP 2019]
Challenge 1: Stage Partitioning•How to partition model layers into the stages evenly?
•Throughput depends on the slowest stage in pipeline
•Solution: 
•Proﬁle layer-wise perf and comm perf
•Allows a stage to be replicated (DP)
•Uses dynamic programming to ﬁnd optimal partition and layer replication
Image from Narayanan eval. “PipeDream: Generalized Pipeline Parallelism for DNN Training” [SOSP 2019]Challenge 2: Work Scheduling•How to schedule forward and backward computation on a worker?
•Solution: 1F1B-RR
•Run one forward and one backward
•Round-robin across replicated stages
Image from Narayanan eval. “PipeDream: Generalized Pipeline Parallelism for DNN Training” [SOSP 2019]Challenge 3: Weight Versioning•How to ensure the same minibatch uses the same weight version across workers for forward and backward?
•Otherwise computation will be far oﬀ and training not able to converge
•Solution: Store multiple weight versions so that the backward and forward of the same minibatch
•Weights across workers can be diﬀerent!
Image from Narayanan eval. “PipeDream: Generalized Pipeline Parallelism for DNN Training” [SOSP 2019]PipeDream Pros and Cons•Pros
•no pipeline bubble
•very eﬃcient GPU utilization
•Cons
•need to store multiple versions of weights (and intermediate results)
•lossy computation (not the original maths)