11868/11968
Large Language Model Systems
Lei Li
1
220202023
2024
2000NN LM
2018BERT
20223Scale of LLMs
TransformerGPT1GPT2GPT3GopherPALMGPT4
LLaMA3.1DeepSeek -v3
Qwen3
001101001,00010,000
2016 2017 2018 2019 2020 2021 2022 2023 2024 2025size(B)â€¢Understand key techniques for modern LLM systems
oHow much resources do you need to train a 100B model?
â€¢Engineering skills to implement key components for LLM 
systems
oFast CUDA kernel for LLM
oScalable training system
oEfficient inference
â€¢Innovation: discover opportunities and solve new critical 
challenges in LLM system researchLearning Objective of This Courseâ€¢Capabilities of LLMs
â€¢Mathematical Foundations 
â€¢Challenges in LLM Systems
â€¢Logistics
5Outline
6
create a viral tweet about my Large language model systems 
course at Carnegie Mellon University (CMU, in Pittsburgh). Please 
refer to the course description at 
https:// llmsystem.github.io /llmsystem2025fall/
 Ready to build the future of AI? 
This Fall at CMU: "Large Language Model Systems" 
 Dive into LLM design, implementation & scaling.
 Tackle cutting -edge topics like agents, fine -tuning & efficiency.
 Learn in the AI innovation hub: Pittsburgh.
 Details Here
#AI #LLM #CMU # TechInnovation
LLM
Translate
Polish Email
Summarize
Answe r daily life questions
Math Calculation
Write Code
7Translation
8
Google MT
 ChatGPT /GPT4â€¢https://github.com/Ads -cmu/WhatsApp -Llama
â€¢Train on your WhatsApp chat data (about 10 chat histories 
are adequate), about a couple hours
â€¢Generate responses to mimic user style
9ChatBot  to Mimic a UserRewrite Text with Style
Please write the following sentence in Yoda's style. Here are a few 
examples. 
You have much to learn. -> Much to learn, you still have. 
I would say you have found someone, hmmm? -> Found someone, you 
have, I would say, hmmm? 
You should not drive that fast. ->
Drive that fast, you should not.
Commonsense Reasoning
Fill in the blank: Steven was disgusted that Robert chose to cook pork 
for dinner, because _ was a vegetarian.
"option1": "Steven",                           "option2": "Robert"
The correct answer is "Steven" 
Math Reasoning
12Roger has 4 tennis balls. He buys 2 more cans of tennis balls. Each 
can has 3 tennis balls. How many tennis balls does he have now?
Roger initially has 4 tennis balls. When he buys 2 more cans of tennis 
balls, and each can has 3 tennis balls, he adds 2 cans * 3 tennis balls 
per can = 6 tennis balls to his collection.
So, Roger now has a total of 4 initial tennis balls + 6 newly purchased 
tennis balls = 10 tennis balls.
13
ChatGPT  scored between a B - and B on the MBA exam.The Circored  process produces direct reduced iron in Latin America. The production 
process starts with a pile of iron ore fines. The ore is moved through seven process steps 
before it emerges as finished direct -reduced -iron briquettes. The steps and their capacities 
are as follows:
â€¢ Pre -Heater: 60 tons per hour 
â€¢ Lock Hoppers: 55 tons per hour 
â€¢ First reactor: this reactor holds up to 14 tons of iron and iron needs to spend 15 minutes in 
the reactor 
â€¢ Second reactor: this reactor holds up to 200 tons of iron and iron needs to spend 4 hours 
in the reactor 
â€¢ Flash heater: 62.5 tons per hour 
â€¢ Discharge: 59 tons per hour 
â€¢ Briquetting: three machines of 27.5 tons per hour each 
Where in the process is the bottleneck and what is the overall process capacity? Justify 
your answer.
Christian Terwiesch . Would Chat GPT3 Get a Wharton MBA? 2023.To identify the bottleneck in the Circored  process and the overall process capacity, we need to 
look at the capacities of each step. The bottleneck in any production process is the step with 
the lowest throughput rate, as it limits the rate at which the entire system can operate.
Here are the capacities given:
1.Pre-Heater : 60 tons per hour
2.Lock Hoppers : 55 tons per hour
3.First Reactor : Holds 14 tons total; each batch takes 15 minutes, so its rate is 
14tons0.25 hours=56 tons perhour0.25 hours14 tonsâ€‹=56 tons perhour
4.Second Reactor : Holds 200 tons total; each batch takes 4 hours, so its rate is 
200 tons4 hours=50 tons perhour4 hours200 tonsâ€‹=50 tons perhour
5.Flash Heater : 62.5 tons per hour
6.Discharge : 59 tons per hour
7.Briquetting : Three machines at 27.5 tons per hour each, so 
3Ã—27.5 tons perhour=82.5 tons perhour3 Ã—27.5 tons perhour=82.5 tons perhour
Now, let's calculate and compare the capacities of each step to identify the bottleneck.
The bottleneck in the Circored  process is the Second Reactor , with a capacity of 50 tons per 
hour . â€¦# Capacities of each step in tons per hour  
pre_heater  = 60 
lock_hoppers  = 55 
first_reactor  = 14 / 0.25  
second_reactor  = 200 / 4 
flash_heater  = 62.5  
discharge = 59 
briquetting = 3 * 27.5  
# Identifying the bottleneck (the smallest capacity) and overall process capacity 
capacities = { 
  "Pre -Heater" : pre_heater , 
  "Lock Hoppers" : lock_hoppers , 
  "First Reactor" : first_reactor , 
  "Second Reactor" : second_reactor , 
  "Flash Heater" : flash_heater , 
  "Discharge" : discharge, 
  "Briquetting" : briquetting } 
bottleneck_step  = min(capacities, key= capacities.get ) 
bottleneck_capacity  = capacities[ bottleneck_step ] 
(bottleneck_step , bottleneck_capacity )ChatGPT  even 
generates code to verify
It connects to a 
python engine, 
and executes the 
program to obtain 
result17def find_peaks (mountain):
    peaks = []
    for i in range(1, len(mountain) - 1):
        if mountain[ i] > mountain[ i - 1] and 
mountain[ i] > mountain[ i + 1]:
            peaks.append (i)
    return peaksYou are given a 0 -indexed array mountain. Your task is to find all 
the peaks in the mountain array. please write a python program. 
18
please draw a llama climbing along the "walking 
into the sky" on the campus of CMU
please draw an image for my slides about â€The 
power of Large Language Models"â€¢Capabilities of LLMs
â€¢Mathematical Foundations 
â€¢Challenges in LLM Systems
â€¢Logistics
20Outline
Probability Model for Next Token
21Santa Barbara has very nice ____beach
weather
snow  
bridges
cornPittsburgh is a city of ____0.5
0.4
0.01 
0.6
0.02 P(next word ğ‘¦ğ‘¡ | Prompt ğ‘¥,previous words ğ‘¦1:ğ‘¡âˆ’1)Mathematics of Language Model
22Prob ability â€œPittsburgh  is a city of bridgesâ€
=ğ‘ƒ(â€œğ‘ƒğ‘–ğ‘¡ğ‘¡ğ‘ ğ‘ğ‘¢ğ‘Ÿğ‘”â„â€ )âˆ™ğ‘ƒ(â€œğ‘–ğ‘ â€|â€œğ‘ƒğ‘–ğ‘¡ğ‘¡ğ‘ ğ‘ğ‘¢ğ‘Ÿğ‘”â„â€ )
âˆ™ğ‘ƒ(â€œğ‘â€|â€ğ‘ƒğ‘–ğ‘¡ğ‘¡ğ‘ ğ‘ğ‘¢ğ‘Ÿğ‘”â„  ğ‘–ğ‘ â€)âˆ™ğ‘ƒ(â€œğ‘ğ‘–ğ‘¡ğ‘¦ â€|â€¦)âˆ™ğ‘ƒ(â€œğ‘œğ‘“â€|â€¦)
âˆ™ğ‘ƒ(â€œğ‘ğ‘Ÿğ‘–ğ‘‘ğ‘”ğ‘’ğ‘  â€|â€¦)
Prob .ğ‘¥1..ğ‘‡=à·‘
t=1T
P(xt+1|x1..t)
Predicting using Neural Nets
(Transformer network, CNN, RNN)â€¢Pre-training on very large raw data (300B tokens) + small 
human feedback
â€¢Instruction following  â€“ easy to use through natural 
instruction
â€¢In-context learning  â€“ Generalize well to versatile tasks, by 
showing a few examples at use time.Why is ChatGPT changing AI landscapeType of Language Models
24
Decoder
EncoderEncoder -only
Masked LM
Non-autoregressiveEncoder -decoder Decoder -only
Autoregressive
Encoder
 Decoderâ€¢Masked Prediction: BERT ğ‘ƒ(ğ‘¥ğ‘šğ‘ğ‘ ğ‘˜ |ğ‘¥)
â€¢Can also be used to generate language
oe.g. NAT, REDER (reversible duplex model)
25Encoder -only Language Model
Transformer layer
[CLS] John        visited      [MASK]     yesterday    and     really    [MASK]   all  it    [SEP]  I like 
Madonna.NotNext                              Pittsburgh                                                 likeâ€¢Model the probability ğ‘ƒğ‘Œğ‘‹
â€¢Encoder may choose Bi -LSTM, Transformer, CNN
â€¢Decoder may choose LSTM, Transformer or Non -auto 
Transformer, CNN
26Encoder -Decoder Language Model
Decoder
Encoder
Zh: æˆ‘å–œæ¬¢å”±æ­Œå’Œè·³èˆ ã€‚En: I like singing and dancing.
En: I like singing and dancing.â€¢Causal Language Model: ğ‘ƒğ‘‹=Ï‚ğ‘›=1ğ‘ğ‘ƒğ‘¥ğ‘›ğ‘¥1..ğ‘›âˆ’1
â€¢Model choice: Transformer or ( uni-directional) LSTM
â€¢Most popular choice of LLM architecture
27Decoder -only Language Model
Decoder
[BOS]   Pgh   is       a       city      ofPgh       is  a       city      of   bridgesQuestion: Why is the sky blue?
The sky appears blue because â€¦
The sky is not always blue  â€¦LLM Learning Framework
Pre-training
Supervised Fine -
tuning (SFT)
 RLRaw text 
corpus (wiki, 
books, 
reddits )
 Preference data
â€¢Data: raw text corpus 
othe larger the better, often crawled from the web
oquality is important: Wikipedia, books, filtered Reddit posts
â€¢Design of Model Architecture
oSparse Attention, Mixture of Expert, etc.
â€¢Training loss: 
ocross -entropy loss for next token prediction. 
29Pre-training of Language Modelsâ€¢Cross entropy loss
ğ¶ğ¸=âˆ’1
ğ‘à·
ğ‘›=1ğ‘
logğ‘ƒğœƒ(ğ‘¥ğ‘›|ğ‘¥<ğ‘›)
30Training Objective
Decoder
[BOS]   Pgh   is       a       city      ofPgh       is  a       city      of   bridgesâ€¢Perplexity: for open -end generation
ğ‘ƒğ‘ƒğ¿ =exp âˆ’1
ğ‘à·
ğ‘›=1ğ‘
logğ‘ƒğ¿ğ‘€(ğ‘¥ğ‘›|ğ‘¥<ğ‘›)
â€¢Reference -based metric:  
oSEScore2/COMET/BLEURT: model -based score
oBLEU (or ROUGE): based on n -gram matching, old but classic
oInstructScore : explainable score [Xu et al 2023]
31Measuring Performance of Language Modelâ€¢Using downstream task performance metrics 
oNamed entity recognition: F1
oQuestion Answering: accuracy, matching rate
oCode generation: pass -rate @k
oRetrieval: ndcg
oSummarization: ROUGE
oTranslation: COMET/SEScore2/BLEU
32Measuring Performance of Language Modelâ€¢GPT3 trained on 300B tokens
â€¢Both model scale and data are important
33Performance of LLM
Perplexityâ€¢Capabilities of LLMs
â€¢Mathematical Foundations 
â€¢Challenges in LLM Systems
â€¢Logistics
34Outline
â€¢Generalist
oFormulate any AI tasks as token -based sequence generation
â€¢Instructibility
oInteractive system that is able to take natural or infinite commands
â€¢Agentic
oCall external tools, execute actions, take feedback
35Modern LLMsâ€¢Key system problem: compute (train/inference) larger  LLMs on 
bigger  datasets with fewer  resources (GPU/memory/power) 
faster
â€¢Right Abstraction
oDesigning useful building blocks that hide complexity from application 
developers â€“ computation at different levels
â€¢Tradeoffs
oWhat are the fundamental constraints and main success metrics?System Design for LLMâ€¢Common network layer 
oMulti -head Attention
oLayer norm
oDropout
oLinear layer
oNonlinear activation: Tanh, 
RELU, GELU
oSoftmaxâ€¢Low level operators:
oMatrix/Tensor multiplication
oReduction: summation, avg
oMap: element -wise applying
oMemory movement
37Computation in Language Models38System Challenges at Different Abstraction 
Levels
Operators on 
block of datafast CUDA/TPU kernel
data/model compression
DL 
Frameworkseasy dev/modify model
easy dev ML algorithm
distributed / 
parallel systemscaling to gigantic model
gigantic dataset
very long sequence/context
partitioning/scheduling/commâ€¢Scaling is all you need! â€“ scale up and scale down
â€¢Joint design of
oModel architecture
oTraining/inference Algorithms
oSoftware optimization (partitioning, scheduling, data movement, 
latency hiding)
oHardware Acceleration (device -specific instructions)LLM needs Model -Algorithm -System Co -designâ€¢Making computation fast is not enough
â€¢Data Transfer needs time
oLarge model has a lot of parameters
â–ªTransferring parameters/gradients across devices/nodes can take more 
time
oData sample in batch versus single single sequence
â€¢needs large working memory for long context LLM
40Computation versus Data Transferâ€¢What are good abstractions to build AI applications?
oUpper level: integrating models into a product system, tracking and 
improving product quality over time 
oMiddle level: building model training and inference software, streaming 
dataflow
oLower level: building GPU kernels, compilers, etc
â€¢A good abstraction successfully frees the programmer of one or 
more concerns (e.g., performance or failures) while supporting 
a wide range of apps on topProgramming Modelsâ€¢Capabilities of LLMs
â€¢Mathematical Foundations 
â€¢Challenges in LLM Systems
â€¢Logistics
42Outline
Instructor Prof. Lei Li
CMU
PhD
SCS, CSD
2011
UC Berkeley
PostDoc
EECS
2014
 2016
Principal Scientist
Institute of Deep Learning
2021
Founding Director
AI Lab
2023
Assist. Prof. 
CS
Research Area: 
Large language models
 (Safety, Agents, Alignment)
Machine Translation
AI drug discoveryâ€¢led and developed ByteDanceâ€™s  machine translation system VolcTrans  
and AI writing system Xiaomingbot , which have been deployed in 
products ( Toutiao , Douyin , Tiktok , Lark), serving over a billion users. 
â€¢Deep learning system research
oLightSeq : An acceleration library for Transformer models
â€¢Machine translation: LegoMT , VOLT(ACL21 best paper), mRASP , 
GLAT, WACO, InstructScore
â€¢Large Language Models: SALAM, ToolDec , ALGO, Ginsew
â€¢AI for Science: drug discovery (MARS), protein design ( EnzyGen )
44Highlight of Past Work45TA
Siqi 
OuyangYingyi  
HuangAndrea 
Vigano
office hour: see canvas page
â€¢If you want to learn how to create DL models and 
use TensorFlow, PyTorch
oTake 11685/11785
â€¢If you want learn how to use LLM but not interested in the 
underlying system
oTake 11667
â€¢You are unable to attend lecture, complete homework/project
oWarning: requires system implementationReasons not to take this class â€¦ 
  
If you plan to drop this 
class, please drop it 
soon so others can 
enroll!In order to take this course, you need to be proficient with:
â€¢Python and C++ development
â€¢Systems programming (e.g., 15 -213)
â€¢Basic ML/DL/NLP (e.g. 10701, 11785, 11711, or similar)
47Prerequisites â€¢The course will consist of programming -based homework 
assignments
â€¢Homeworks  are done individually
â€¢Through the assignments you will develop the main 
components for miniTorch  and train a Transformer -based 
LLM
â€¢Expected to write Python and C++/CUDA code
oprior knowledge of CUDA is helpful but not required. 
48Assignmentsâ€¢GPU: We already requested PSC bridge2 credits
â€¢Additionally, you may use google colab  on your own.
50Computing Resourcesâ€¢Inside a GPU
â€¢GPU Programming
51Next