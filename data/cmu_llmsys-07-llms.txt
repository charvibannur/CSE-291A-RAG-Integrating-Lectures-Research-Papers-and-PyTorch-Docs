Pre-trained LLMs
Lei Li
â€¢Sequence -to-sequence encoder -decoder framework for 
conditional generation, including Machine Translation
â€¢Key components in Transformer
oPositional Embedding (to distinguish tokens at different pos)
oMultihead attention
oResidual connection
oLayer norm
2Recapâ€¢T5
â€¢LLaMA
â€¢GPT3
3Todayâ€™s Topicâ€¢Model Architecture
oStandard encoder -decoder Transformer
oDecoding: beam search
â–ªBeam width=4, length penalty=0.6
â€¢Model Size
o T5-base: 220 million parameters
â–ª12 blocks, dff = 3072, dkv = 64, 12 -headed attention, dmodel = 768
o T5-3B
â–ª24 blocks, dmodel = 1024 , dkv = 128, dff = 16384, 32 -headed attention
oT5-11B
â–ª24 blocks, dmodel = 1024 , dkv = 128,  dff = 65536, 128 -headed attention
4T5
Raffel , Colin, et al. "Exploring the limits of transfer learning with a unified text -to-text transformer." The Journal of Machine Learning Research 21.1 (2020): 5485 -5551.  â€¢Pretraining: C4: filtered English corpus from Common Crawl
o750GB, still used often today
â€¢Supervised fine -tuning
oLanguage understanding/Text classification: GLUE/ SuperGLUE
oSummarization: CNN/Daily mail corpus
oQuestion answering: SQuAD
oTranslation: WMT English to German, French, Romanian
5Training
Pre-training Multitask Finetuning6T5 Pre -training: Recover randomly 
corrupted spans
Cloze -style QA Standard next token cross entropy loss, plus â€¦
15% of text are corrupted
Pre-train for 0.5 million steps on a batch size of 128 sequences of length 512. 
packing multiple seqs 65k tokens per batch, result in 34B trained tokens. 7T5 Multitask SFT with Task Instruction 
Unified format to put task instructions as natural language in the input, 
 enables transfer to new tasks. with more instruction tuning â” T0, Flan -T5 â€¢Model Architecture: 
oBased on Transformer decoder -only, with a few improvements.
oPre-normalization [GPT3]
oSwiGLU  activation function [ PaLM ]: Swish -Gated Linear Unit
oRotary Embeddings [ RoFormer ]
8LLaMA
Touvron , Hugo, et al. "Llama: Open and efficient foundation language models." arXiv  preprint arXiv:2302.13971 (2023).pre Layer Normalization
9
10FFN with SwiGLU
Swish activation
ğ‘ ğ‘¤ğ‘–ğ‘ â„ ğ‘¥=ğ‘¥ğœğ›½ğ‘¥
ğœğ‘¥=1
1+ğ‘’âˆ’ğ‘¥FFN (ğ‘¥)=ğ‘šğ‘ğ‘¥ (0,ğ‘¥â‹…ğ‘Š1+ğ‘1)â‹…ğ‘Š2+ğ‘2FFN with ReLU
FFN ğ‘†ğ‘¤ğ‘–ğºğ¿ğ‘ˆ ğ‘¥
=ğ‘†ğ‘¤ğ‘–ğ‘  â„ğ‘¥â‹…ğ‘Š1+ğ‘1â¨€ğ‘¥âˆ™ğ‘Š2+ğ‘2
â‹…ğ‘Š3+ğ‘2FFN with SwiGLU
dim=2/3 * 4ddim=4d11Rotary Embedding ( RoPE )
ğ‘“ğ‘¥ğ‘š,ğ‘š=cosğ‘šğœƒ âˆ’sin ğ‘šğœƒ
sin ğ‘šğœƒ cosğ‘šğœƒğ‘¥ğ‘š,1
ğ‘¥ğ‘š,2
ğ‘“ğ‘¥ğ‘š,ğ‘šâˆ™ğ‘“ğ‘¥ğ‘›,ğ‘›=ğ‘¥ğ‘šğ‘‡ğ‘…ğ‘›âˆ’ğ‘šğ‘¥ğ‘› make the attention weight depending (only) on position distance
Su et al. RoFormer : Enhanced Transformer 
with Rotary Position Embedding 2021.12Rotary Embedding ( RoPE )
ğ‘“ğ‘¥ğ‘š,ğ‘š
=cosğ‘šğœƒ1 âˆ’sin ğ‘šğœƒ1
sin ğ‘šğœƒ1 cosğ‘šğœƒ1
cosğ‘šğœƒ2 âˆ’sin ğ‘šğœƒ2
sin ğ‘šğœƒ2 cosğ‘šğœƒ2ğ‘¥ğ‘š,1
ğ‘¥ğ‘š,2
ğ‘¥ğ‘š,3
ğ‘¥ğ‘š,4
ğ‘“ğ‘¥ğ‘š,ğ‘šğ‘‡âˆ™ğ‘“ğ‘¥ğ‘›,ğ‘›=ğ‘¥ğ‘šğ‘‡ğ‘…ğ‘›âˆ’ğ‘šğ‘¥ğ‘› 
Su et al. RoFormer : Enhanced Transformer 
with Rotary Position Embedding 2021.â€¢Model Size
13LLaMA
Touvron , Hugo, et al. "Llama: Open and efficient foundation language models." arXiv  preprint arXiv:2302.13971 (2023).14LLaMA
â€¢Training Strategy
oTrained with the standard language modeling loss function: the 
average log probability of all tokens without label smoothing
oAuxiliary loss to encourage the softmax  normalizer to be close to 0
â€¢Pre-training Details
o Using only open -source data
Touvron , Hugo, et al. "Llama: Open and efficient foundation language models." arXiv  preprint arXiv:2302.13971 (2023).15GPT3
â€¢Model Architecture
o Based on the standard Transformer architecture
o With modified initialization, pre -normalization, and reversible tokenization
o Alternating dense and locally banded sparse attention patterns 
Brown, Tom, et al. "Language models are few -shot learners." Advances in neural information processing systems 33 (2020): 1877 -1901.
â€¢Model Size
16GPT3
Brown, Tom, et al. "Language models are few -shot learners." Advances in neural information processing systems 33 (2020): 1877 -1901.17GPT3
â€¢Training Strategy
o Unsupervised Pre -training
â€¢Training Details
Brown, Tom, et al. "Language models are few -shot learners." Advances in neural information processing systems 33 (2020): 1877 -1901.Computation
18
Brown, Tom, et al. "Language models are few -shot learners." Advances in neural information processing systems 33 (2020): 1877 -1901.â€¢https:// canvas.cmu.edu /courses/44373/quizzes/140384
19Quiz 4â€¢Pretraining: Mask -labeled recovery of random spans + next 
token prediction
â€¢Multitask supervised fine -tuning with instruction templates
oT5, InsturctGPT
â€¢Relative position: Rotary positional embedding
â€¢Smooth activation: SwishGLU
â€¢Sparse attention
20Summary â€“ Key Ideas in Modern Pre -
trained LLMshttps://github.com/llmsystem/llmsys_code_examples/tree/main
/minitorch_notebook
21Minitorch  Code walkthroughâ€¢Neural Machine Translation of Rare Words with Subword  
Units. Sennrich  et al. 2016.
â€¢SentencePiece : A simple and language independent 
subword  tokenizer and detokenizer  for Neural Text 
Processing. Kudo and Richardson. 2018
23Reading for Next Class