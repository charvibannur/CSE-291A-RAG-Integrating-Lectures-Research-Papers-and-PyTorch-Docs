1
The Early Bird Catches the Leak: Unveiling Timing
Side Channels in LLM Serving Systems
Linke Song*, Zixuan Pang*, Wenhao Wang, Zihao Wang, XiaoFeng Wang Fellow, IEEE,
Hongbo Chen, Wei Song, Yier Jin, Dan Meng, and Rui Hou
Abstract ‚ÄîThe wide deployment of Large Language Models
(LLMs) has given rise to strong demands for optimizing their
inference performance. Today‚Äôs techniques serving this purpose
primarily focus on reducing latency and improving throughput
through algorithmic and hardware enhancements, while largely
overlooking their privacy side effects, particularly in a multi-
user environment. In our research, for the first time, we dis-
covered a set of new timing side channels in LLM systems,
arising from shared caches and GPU memory allocations, which
can be exploited to infer both confidential system prompts
and those issued by other users. These vulnerabilities echo
security challenges observed in traditional computing systems,
highlighting an urgent need to address potential information
leakage in LLM serving infrastructures. In this paper, we report
novel attack strategies designed to exploit such timing side
channels inherent in LLM deployments, specifically targeting
the Key-Value (KV) cache and semantic cache widely used to
enhance LLM inference performance. Our approach leverages
timing measurements and classification models to detect cache
hits, allowing an adversary to infer private prompts with high
accuracy. We also propose a token-by-token search algorithm to
efficiently recover shared prompt prefixes in the caches, showing
the feasibility of stealing system prompts and those produced
by peer users. Our experimental studies on black-box testing
of popular online LLM services demonstrate that such privacy
risks are completely realistic, with significant consequences. Our
findings underscore the need for robust mitigation to protect
LLM systems against such emerging threats.
Index Terms ‚ÄîLLM, KV cache, Semantic cache, Side channels
I. I NTRODUCTION
LARGE Language Models (LLMs) are widely used
in applications such as chatbots [1], [2], search en-
gines [3], and coding assistants [4]. However, LLM infer-
ence is resource-intensive, requiring substantial computational
power and memory due to the model‚Äôs vast parameters, numer-
ous layers, and large context sizes. Improving LLM inference
performance has thus become essential, leading to solutions
such as weight quantization [5]‚Äì[9], model compression [10]‚Äì
[12], algorithm optimization [13]‚Äì[15], hardware advance-
The two lead authors contribute equally to the work.
Corresponding author: Wenhao Wang (wangwenhao@iie.ac.cn).
L. Song, W. Wang, W. Song, D. Meng and R. Hou are with the State
Key Laboratory of Cyberspace Security Defense, Institute of Information
Engineering, Chinese Academy of Sciences, and University of Chinese
Academy of Sciences.
Z. Pang and Y . Jin are with University of Science and Technology of China.
Z. Wang, X. Wang and H. Chen are with Indiana University Bloomington.
The authors from Institute of Information Engineering were supported by
the National Natural Science Foundation of China (Grant No. 62272452),
the Strategic Priority Research Program of the Chinese Academy of Sciences
(Grant No. XDB0690100) and the research grant from Huawei.ments [16], [17], and parallel processing techniques [18].
These approaches aim to reduce latency and improve inference
efficiency [19], though their privacy implications remain less
clear.
In this paper, we conduct the first security analysis of
performance optimization techniques employed by modern
LLM systems that serve multiple users or applications con-
currently. Our research reveals significant information leaks
arising from distinct side channels introduced by these tech-
niques. Specifically, current LLM performance optimizations
use shared caches to reduce computation and storage overhead
during inference. However, memory sharing, cache contention
and eviction and task scheduling among different users and
applications can interfere with user requests, creating notice-
able timing side channels. Exploiting these side channels can
expose private prompts from other users or applications.
LLM cache channels. In our work, we examined var-
ious caches in LLM systems, which not only reduce the
computational cost of LLM inference but also improve user
experience by lowering service latency. We found that these
caches can be misused to infer proprietary system prompts
or sensitive prompts from peer users. These prompts may
contain private user information and also hold commercial
value, as they enable an LLM to carry out various downstream
tasks without additional fine-tuning. We identified two primary
cache channels:
‚Ä¢Leakage from the KV cache. For each inference request,
the LLM maintains an in-memory state called the KV cache,
which is reused in every iteration throughout the request‚Äôs
entire service time. Due to the causal attention mask in LLMs,
each token‚Äôs activations are influenced only by preceding
tokens in the sequence. Thus, if multiple requests share a
common prefix, the key and value embeddings for those prefix
tokens are identical across sequences. To optimize the KV
cache‚Äôs memory usage, the system identifies matching prompt
prefixes across multiple requests and shares their key and value
embeddings in memory at runtime [14], [20]. This sharing
occurs when prompts include a common prefix, which fre-
quently happens with few-shot examples [21], chatbot system
prompts [22], or prompt templates [23]. For example, it has
been noted that Claude‚Äôs prompt caching feature can reduce
costs by up to 90% and decrease latency by up to 85% for
long prompts [24].
‚Ä¢Leakage from the semantic cache. The semantic cache boosts
LLM performance by caching responses based on the semantic
content of the requests. For example, for the prompts ‚ÄúgivearXiv:2409.20002v4  [cs.CR]  13 Aug 20252
me suggestions for a comedy movie‚Äù and ‚Äúrecommend a
comedy movie‚Äù, the LLM system can detect their semantic
similarity and return similar responses without querying the
LLM backend. Experiments show that when GPTCache is
integrated with OpenAI‚Äôs service, response speed can be
improved by a factor of 2 to 10 upon a cache hit [25].
Challenges and solutions. A straightforward way to exploit
these vulnerable caches is to directly search the prompt space
for one that triggers a cache hit. However, this method faces
multiple hurdles. First, the time difference resulting from
hitting a single cache block is often minimal and can blend
with GPU system noise and fluctuations in voltage and power,
making it difficult to detect and exploit. Second, the KV
cache only works when prompts share a common prefix,
limiting attack opportunities. Additionally, the vastness of the
prompt space makes it infeasible to systematically test every
potential prompt to find a cached one. Complicating matters
further, the attacker‚Äôs own requests might be cached during the
process, introducing additional noise and potentially causing
the victim‚Äôs cached data to be evicted.
To address these challenges, we developed various attack
strategies to exploit LLM side channels. Specifically, we use
a threshold-based classification model to detect token-level KV
cache hits based on offline timing measurements. We observe
that online detection accuracy can be substantially improved
with only a few repeated trials. To reduce the search space
for the KV cache channel, we propose an incremental search
algorithm that capitalizes on the requirement for prompts to
share a common prefix, allowing us to recover the victim‚Äôs
prompt token by token. For the semantic cache channel, we
design an algorithm to select the most representative prompts
as the attacker‚Äôs requests, given a targeted semantic focus. To
minimize interference from the attacker‚Äôs own requests, we
introduce a mechanism to clear cached data via batches of
irrelevant requests. For the semantic cache, our method also
ensures the attacker‚Äôs requests remain distinct by computing
their semantic similarities.
Experimental studies. In our study, we verified the pres-
ence of timing leakages in open-source projects, including
SGLang [26], Langchain [27], and GPTCache [28]. Building
on these findings, we demonstrate the feasibility of deducing
proprietary system prompts (i.e., prompt stealing attack ) and
inferring sensitive requests from neighboring users (i.e., peep-
ing neighbor attack ).
For the prompt stealing attack, our evaluation indicates that
the accuracy of detecting per-token cache hits or misses in
the KV cache is 99%, with a false positive rate (FPR) of
0.003. Using the incremental search algorithm, we recovered
the system prompt token by token, requiring an average of
111.46 queries per recovered token. This approach achieved an
average recovery accuracy of 89.0% and a corresponding FPR
of 0.04. For the peeping neighbor attack, our measurements
show an 81.4% accuracy in distinguishing hits from misses,
with an average FPR of 0.045 in a single trial. This accuracy
improved to 95.4% with a 0.056 FPR after 5 trials under
GPTCache‚Äôs default settings. We further observed that it is
possible to infer the documents processed by a victim userin a vulnerable LLM application, even when using standard
commodity LLM services. Moreover, our black-box study of
existing online services shows that popular LLM systems‚Äî
such as Claude, DeepSeek, and Azure OpenAI‚Äîemploy KV
or semantic cache sharing to cut costs, rendering them sus-
ceptible to timing side-channel attacks.
Finally, we propose initial defenses against these side-
channel risks. To mitigate KV cache leakage, we recommend
sharing prefix caches only in batches of at least ktokens
(k= 2,3,4, etc.). Although this increases the prompt search
space and thus the required number of guesses, the larger
timing differences for sharing multiple tokens also make
classifiers more robust. Consequently, attacks remain accurate
but incur higher query overhead. To address semantic cache
leakage, we advise anonymizing privacy-related content in
user inputs before performing semantic-similarity searches.
Preliminary experiments show that this measure adds modest
overhead (around 4%).
Contributions. Our paper makes the following contributions:
‚Ä¢New discovery . We identified new timing side channels in
both open-source and online LLM serving systems, arising
from the sharing of KV caches and semantic caches to lower
inference costs.
‚Ä¢Novel exploit strategies . We introduced new attack strategies
to leverage the inherent side channels in LLM inference
optimizations, enabling two distinctive attacks: prompt stealing
attack and peeping neighbor attack.
‚Ä¢Experimental validations, real-world measurements and
mitigations . We validated the side-channel leakages locally on
prominent LLM systems and conducted a black-box measure-
ment study of popular online LLM services. We also presented
preliminary mitigation measures for these risks.
Responsible disclosure. We disclosed our findings to all
relevant developers (SGLang, GPTCache, etc.) and LLM ser-
vice providers (OpenAI, Claude, Google Gemini, etc.) upon
identifying the side channels in September 2024. At the time of
this manuscript‚Äôs preparation, we received positive responses
from the SGLang team, which noted that we were among the
first two groups to report this issue, both within the same
week. Moreover, we were the first to raise the topic during
the SGLang development meeting, and we are now working
closely with their team on a resolution.
Comparison with concurrent and follow-up works (Ta-
ble I). Concurrently and independently to our research, Wu et
al. proposed PROMPTLEAK [29], an attack that exploits the
Longest Prefix Match (LPM) scheduling policy in SGLang,
which prioritizes requests with longer prefix matches, to leak
user prompts. Their method performs collision attacks by
sending carefully crafted batched prompts. When a request
shares more KV cache entries than others, SGLang prioritizes
it, enabling adversaries to extract victim tokens. In contrast,
our work identifies a more general KV cache side channel that
does not depend on LPM-based scheduling and thus remains
effective even without prefix-based prioritization. Moreover,
we are the first to reveal a semantic cache side channel,
propose practical mitigations, and conduct real-world measure-
ments to assess leakage risks in commercial LLMs.3
TABLE I
COMPARISONS WITH CLOSELY -RELATED WORKS .
Independent of
scheduling policy?Independent of
prompt templates?Cache eviction
supported?Real-world
measurement?KV cache
channel?Semantic cache
channel?Mitigations
PROMPTLEAK [29] % ! ! % ! % %
InputSnatch [30] ! % % % ! ! %
This work ! ! !!!!!
Inspired by our findings, Gu et al. conducted a subsequent
large-scale measurement study on prompt caching in real-
world LLM services [31], identifying its presence in 8 out
of 17 evaluated providers.
More recently, Zheng et al. investigated timing side channels
in LLMs through the InputSnatch attack [30]. However, their
work lacks the optimized search strategy we introduce for ef-
ficient request recovery and suffers from practical limitations.
Specifically, their KV cache attacks on vLLM can extract
blocks of 16 tokens but are limited to template-dependent
scenarios, requiring attackers to have prior knowledge of fixed
input structures. Additionally, their semantic cache attacks on
GPTCache aim to infer patterns from retrieved documents but
do not address self-induced interference from the attackers‚Äô
own queries and lack eviction strategies to mitigate adversarial
noise. In contrast, our approach enables template-free extrac-
tion, independent of application-specific formats, and incorpo-
rates eviction-controlled probing to systematically eliminate
interference. Furthermore, we validate the effectiveness of
our method in a realistic setting: a document summarization
service powered by a commercial LLM API.
Availability. All the code and datasets necessary to re-
produce our experiments are publicly available at: https:
//github.com/Maxppddcsz/llm-sidechannel. The demos for
our attacks are available at: https://sites.google.com/view/
early-bird-catches-the-leak/.
II. B ACKGROUND
A. LLM Serving Systems
In this paper, we explore the deployment of a shared LLM
to serve multiple users or applications within a computing
system. This setup is frequently observed in public services of-
fered by commercial companies (e.g., OpenAI‚Äôs ChatGPT) and
also applies to locally deployed shared enterprise LLMs , which
are tailored to handle specific tasks, process large volumes of
proprietary data, and meet unique business requirements. Ad-
ditionally, the rise of LLM-based applications‚Äîoften referred
to as AI agents or co-pilots‚Äîhas introduced a novel software
paradigm that merges the capabilities of LLMs with traditional
software functionalities. With the emergence of LLMs as
operating systems (e.g., AIOS [32]) and agents functioning as
apps, multiple LLM-based applications or agents can operate
on the same shared LLM, treating it as a foundational model.
These LLM agents are typically developed and deployed by
different teams or organizations. This concept also extends to
local LLM instances in a browser environment. For example,
Lumos [33] is a Chrome extension powered by Ollama, a
Retrieval-Augmented Generation (RAG) LLM co-pilot forweb browsing, running entirely on local hardware without
relying on remote servers.
In these scenarios, LLMs are typically optimized to achieve
efficient latency and throughput, focusing on memory usage
optimization, effective batching, and scheduling. However,
complications arise from memory sharing, cache contention
and eviction, and GPU scheduling across different users and
applications. Such factors can introduce interference among
concurrent requests, potentially leading to observable timing
side channels. As these users and applications are not all
mutually trusted, sensitive information leakage becomes a
concern. This includes the potential exposure of other users‚Äô
confidential data‚Äîsuch as sensitive queries, proprietary sys-
tem prompts, and processed documents‚Äîthrough timing side
channels.
B. Serving Frontend
LLM serving modes. The LLM service offers two operation
modes. In non-streaming mode, the response is fully generated
and then delivered once the request has been processed.
However, for long completions, this approach can result in an
extended waiting period, possibly lasting several seconds. To
achieve faster responses, the streaming mode is available. In
this mode, the LLM emits tokens sequentially, allowing users
to view the beginning of the completion while the remaining
tokens are still being generated. Streaming is the preferred
method for interacting with LLMs, especially in chatbot
scenarios where real-time conversation is essential. Popular
LLM applications (e.g., Bing Copilot [34], ChatGPT [1]) use
a system prompt containing task definitions, examples, and
safety rules to guide their behavior. This prompt is typically
static and shared among all users.
Metrics. Latency measures how long it takes for an LLM
to respond to a user‚Äôs query, shaping users‚Äô perceptions of
speed and efficiency in generative AI applications. Low latency
is particularly important for real-time interactions, such as
chatbots and AI copilots. Time to First Token (TTFT) is
the interval from the moment a user submits a prompt until
receiving the first token of the response. It reflects the initial
processing delay and serves as a crucial indicator of user-
perceived responsiveness. Throughput, on the other hand,
represents how many requests or tokens an LLM can process
within a given time window. Since requests per second is
affected by the model‚Äôs total generation time‚Äîwhich depends
on output length‚Äîtokens per second is often used as the
key metric for measuring throughput. This paper examines
the risks arising from optimizing an LLM‚Äôs serving latency4
and employs TTFT as the primary metric for side-channel
observations.
C. Serving Backend
Most LLMs rely on the Transformer architecture, which
uses the attention mechanism [35] to pinpoint the most relevant
parts of the input. Core to this mechanism are Query (Q), Key
(K), and Value (V) embeddings: Q represents what the model
is seeking at the current position, K encodes how to match
relevant information across the sequence, and V holds the
actual data to be retrieved when a match occurs. Leveraging
scaled dot-product attention, the model processes Q, K, and V
to selectively focus on the most pertinent parts of the input.
LLM inference consists of two stages: the prefill phase and the
decoding phase . The prefill phase processes the entire request
prompt to produce the first output token, while the decoding
phase generates subsequent tokens one by one.
Prefill phase. During the prefill phase, the LLM takes the
request prompt as input and converts it into a sequence of
tokens. Each token is transformed into a numerical represen-
tation, called an embedding, which the model can process. In
this phase, the LLM computes the K and V embeddings for
each token across every attention layer, enabling the generation
ofthe first token of the response in a single step.
Decoding phase. In the decoding phase, the LLM generates
each subsequent token by using the prefilled information and
the single token produced in the previous step. For every layer,
the engine computes the Q, K, and V embeddings for the
new token and performs attention against all existing context
tokens. Unlike the prefill phase, the decoding phase processes
only one token at a time.
Memory management of KV cache. The attention mecha-
nism in LLMs requires computing pairwise similarities among
tokens in an input sequence, which leads to quadratic com-
plexity with respect to sequence length [36]. To address this,
KV caching stores the key and value embeddings in GPU
memory, eliminating redundant computations and allowing the
computation cost to scale linearly with sequence length.
Originally, LLM serving systems would statically allocate
a sizable portion of memory for storing the KV cache,
due to the unpredictable lengths of model outputs. However,
this led to significant internal and external fragmentation.
To mitigate these issues, vLLM introduced PagedAttention,
which divides the KV cache into blocks and accesses them
through a lookup table [14]. This table maps virtual cache
blocks to physical locations in GPU memory, enabling effi-
cient memory sharing across different requests. Modern LLM
inference frameworks such as Nvidia‚Äôs TensorRT-LLM [37]
and Huggingface‚Äôs TGI [38] incorporate similar concepts, but
the security implications of sharing KV caches have not been
thoroughly studied, leaving a critical gap in existing research.
D. Threat Model
In this paper, we examine the security implications of
deploying a shared LLM to serve multiple users or appli-
cations within a single computing system. Specifically, weconsider two main scenarios. First, an LLM service provider
offers public APIs that registered users can employ to send
requests, all of which are processed by the same underlying
serving system. In this context, a victim user may establish
a proprietary system prompt to power a widely used LLM
application. Meanwhile, an attacker could leverage the same
LLM APIs to infer this system prompt, thereby gaining poten-
tial financial benefits or circumventing the safety instructions
encoded in the prompt. Second, public LLM applications‚Äî
such as chatbots (e.g., OpenAI‚Äôs GPT-4) or document analysis
services (e.g., AnythingLLM [39], Klu [40], etc.)‚Äîhandle
concurrent requests from multiple users via the same LLM
serving system. If the application itself relies on a public
LLM API, these requests are generally routed through the
same developer‚Äôs API key. An attacker could register as
a user of such an application to discover whether specific
requests have been submitted by others. For instance, they
might seek to determine whether a user has shown interest
in a particular topic or uploaded a specific file. They could
also monitor requests over time to detect private attributes or
sensitive personally identifiable information (PII) (Table III).
In both scenarios, the attacker‚Äôs and the victim‚Äôs requests
share the same platform and thus make use of the same
caches. This shared environment can produce interference and
create observable timing side channels‚Äîthe core subject of
our investigation. The attacker needs only black-box access to
the underlying model, without knowledge of its architecture or
weight parameters. However, the attacker must first examine
the system‚Äôs leakage profile in an offline phase, analyzing
how different inputs affect timing. This analysis helps them
craft inputs that exploit the timing discrepancies introduced
by cache sharing.
In this paper, we explore the side-channel risks linked to
both local and remote LLM services. For local settings, we
assume a stable network connection between client and server.
For remote services, previous works‚Äîsuch as NetCAT [41]
and NetSpectre [42]‚Äîhave addressed mitigating noise caused
by unstable connections and jitters, particularly in CPU cache
side-channel attacks. Extending such noise-reduction strate-
gies to remote LLM scenarios remains an avenue for future
research. We do not consider hardware side channels tied
to GPU micro-architectures (e.g., GPU caches [43], residue-
based leakage [44], or power/frequency side channels [45]).
Instead, our focus lies on software caches maintained by the
LLM serving system, making our attacks applicable across
various hardware platforms (CPUs, GPUs, ASICs, etc.).
III. A TTACKS
A. Overview
Creating effective prompts is a challenging task that re-
quires substantial effort, particularly in scenarios like in-
context learning where extensive data is needed to optimize
LLM performance. Furthermore, prompts can include personal
or sensitive information, making them valuable assets that
must be safeguarded. For instance, Samsung Electronics has
prohibited employees from using generative AI tools like
ChatGPT to prevent accidental disclosure of confidential data
to OpenAI [46].5
In our research, we investigated two types of attacks. The
first is the prompt stealing attack (PSA) , which targets system
prompts. A system prompt defines the model‚Äôs operational
behavior and may incorporate carefully crafted business logic,
private data, or safety-related instructions. Consequently, LLM
application developers treat it as confidential intellectual prop-
erty [47]. Moreover, once exposed, the system prompt could
facilitate other attacks, such as jailbreaking. The second is the
peeping neighbor attack (PNA) , which focuses on uncovering
the semantics of another user‚Äôs prompt. Since these prompts
may contain personally identifiable information (PII) or other
sensitive data, any disclosure poses a substantial risk to user
privacy. There are three entities involved in these attacks:
the server ( S), the victim user ( C), and the attacker ( A).
The attacker‚Äôs goal is to infer the prompt submitted by the
victim user. The attack proceeds in two phases. In the offline
phase , the attacker studies how a request alters the server‚Äôs
state and how these modifications manifest in the latency
of subsequent requests. Critically, these timing profiles stem
primarily from the system‚Äôs optimization techniques rather
than from a specific model or parameter set.
In the online phase , the attacker leverages insights gained
during the offline phase to craft requests that exploit the
identified timing properties. Initially, Sis in state State 0.
WhenCissues a request, the state changes to State 1, reflecting
updates like modifications to the KV or semantic cache. These
state transitions can affect the performance of later requests.
To track the system state, Aregularly sends a request rat
intervals starting from time tstart, measuring the resulting
latency l=tend‚àítstart, where tenddenotes the time point
when the first token in the response arrives. By analyzing these
latency readings, Acan infer the prompt submitted by C.
B. Prompt Stealing Attacks (PSA)
Background. KV caching is a widely adopted optimization in
LLM serving systems, retaining the key and value embeddings
from earlier inference steps to circumvent redundant compu-
tations during autoregressive text generation. Recent innova-
tions, notably PagedAttention [14], improve on this concept by
allowing the reuse of cached embeddings when prompts share
common text segments, such as system messages, templates, or
documents frequently included across multiple prompts. Rep-
resentative implementations include automatic prefix sharing
in vLLM [48], which detects shared prefix segments at runtime
for KV cache reuse, and RadixAttention in SGLang [20],
which efficiently manages shared KV caches for prompts
containing common prefixes.
The side channel. Sharing KV caches can introduce a timing
side channel. During the prefill phase of LLM inference,
If a request‚Äôs prefix matches one already stored in the KV
cache, it will be processed more quickly. Because most LLMs
stream their outputs (i.e., token by token), it is possible to
measure the fine-grained Time to First Token (TTFT) and
detect timing discrepancies associated with cache hits versus
misses. Assuming a stable network latency, we estimate the
timing gap under a typical LLM deployment [49] as follows.
Consider a model with 7 billion parameters (e.g., Llama-7B)running on an A100 GPU with 312 TFLOPS of computational
power and a memory bandwidth of 1.5 TB/s. In the best case,
with full GPU utilization, a cache miss for a single token
during the prefill phase may take:
prefill time (miss) = #tokens √ó#parameters
GPU compute bandwidth
=1√ó(2√ó7B)FLOP/token
312TFLOP/s
‚âà0.045ms.
By contrast, if the token hits the cache, the time is dom-
inated by loading the precomputed KV cache from HBM
(assuming 16-bit precision parameters):
prefill time (hit) = #tokens √óKV cache per token
GPU memory bandwidth
=1√ó(2√ó4096√ó32)√ó2Bytes/token
1.5TB/s
‚âà0.35¬µs.
These gaps become larger for more complex models or
when serving multiple requests concurrently. For instance, on
aLlama-3.1-70B-Instruct-GPTQ-INT4 [50] model
(70 billion parameters at 4-bit precision), the prefill time for
a single token miss is about 0.45 ms, while a hit is roughly
0.22¬µs.
These timing differences underpin the prompt stealing at-
tack, which leverages KV cache sharing. Specifically, an
attacker sends a request to the LLM and observes TTFT to
detect whether the victim‚Äôs prefixes match. Since KV cache
sharing occurs only for prompts with the same prefix, we
devised an incremental search algorithm to recover prompts on
a token-by-token basis. We present this algorithm and evaluate
its real-world efficiency in Section IV-A.
C. Peeping Neighbor Attacks (PNA)
Background. Semantic caching (e.g., GPTCache [25], [28])
stores prior requests and their corresponding responses. Upon
receiving a new request, it measures semantic similarity with
cached requests. If the similarity surpasses a certain threshold,
the system returns the cached response; otherwise, it queries
the LLM again. Semantic caching can significantly reduce
costs and enhance performance, and is integrated into major
LLM frameworks like LangChain [27] and LlamaIndex [51].
The side channel. Unlike KV caching, which reuses data only
for identical prompt prefixes, semantic caching allows reuse
based on semantic similarity beyond a predefined threshold.
However, sharing a semantic cache among multiple users
can inadvertently reveal their requests. Cache hits provide
responses in mere milliseconds, whereas cache misses can take
several seconds‚Äîcreating a clear timing difference that can be
exploited by an attacker. This discrepancy enables the attacker
to infer the semantics of concurrent requests issued by nearby
users, a scenario we refer to as the peeping neighbor attack .
Despite this, when the attacker tries to match a victim‚Äôs request
semantically, the attacker‚Äôs own requests may also be cached,6
introducing noise into subsequent attempts. To address this
challenge, we propose an efficient search algorithm that both
minimizes the caching effects of the attacker‚Äôs own requests
and improves the detection rate for the victim‚Äôs request. We
describe this algorithm and illustrate how it can recover private
information from neighboring users‚Äô prompts in Section IV-B.
IV. S IDE-CHANNEL ANALYSIS AND EVALUATION
In this section, we present our empirical analysis of the
identified side channels and describe strategies for their ef-
ficient exploitation. All experiments were conducted on a
Lenovo server equipped with two Intel Xeon E5-2678 v3
CPUs (12 cores at 2.50 GHz each), 100 GB DDR4 memory,
and two NVIDIA A100 PCIE GPUs (80 GB memory each).
The system ran Ubuntu 22.04 (kernel 5.15.0-125-generic) with
GPU driver 550.127.05, CUDA 12.4, and PyTorch 2.4.0. We
used open-source models from the Llama family as the under-
lying LLM, adhering to their default hardware and software
configurations for all evaluations.
A. Analysis on PSA
Attack setup. With increasing concerns about client data
leakage in public LLM services, enterprise LLMs have become
increasingly popular [52], [53]. In our study, we focus on
a use case where the LLM API service is constructed from
open-source projects within a local network environment . As
described in Section II-D, we consider a scenario in which a
victim develops a popular LLM application (e.g., a chatbot)
using a proprietary system prompt via the LLM service. The
attacker interacts with this LLM application over the local
network, measures the TTFT, and attempts to uncover the
system prompt based on timing discrepancies. Specifically,
the LLM service uses the SGLang backend API server [26],
which supports KV cache sharing for common prefixes. No-
tably, LLM API servers including OpenAI allow users to
define various roles, such as ‚Äúsystem‚Äù and ‚Äúuser‚Äù, within
their requests [54], or send requests directly. Based on this
capability, we categorize requests into two modes: synthesized
and direct. The victim‚Äôs LLM chatbot, built on the FastChat
framework [55], explicitly supports both, as shown in Figure 1.
In direct mode, the user sends requests directly to the SGLang
backend, whereas in synthesized mode, the full prompt is
created by concatenating messages from each role according
to predefined templates.
We consider that the victim‚Äôs chatbot employs a proprietary
system prompt for the ‚Äúsystem‚Äù role in the synthesized mode,
while the user‚Äôs inputs fall under the ‚Äúuser‚Äù role. In the
synthesized mode, this system prompt is prepended at every
conversational turn to form the complete prompt sent to the
SGLang backend. Notably, BloombergGPT [56] serves as a
real-world example of a purposely built LLM for financial
use, deployed for internal use at Bloomberg. It leverages 3-
shot prompting to handle domain-specific tasks more effec-
tively, safeguarding sensitive data and maintaining control over
proprietary financial processes. As illustrated in Figure 2, an
attacker can masquerade as a chatbot user, submitting either a
direct request or a synthesized request that includes the static
[INST]<<SYS>>Direct reques t
Synthesi zed request
‚Ä¶..<<SYS>> You are a ‚Ä¶ Tell me
Separator System Prompt User PromptTell meFig. 1. LLM API servers like OpenAI allow user input through both direct
requests (top) and synthesized requests via a template (bottom).
system prompt. When the LLM processes these synthesized
prompts, it retains the separator and system prompt in the KV
cache used by the SGLang backend, expediting subsequent
requests that share partial prefixes of the system prompt. In
this attack, the attacker first submits a synthesized request to
cache the system prompt, then employs direct queries to reveal
it through timing leakages.
Characterizing the leakage. We began by examin-
ing the timing difference between a cache hit and a
miss for a single token, comparing multiple model sizes.
Specifically, we tested SGLang v0.3.0 [26], which orga-
nizes KV cache blocks in a radix tree to facilitate effi-
cient prefix matching and reuse. Two models were eval-
uated: Llama-3.1-8B-Instruct [57] and Llama-3.
1-70B-Instruct-GPTQ-INT4 [50]. System prompts of
varying lengths were derived from the gabrielchua/
system-prompt-leakage [58] dataset on Hugging Face.
We measured TTFTs using 15-token prompts where only the
last token differed, resulting in a shared-prefix length differing
by exactly one‚Äîsignifying cache hits versus misses‚Äîacross
4,000 runs. Figure 3 illustrates the resulting time distributions,
revealing a pronounced distinction between hit and miss
scenarios for both models.
Based on these observations, a straightforward classifier
can be built to categorize the last token in the prompt as a
hit if its latency is below a predefined threshold. However,
as prompts grow longer, the latency also tends to rise due
to increased computational demands, necessitating a distinct
length-dependent threshold for each prompt length. To over-
come this limitation, we measured the relative latency differ-
ence between hit and miss cases for last tokens across prompts
of varying lengths (1‚Äì200 tokens). Our findings reveal that
this difference is stable, independent of prompt length, and
significantly larger than the variance within miss cases. This
stability allows us to use a classifier with a single threshold
instead of separate classifiers for different lengths, ensuring
reliable hit detection.
In real-world evaluations of our classifier, we noted that
TTFT varies due to factors such as GPU system noise and
power fluctuations, weakening the effectiveness of a fixed
classification threshold, as shown in Figure 3. To mitigate this,
we first construct a miss prompt by appending a rare token
to a known prompt (i.e., requests without predicted tokens).
Then, within a brief time window, we simultaneously collect
TTFT data for both the miss and target prompts within a
brief time window, using their difference for threshold-based
classification.
To illustrate the classifier‚Äôs effectiveness,7
Attacker
Victim
System Prompts Dataset
System 
Prompt
Chat 
TemplateUnseen Info
Known Info1. Direct SendUser Prompt
AttackerSeperator‚ë†. FixSGLang Chatbot Frontend
User
Prompt
A. Direct request: Attack
B. Synthesized request: Trigger system promptAttack Prompt
System Prompt User PromptPromptsSGLang Backend
KV Cache
TTFT
Return
‚ë°.Send requests
‚ë¢.Return and record
Fig. 2. Overview of prompt stealing attacks.
0.029 0.030 0.031 0.032 0.033 0.034
Latency (s)020406080100120140160FrequencyHIT
MISS
(a). Llama-3.1-8B-Instruct
0.14 0.16 0.18 0.20 0.22 0.24 0.26
Latency (s)020406080100FrequencyHIT
MISS (b). Llama-3.1-70B-Instruct-GPTQ-
INT4
Fig. 3. Latency distribution of one token hit and miss. We used 2
representative models of different sizes: Llama-3.1-8B-Instruct
and Llama-3.1-70B-Instruct-GPTQ-INT4 . We used the
Llama-3.1-70B-Instruct-GPTQ-INT4 model for subsequent
evaluations.
consider an example using the Llama-3.
1-70B-Instruct-GPTQ-INT4 [50] model under
our evaluation settings. We build a classifier to determine
whether the last token of prompts hits the KV cache. We test
the classifier using 4,000 hits and 4,000 misses drawn from
randomly selected system prompts, achieving a TPR of 0.88
and a FPR of 0.10. To further mitigate noise, we employ
multi-sampling by collecting nTTFT samples per token. The
token is classified as a hit only if the number of hit detections
exceeds a threshold k. With n= 10 andk= 5, the final
classifier attains a TPR of 0.99 and an FPR of 0.003.
End-to-end attacks. We built a local chatbot using
the FastChat framework as the victim application. Its
backend API server is configured with SGLang v0.3.0,
which supports KV cache sharing for common pre-
fixes. In this evaluation, we used the gabrielchua/
system-prompt-leakage [58] dataset from Hugging
Face, containing over 350,000 synthetic system prompts. We
applied Llama-3.1-8B-Instruct [57] to compute em-
beddings for each prompt and visualized the results using
Uniform Manifold Approximation and Projection (UMAP),
which reduces dimensions of datasets and enables effective
visualization of data distribution [59]. UMAP visualization
confirmed substantial semantic diversity across the dataset,
reflecting the varied topics and styles found in real-world
prompt engineering. As shown in Figure 4, this semantic
heterogeneity increases as the dataset expands, significantly
complicating predictive modeling [60]. Consistent with the
threat model detailed in Section II-D, we assume the attacker
has a public dataset of system prompts that mirrors those of
the victim. To more accurately reflect real-world conditions‚Äî-
where production systems typically implement a small number
5
 0 5 10
UMAP Dimension 14
2
0246810UMAP Dimension 2UMAP Projection of Embeddings(a). Random sample of 10k prompts
from the dataset
10
 5
 0 5 10 15 20
UMAP Dimension 110
5
05101520UMAP Dimension 2UMAP Projection of Embeddings(b). Complete dataset (over 350k
prompts)
Fig. 4. UMAP projection demonstrating the dataset‚Äôs heterogeneity: As
sampling density increases, the visualization reveals expanded dimensional
ranges with distinct semantic clusters and isolated points, highlighting the
complex multidimensional structure of the prompt dataset.
of carefully crafted system prompts‚Äî-we randomly select
only 200 prompts as the victim‚Äôs configuration while reserving
the remainder as the attacker‚Äôs training corpus. This experi-
mental design deliberately simulates the ‚Äúneedle in a haystack‚Äù
challenge faced in realistic attack scenarios, where adversaries
must identify specific target prompts within a substantially
larger heterogeneous corpus.
To streamline the search process, as illustrated in Figure 6,
we propose an incremental token-by-token approach to recover
the target prompt. This approach relies on multiple compo-
nents: a classifier for validation, a next-token predictor to
estimate token probabilities, and a sampler that selects candi-
date tokens based on the predictor‚Äôs temperature setting. The
next-token predictor is fine-tuned on the public system prompt
dataset available to the attacker, allowing it to forecast the next
token given the already retrieved tokens. For each candidate
token at position i, we feed the partially reconstructed prompt
and miss prompt into the LLM to obtain TTFT difference
(same method in Section IV-A), which then serves as the input
to the classifier . If the classifier identifies this token as a
cache hit, the token is appended to the prompt; otherwise,
arepetition penalty is applied by adjusting the probability
distribution of the current token (in our implementation, this
penalty is applied by halving the sampling probability for an
incorrect token in the next round).
In constructing the next-token predictor , we adopt
Llama-3.1-8B-Instruct [57] as base model, chosen
as it shares an identical tokenizer with the victim LLM.
We fine-tune this model using the attacker‚Äôs training dataset.
Given the partially recovered prompt and the chosen tem-
perature, the predictor generates a probability distribution
for the next token. Temperature scaling introduces variability
into the prediction. Our predictor is not heavily optimized;
it was fine-tuned on a single NVIDIA A100 PCIE GPU for8
def get_ttft(text):
start_time = time.perf_counter()
# In stream mode, max_tokens = 1
response = requests.post(
{..., max_tokens=1,},
stream = True
)
for line in response.iter_lines():
if line:
end_time = time.perf_counter()
break
ttft = end_time - start_time
return ttft
def complete(text):
# Trigger the system prompt first
response = client.chat.completions
.create(...)
for line in response.iter_lines():
if line:
data = json.loads(line)
if data.get(
"end_of_sequence",
False):
break
# Wait for complete
complete(triggering_prompt)
# Short delay to ensure KV cache is updated
time.sleep(0.2)
ttft = get_ttft(predicted_prompt)
Fig. 5. Code for measuring response latency in PSA.
only a few hours with limited training resources. With larger
models, more epochs, and bigger batch sizes, the predictor‚Äôs
performance could be further improved.
To obtain TTFT values in the end-to-end scenario, the
attacker first clears both the victim‚Äôs and the attacker‚Äôs requests
from the cache. Next, the attacker measures the TTFT for their
own request after the victim‚Äôs system prompt has been cached.
Below, we describe how the TTFT is gathered and how caches
are flushed.
‚Ä¢Timing measurement . As shown in Figure 5, the attacker
begins by issuing a synthesized request containing the targeted
system prompt. Once the end-of-sequence token is received in
the POST response, a short delay is introduced, ensuring the
system prompt resides in the KV cache. The attacker then
sends a direct request via a POST call using the anticipated
prompt, configured to generate only one token of output.
The TTFT is computed as the interval between sending the
request and detecting the first non-blank token in the streamed
response.
‚Ä¢Flushing the caches through eviction . We observed SGLang
provides a flush cache API [61] that efficiently clears the
cache. However, for our end-to-end attack scenario, we chose
not to use this API, as it is unlikely to be accessible to
attackers in real-world environments. Instead, we employed
a more robust method of evicting the KV cache by issuing
batches of irrelevant requests. Under default SGLang settings,
sending 15 such requests (each containing about 200 tokens)
was sufficient to trigger eviction in about 5 seconds. This
False: Penalty
User
Prompt
Next -Token Predictor
‚ë¢. Append
Predicated Prompt
        (ùíä Tokens)
Chatbot
‚ë§. RecordTTFTs
‚ë£. n TrialsClassifier 
‚ë•. JudgeTrue: Append New Token
Probability
GenerationTokenprompt ùëùùëó 
ùë°ùëúùëòùëíùëõ ùëó 
‚ë°. Sample new token
 Sampler‚ë†. SendFig. 6. Efficient token-by-token request recovery.
approach proved successful in 100% of our 10,000 tests.
We evaluated both the token recovery accuracy and the
average number of queries required per token. The results
indicate a success rate of 89.0%, an FPR of 0.04, and an
average of 5.57 guesses with 111.46 attack queries needed
per recovered token when n= 10 . The corresponding cost
per token recovery is approximately $0.16 using OpenAI o1,
$0.012 with OpenAI o3-mini, and $0.001 with Deepseek-
Chat (as of March 5, 2025). Out of 200 victim prompts,
we successfully recovered an average of 11.58 tokens for
the top 100 prompts, and 17.9 tokens on average for the
top 50 prompts. The maximum number of tokens recovered
for a single prompt was 81, achieved with just 513 total
guesses. Table II presents several examples of target system
prompts alongside the prompts recovered via PSA. Notably,
we limited the token prediction attempts to a maximum of 80
per position, ceasing further attempts if the correct token was
not identified within these trials. Consequently, the primary
constraint in reconstructing entire system prompts stems from
the precision of the predictor rather than the accuracy of
the classifier‚Äîan aspect that falls outside the scope of this
study. In real-world attacks, the attacker can recover additional
tokens by deploying a more advanced next-token predictor and
increasing the maximum number of attack queries. A demo for
the end-to-end attack is presented in our website [62].
B. Analysis on PNA
Attack setup. In the PNA attack, we note that not all user
queries contain sensitive data. An attacker is unlikely to pursue
generic requests like ‚ÄúWhat‚Äôs the weather today?‚Äù. Instead,
they are expected to focus on specific requests more likely
to reveal private information. For example, a request such
as ‚ÄúDraft a travel plan for my husband and me to Rome
in September for a 3-day stay at the Hilton hotel‚Äù could
expose personal and location details. By identifying such high-
risk queries, the attacker can exploit timing side channels to
recover private information from other users‚Äô requests. For this
purpose, the attacker could compile a list of privacy-related
prompts from online sources (Table III). The attacker has
obtained a list of private attributes, and its goal is to discover
connections between entities and these private attributes, e.g.,
whether a specific user is linked to a particular medical
condition.9
TABLE II
EXAMPLES OF RECOVERED SYSTEM PROMPTS ,INCLUDING THE NUMBER OF ATTACK QUERIES AND RECOVERED TOKENS . W E ONLY LISTED TRAVEL
PLANNING -RELATED PROMPTS TO DEMONSTRATE THE PSA‚Äô S ABILITY TO RECOVER DIVERSE EXPRESSIONS .
No. Recovered system prompts#queries, #recovered
tokens
1In your role as a dedicated travel itinerary assistant, you will craft well-organized travel plans based on user
preferences and interests. Gather information from the user‚Äôs inputs such as destinations, travel dates, and preferred
activities to formulate a comprehensive itinerary. The output should include: dates, activities planned for each day,
estimated costs, and important local information such as culture or tips. Emphasize clear, organized, ¬∑¬∑¬∑10,260/81
2You are programmed to function as a travel itinerary planner focusing exclusively on creating unique travel
experiences. Provide tailored itineraries for destinations worldwide. ¬∑¬∑¬∑3,620/26
3Imagine you are a travel itinerary planner specializing in creating unique and personalized travel experiences. Your
role is to craft itineraries that cater to the diverse interests and needs of travelers. ¬∑¬∑¬∑1,360/32
TABLE III
EXAMPLES OF USER PROMPTS THAT CONTAIN PRIVATE ATTRIBUTES .
Use cases Prompts
Healthcare Compose a meeting agenda for an interdisciplinary team discussing the treatment plan for [Name] with [medical condition].
Travel planningI‚Äôm [flying/driving] to [destination] with [Name] for a leisurely trip, and we‚Äôll be staying at [hotel] for [number of days]. Can
you create a comprehensive packing list organized by category?
Business planningAct as an expert business plan writer, and help me generate a product and services section of my [business type] business
plan. My company is [business] called [business name] that specializes in [USP or specialization].
Performance reviewI‚Äôm preparing for the annual performance review of an employee named [Name]. [Name]‚Äôs role involves [roles]. Draft a
performance review for [Name] and suggesting improvements in [area of improvement].
E-mails Draft an e-mail to the [company] on [subject].
Cover letter Write a conversational cover letter for [Name] for a job application as a [position] at [company].
Out-of-office
messageWrite a short out-of-office message. Reason: [vacation]. Dates: [month and dates]. Person to contact in an emergency or for
immediate help: [name] at [email address].
Victim
Attacker
Interested TopicsRepresentative Queries 
(e.g. Table 1)
Healthcare, Travel 
Planning‚Ä¶3. Collect
Serving System
User PromptsHIT: TTFT Sharp Decrease
Cache Req
Ans‚ë†. Victim sendAttack 
Prompts
‚ë¢. Append 
private  attributes
‚ë§. Return  TTFT ‚ë£. Attacker send
‚ë°. Cached 
response 
Fig. 7. Peeping Neighbor Attacks.
Figure 7 illustrates the PNA steps. When semantic caching
is used, the LLM stores victim requests and serves cached
responses for similar queries. To exploit this channel, the
attacker creates requests containing private attributes and mon-
itors TTFT. A noticeable reduction in TTFT indicates that a
cached victim‚Äôs request has been matched semantically with
the attacker‚Äôs probe, thereby revealing private user data with
high accuracy.
In practice, users may express the same intent through
varied phrasing and may embed diverse private attributes‚Äî
such as personal names or medical conditions‚Äîwithin their
queries. Our objective is to determine how these different
attributes affect semantic similarity and whether the resulting
variations exceed a threshold described in Section III-C. If
so, such differences can be detected, enabling the attacker
to determine whether the victim‚Äôs request carries particular
attributes, even when the victim rephrases the content.
In our experimental study, we chose LangChain as the
LLM serving framework, given its widespread adoption and
extensive application ecosystem [63]. The local chatbot system
was built using LangChain and integrated with GPTCache [64]
for semantic caching. We used gpt-3.5-turbo API as the
backend LLM, and MedQuAD [65] as the evaluation dataset.Characterizing the leakage. Figure 8 outlines our evalu-
ation steps. We illustrate the process with a query template
‚ÄúCompose a meeting agenda ... for [Name] with [medical
condition]‚Äù, denoted as T0. Here, both the name and medical
condition are treated as private attributes.
Step 1. We randomly sampled 10 names from the Python
package names-dataset [66] ( Names ). To obtain 10
random medical conditions ( Medconds ), we selected 10 se-
mantically unrelated Q&A pairs from the MedQuAD dataset
and used GPT-3.5-turbo to extract medical conditions from
the questions. We then randomly chose 1 name ( Name 0) and
1 medical condition ( Medcond 0) as the private attributes to
be recovered; the remaining pairs serve as the negative group
(described below).
Step 2. Since real-world users may phrase the same content
differently, we generated multiple sentences that share the
same semantics as T0. Specifically, we asked GPT-3.5-turbo
to paraphrase T0intonvariations {T1, . . . , T n}, each filled
withName 0andMedcond 0.
Following these steps, we created two sample sets: (1)
Positive Group : Sentences that are semantically similar
toT0, all containing Name 0andMedcond 0. Formally,
{(Ti, Name 0, Medcond 0)|i= 1, . . . , n }. (2) Negative
Group : The original template T0populated with other names
or medical conditions. Formally, {(T0, Name i,
Medcond j)|iÃ∏= 0 orjÃ∏= 0}.
Step 3. We split the positive group into two subsets. The first
(20% of the samples) is designated as the ‚Äúpositive‚Äù reference
set, while the remaining 80% forms the evaluation set. We also
ensure that the evaluation set‚Äôs positive and negative samples
are equal in size. Finally, we compute semantic similarities
between the evaluation samples and both the positive and neg-10
MedQuAD
‚ë†. Fill in
Name
Compose a meeting agenda ‚Ä¶
the treatment plan for [Name] 
with [Medical condition]Template  ùëªùüé
ùë¥ùíÜùíÖùíÑùíêùíè ùíÖùíã ùëµùíÇùíéùíÜ ùíä{(ùëªùüé,ùëµùíÇùíé ùíÜùíä,ùë¥ùíÜùíÖùíÑùíêùíè ùíÖùíã)|ùíä‚â†ùüé ùíêùíì ùíã‚â†ùüé}
(ùëªùüé,ùëµùíÇùíé ùíÜùüé,ùë¥ùíÜùíÖùíÑùíêùíè ùíÖùüé)
Standard PromptNegative Samples : Other Prompts
Positive Group : Similar Templates‚ë°. Generate and filter
{(ùëªùíå,ùëµùíÇùíé ùíÜùüé,ùë¥ùíÜùíÖùíÑùíêùíè ùíÖùüé)|ùíå‚â†ùüé}
Semantic Similarity
Samples:
Evaluation Group‚ë¢. SplitPositive SamplesNegative Samples
‚ë£. Evaluate
Fig. 8. Evaluating semantic leakage of private attributes.
0.0 0.2 0.4 0.6 0.8 1.0
False Positive Rate0.00.20.40.60.81.0True Positive RateSemantic ROC Curves for Different Thresholds
Threshold 0.60
Threshold 0.70
Threshold 0.80
Threshold 0.90
(a). The ‚Äúname‚Äù and ‚Äúmedical condi-
tion‚Äù in the negative group are both
different from the positive group.
0.0 0.2 0.4 0.6 0.8 1.0
False Positive Rate0.00.20.40.60.81.0True Positive RateSemantic ROC Curves for Different Thresholds
Threshold 0.60
Threshold 0.70
Threshold 0.80
Threshold 0.90(b). Either the ‚Äúname‚Äù or ‚Äúmedical
condition‚Äù in the negative group is
different from the positive group.
Fig. 9. Leakage profile of semantic cache sharing. We plotted the ROC curve
to fingerprint the relationship between the similarity vectors of the positive
and negative groups.
ative groups, yielding two respective similarity distributions.
We tested similarity thresholds from 0.6 to 0.9 (the default
value in GPTCache is 0.8). Figure 9a shows the receiver
operating characteristic (ROC) curves for the positive and
negative similarity distributions, revealing a clear separation
between the two. At the default threshold of 0.8, a TPR of 0.95
can be achieved with an FPR below 0.1. We also examined
cases where only one private attribute (either Name 0or
Medcond 0) matched. Here, the negative group consists of
sentences with only one correct private attribute, while the
positive group remains the same. Figure 9b shows the semantic
distinctions remain substantial: at the default threshold of 0.8,
a TPR of 0.85 corresponds to an FPR under 0.1.
End-to-end attacks. We consider a typical open-world
scenario in which a victim user requests healthcare assistance
from an LLM. For instance, the user might submit a query
with semantics similar to the template ‚Äúcompose a meeting
agenda...‚Äù shown in Table III, but with various names and
medical conditions. The user may also send queries unrelated
to the targeted request. To simulate this, we model the user‚Äôs
queries as follows:
‚Ä¢Type-1 (true samples) : Queries with the specific name
(e.g., ‚ÄúAlice‚Äù) and the specific medical condition (e.g., ‚Äúheart
disease‚Äù).
‚Ä¢Type-2 (false samples) : Queries that use the same name as
the true samples (e.g., ‚ÄúAlice‚Äù) but feature different medical
conditions, such as ‚Äúdiabetes‚Äù, ‚Äúhypertension‚Äù, or ‚Äúasthma‚Äù.
‚Ä¢Type-3 (false samples) : Queries with the same medical
condition as the true samples (e.g., ‚Äúheart disease‚Äù) but with
different names.
‚Ä¢Type-4 (false samples) : Queries unrelated to the target
scenario.
We assume that the attacker focuses on uncovering the pri-
vate attribute associations found in Type-1 queries. The victim
can freely choose different paraphrases while preserving the
‚Ä¢
‚Ä¢‚Ä¢‚Ä¢
‚Ä¢‚Ä¢
‚Ä¢‚Ä¢‚Ä¢
‚Ä¢
‚Ä¢‚Ä¢
‚Ä¢‚Ä¢
‚Ä¢‚Ä¢
TargetRequest  Space
‚Ä¢
‚Ä¢‚Ä¢‚Ä¢
‚Ä¢‚Ä¢ Attack Candidates
Semantic Neighborùõæ
ùõΩ
CoverageùõºFig. 10. The greedy search strategy for PNA.
same underlying semantics. To simulate this, we configure the
victim to send five random requests per round: one Type-
1 query (true sample) and four false samples (one Type-
2, one Type-3, and two Type-4 requests). To measure the
effectiveness of the attack, we use the TPR to assess how
successfully the attacker retrieves private attributes from Type-
1 requests. We also measure separate FPRs to capture how
often the attacker incorrectly categorizes each of the three false
sample types as positive.
To perform effective end-to-end attacks on the semantic
cache, we must eliminate noise introduced by the attacker‚Äôs
own requests, which also remain in the cache. To address
this challenge, we developed a method that fully clears the
semantic cache after each attack round. Specifically, our ex-
periments show that under GPTCache‚Äôs default configurations,
sending 1,000 semantically unrelated requests is sufficient to
remove any leftover cache entries. In this scenario, we assume
an attacker aims to determine whether a particular user (e.g.,
‚ÄúAlice‚Äù) is associated with a specific medical condition (e.g.,
‚Äúheart disease‚Äù). Since the victim may use different phrases,
the attacker issues multiple requests to enhance coverage
and boost the TPR. However, increasing the total number
of requests also raises the risk of false positives (i.e., a
higher FPR), especially if new requests strongly resemble
earlier ones. To mitigate this issue, the attacker prioritizes
representative requests in the request space, thus increasing
the overall number of queries while minimizing interference
among them.
Representative requests are those that most closely approx-
imate the rest of the request space. To identify them, we use
thedistilbert-base-uncased [67] model by default
to generate embeddings and then compute the L2 distance
between these embeddings. We then sort the requests by their
L2 distances; those with the smallest distances are deemed the
most representative and selected as attack requests to max-
imize coverage. To further expand coverage, we incorporate
orthogonal requests ‚Äîrequests that are semantically distinct11
TABLE IV
ATTACK ACCURACY FOR THE 4TYPES OF VICTIM REQUESTS WITH
DIFFERENT NUMBER OF ATTACK TRAILS .
#TrialsType 1
(TPR)Type 2
(FPR)Type 3
(FPR)Type 3
(FPR)
1 0.814 0.116 0.054 0.004
2 0.884 0.142 0.056 0.005
3 0.930 0.146 0.060 0.005
4 0.946 0.150 0.062 0.005
5 0.954 0.152 0.062 0.005
1
2
3
4
5
6
7
Response Time (s)0246810FrequencyHit
Miss 12,000 tokens
Miss 18,000 tokens
Miss 24,000 tokens
Fig. 11. Timing distribution for hits and misses of processed documents with
12,000, 18,000, and 24,000 tokens.
from one another. This reduces the chance that semantic
overlaps among the attacker‚Äôs own requests degrade accuracy
in identifying victim requests. We classify a cache access as a
hit if at least one of the attacker‚Äôs requests triggers a cache hit
in the timing channel. Although this strategy boosts coverage,
it also raises the FPR, necessitating a careful balance.
Figure 10 depicts our greedy search algorithm for locating
themost representative requests within a semantically similar
target space, thereby improving PNA accuracy. Specifically,
during each iteration, we pick the most representative candi-
date (e.g., Œ±) and add it to the attacker‚Äôs requests unless it is
overly similar to existing ones (e.g., Œ≤). This process continues
until no additional candidates remain or until the FPR exceeds
a predefined threshold œÉ.
In the evaluation, each of the 4 victim request types was
tested 500 times. In each iteration, a random pair of private
attributes in the Type-1 request was selected. We set œÉ= 0.06
and identified 5 orthogonal attack requests using the proposed
greedy search strategy. Table IV summarizes the TPR for true
samples, and the separate FPRs for each false sample type
when increasing the number of attack requests from 1 to 5.
Specifically, we successfully recovered 407 victim requests out
of the 500 true samples with a single attack request, achieving
a recovery accuracy of 81.4% with an average FPR of 0.045.
With 5 attack requests, 477 victim requests are recovered,
demonstrating a recovery accuracy of 95.4% with an average
FPR of 0.056. We provide a demo for this attack in our
website [62].
C. Inferring Documents on Commodity LLM
The KV cache can be shared among the same user, within
an organization, or even across organizations [31], creating
the potential for cross-user information leaks. In our research,
we discovered that such leaks are feasible even in remoteattack scenarios, particularly when the target LLM processes
documents. To demonstrate this, we utilized a document
summarization application powered by a commodity LLM API
service, where all user requests are processed using the same
developer‚Äôs API key, with both the victim and the attacker
are standard users of the application. We provide an end-
to-end demonstration showing how an adversary could infer
processed documents from the application through the KV
cache side channels. Importantly, such cross-user attacks fun-
damentally do not require application-specific vulnerabilities,
as the KV cache can be shared across organizations [31].
However, our use of the application highlights the privacy risks
inherent to LLM-powered applications, even when they fully
comply with the guidelines provided by the LLM service.
Note that the observation of the document uploaded to
an LLM service, even when the content of the document is
known, can expose an organization‚Äôs interest, with substantial
privacy and competitive ramifications across various domains.
For example, a law firm relying on an LLM-based document
processor could unknowingly disclose its involvement in com-
plex litigation or pivotal mergers, tipping off opposing parties
about strategic decisions before they become public. Similarly,
an investment firm analyzing financial statements might inad-
vertently signal which companies it views as high-potential
opportunities, allowing competitors to anticipate emerging
deals or investment moves.
The victim application. We implemented the document
summarization application with direct summarization [68]
(also known as the stuff approach [69]), using the public
Deepseek-chat model as the backend LLM API server.
The application was built in accordance with Deepseek‚Äôs
guideline and operates by first extracting text from user-
uploaded PDF files using the pdfplumber package. This
text is then formatted into a request and sent to the LLM
for summarization. In this setup, documents uploaded by
users are included in messages under the ‚Äúuser‚Äù role and
sent to the Deepseek model, which returns summaries of the
documents. Notably, all user inputs are processed under a
single Deepseek account, which is typical in such applications.
However, this poses a privacy concern because Deepseek‚Äôs
prompt caching [70] can inadvertently allow cached content
to be reused across different users. As a result, a malicious
user could potentially infer which documents other users have
processed by exploiting timing side channels.
Characterizing the leakage. We conducted experiments
with 200 documents of various lengths (approximately 12,000,
18,000, and 24,000 tokens), each saved in PDF format and
derived from segments of the zero_scrolls dataset [71].
Our results revealed distinct latencies between cache hits
(where documents had been cached) and cache misses (where
documents had not been cached), as shown in Figure 11. In
particular, responses to cache hits remained consistently fast
across different document lengths, while cache misses grew
noticeably slower with larger documents.
End-to-end attacks. In this evaluation, we assume an attacker
aims to determine whether a specific document is uploaded
by the victim for summarization. The attacker prepares a12
set of 200 documents of varying lengths (the ‚Äúinterested‚Äù
documents). Meanwhile, the victim submits a total of 200
documents, half of which come from the interested set and
half from outside it. This sequence is repeated 5 times, and
each time the attacker attempts to distinguish which of the
victim‚Äôs documents belong to the interested set.
Specifically, the victim first submits 100 documents from
the interested set. The attacker then probes each of the 200 in-
terested documents once, recording response latencies. Based
on a predefined threshold (2.0 seconds in our experiments),
the TPR is computed as the fraction of probed documents
correctly identified as cache hits (i.e., with latencies below the
threshold). Next, the victim uploads 100 additional documents
outside the interested set, and the attacker probes the entire
set of 200 documents again. The FPR is then calculated as
the fraction of documents incorrectly labeled as hits. Our
evaluation shows that this attack achieves an average accuracy
of 89%, with an average FPR of 0.05. A demo for the attack
is presented in our website [62].
Notes. For ethical reasons, we did not explore techniques
for forcibly evicting cache entries in real-world systems. Such
research would require extensive experimentation, potentially
violating usage policies or interfering with other users‚Äô expe-
rience. Without active cache eviction, the timing-based attack
primarily operates at the granularity where caches naturally
expire due to inactivity‚Äîaround 5 minutes for systems like
OpenAI and Anthropic, as indicated in their documenta-
tion [24], [70].
D. Measurement Study on Commodity LLMs
KV cache sharing. To investigate KV cache sharing in com-
modity LLM services, we conducted experiments by invoking
the APIs provided by these vendors. These APIs support
different roles, such as system and user. For the measurement
study, we designed requests with system and user prompts of
varying lengths and configured them to run in the streaming
mode. For this evaluation, we used the zero_scrolls
dataset for generating requests.
Specifically, we first measured the response latencies by
sending initial requests that were likely to miss the cache.
Then, we sent identical requests multiple times and mea-
sured the average latencies for these subsequent requests. To
maximize the likelihood of co-locating on the same physical
machine and ensuring the requests were cached, we conducted
continuous tests within the same time period. If we observed
lower latencies in the later requests, this indicated the use of
caching mechanisms in the LLM services. With KV cache
sharing, the computation of matched prefix tokens during the
prefill phase can be ignored. However, the output generated
during the decoding phase still requires computation and
inference, which are influenced by parameters such as tem-
perature, introducing randomness. To verify that the latency
reduction was due to KV cache sharing, we deliberately set a
high temperature ( 0.9) in the request. This configuration was
critical because semantic caching mechanisms typically return
identical cached outputs for semantically similar inputs. By
introducing substantial randomness in token selection throughTABLE V
SUMMARY OF KV CACHE SHARING IN REAL WORLD LLM SERVING
SYSTEMS (DATE : 08/29/2024).
LLM service System prompt sharing User prompt sharing
GPT-4o-mini‚Ä†! !
Deepinfra ! !
Deepseek-chat ! !
Claude-3.5 ! !
Qwen-max % %
Moonshot ! !
Baidu Ernie-8k % %
Google Gemini % %
Fireworks.ai ! !
Groq [72] % %
SiliconFLow ! !
‚Ä†We observed a timing difference on 08/29/2024 and reported it to
OpenAI. By late December 2024, the timing difference was no longer
stable, despite the API indicating that the prompt cache was effective.
TABLE VI
NATIVE SUPPORT OF SEMANTIC CACHING OF POPULAR AISERVICE
PROVIDERS (DATE : 08/29/2024).
Service providers Semantic cache support
Azure OpenAI Service models [73] !
Amazon Bedrock [74] !
Google Vertex AI [75] %
Alibaba Elastic Algorithm Service
(EAS) of Platform for AI (PAI) [76]!
high temperature, we ensured the LLM would generate diverse
responses despite input similarities. We verified whether the
LLM produced different responses for each request, with
TTFT reductions consistently observed. If it did, this strongly
indicated that KV cache sharing was supported, enabling a
reduction in TTFT while still allowing for diverse outputs. To
minimize the impact of network latency, we sent requests of
varying lengths, ranging from 200 to 2,000 tokens. The time
difference between cached and uncached responses typically
spanned several hundred milliseconds, making it easy to
distinguish between the two. Additionally, we observed when
the cache is hit, the TTFT remains consistent, regardless of
the request length, whereas when the cache is missed, TTFT
increases almost linearly as the length of the request grows. As
summarized in Table V, most popular LLM service providers
support KV cache sharing in specific scenarios.
Semantic cache sharing. We manually reviewed the doc-
umentation of public cloud AI service providers to verify
whether they support semantic cache APIs. As shown in
Table VI, semantic caching is supported by major AI platform-
as-a-service providers. Notably, even on platforms that do not
offer native semantic caching, users can still implement their
own solutions or leverage open-source alternatives, such as
GPTCache.
V. M ITIGATIONS
A. Mitigating KV Cache Leakages
Design. A straightforward approach to mitigate KV cache
leakages is to eliminate any sharing across requests. How-
ever, this would negate the computational and cost savings13
TABLE VII
TOKEN RECOVERY RESULTS UNDER DIFFERENT NUMBERS OF MINIMUM
SHARED TOKENS .
KRecovery
rateAccuracy#queries per
recovered token#queries
per token
1 91.5% 97.9% 118.58 215.41
2 81.0% 98.8% 108.89 286.79
3 67.5% 98.5% 88.30 337.28
4 49.0% 98.0% 62.55 470.82
associated with caching. Noting that PSA recovers the request
token by token by observing per-token timing differences,
we explore the effect of a simple mitigation strategy that
the prefix cache can only be shared in units of at least K
tokens ( K= 2,3,4etc.). In SGLang, this could be achieved
by modifying the radix tree structure used to manage the KV
cache. To prevent the leakages of cache-aware task scheduling,
the scheduling policy in SGLang needs to be modified to
prioritize requests that have at least Kshared prefix tokens.
Requests with fewer than Kshared prefix tokens would still be
placed in the waiting list. This approach reduces the likelihood
of KV cache sharing, but it is unlikely to significantly impact
performance.
Evaluation. To evaluate the effectiveness of the mitigation
and reduce the cost of querying the LLM, we conducted
a simulation-based experiment. First, we built the classifiers
that detect the hits and misses of Ktokens for each value
ofK(K= 1,2,3,4), following the method outlined in
Section IV-A. Since the timing differences become more
pronounced as Kincreases, we reduced the number of samples
(i.e., n) in multi-sampling, and obtained the corresponding
TPRs and FPRs for each classifier. Then we used an oracle
to simulate the classifiers by randomly sampling a number
and determining whether it fell within the classification range.
In this evaluation, we used the same repetitive trails method,
dataset and fine-tuned model as described in Section IV-A.
The next-token predictor was modified to predict the next
Ktokens. Table VII presents the token recovery rate and
the average number of queries needed to recover 1 token for
K= 1,2,3and4. The results show that as Kincreases, the
attack still achieves a notable recovery rate. For successfully
recovered tokens, the average number of queries decreases
with larger Kvalues, as the predictor quickly identifies tokens
in easier-to-predict prompts. However, the overall recovery
rate declines because the predictor increasingly struggles with
harder-to-predict tokens at higher K values. When accounting
for tokens not recovered within the maximum 80 allowed
guesses, the average query cost per token recovery increases
significantly.
B. Mitigating Semantic Cache Leakages
Design. As investigated in Section IV-B, private attributes
have a significant impact on the semantic similarity between
requests. As a result, the PNA infers private attributes by
probing whether a semantically similar request is cached. To
mitigate this leakage, we propose a strategy that involves
identifying and anonymizing the private attributes present in
the requests. This approach not only prevents the leakage of
LLM 
AdapterPre-
Processor
Post -
ProcessorEmbedding
Generator
Similarity
EvaluatorCache Store
Anonymizer
Mapping
Vector Store
LLMCache Manager
Update
DeanonymizeAnonymize
Cache Miss
Inference
Cache hitFig. 12. Mitigating semantic cache leakages. The shaded components are
customized as part of our mitigation.
private attributes but also increases the potential for sharing
requests across users.
As shown in Figure 12, we integrate a custom pre-processor
and post-processor into the GPTCache framework. The pre-
processor is designed to identify private attributes within the
requests, and replace them with anonymized identifiers. In this
approach, we selectively de-identify Personally Identifiable
Information (PII) attributes, such as names, email addresses,
phone numbers, credit card numbers, and IP addresses, while
ensuring that no essential information needed for the LLMs is
removed.
To facilitate reuse, the cache manager maintains a mapping
structure that stores the anonymized identifier alongside its
corresponding private attribute in a key-value format. The
post-processor then identifies the anonymized identifiers in
the response and replaces them with the private attributes by
referencing this mapping. This ensures that the user receives
an accurate response.
Evaluation. In our prototype implementation, we used the
Presidio tool by Microsoft [77] to automatically identify
private attributes. For performance evaluation, we used the
evaluation dataset released by Presidio, which includes sen-
tences containing private information. Specifically, we ran-
domly sampled 1,000 sentences from the dataset and fed
them into both the original GPTCache and the enhanced
GPTCache. Then we measured the average delay introduced
by the pre-processor and post-processor. The results show
that the anonymization process adds an average delay of
approximately 6 ms, while GPTCache‚Äôs response latency for
a semantic cache hit without anonymization is around 0.14
s. Thus, de-identification introduces only about 4% additional
overhead, which has a minimal impact on GPTCache‚Äôs overall
performance.
VI. D ISCUSSIONS
Co-locations. Co-location is a prerequisite for timing side-
channel attacks. There has been significant research on how
to achieve co-location in the cloud for micro-architectural
side channels [78]‚Äì[81]. Co-location can be more efficient in
LLM systems because these systems often focus on optimizing
cache reuse through improved scheduling policies. For exam-
ple, baseline scheduling policies, such as first-come-first-serve,
typically do not consider the shared prompts within an LLM14
system. As a result, requests may be mixed across different
LLM engines, preventing the reuse of common prompt prefixes
and potentially leading to suboptimal cache efficiency. To
address this, LLM serving systems like SGLang [26], Par-
rot [82], Mooncake [83] and BatchLLM [84] have introduced
modified schedulers that prioritize requests that align with
cached prefixes, a strategy known as cache-aware scheduling.
Unexplored timing leakages. This paper utilizes SGLang [26]
and GPTCache [28] as the most representative KV cache and
semantic cache sharing mechanisms. However, it is impor-
tant to note that more sophisticated optimization techniques
may enhance performance, but they could also amplify the
significance of timing leakage. For example, modular prefix
caching [85] and CacheBlend [86] facilitate KV caching for
not only prefix tokens but also intermediate ones. Cascade in-
ference [87] stores the shared KV cache in GPU shared mem-
ory (SMEM for short), for fast access in multiple requests. The
sharing of KV cache in speculative decoding [88] may also
introduce speculative side channels, akin to Spectre [89]. We
leave the further exploration of the impact of the discovered
side channels to future work.
Real-world attacks. In real world attacks, especially targeting
online LLM services, the network latency will bring additional
noises to the timing. The noise effect could be reduced with
more attack trials. Besides, it has been shown possible to
mount cache attacks from the remote network [41], [42].
VII. R ELATED WORKS
Prompt extraction attacks with adversarial prompts. Most
existing research focuses on stealing system prompts from
LLMs by eliciting the previous prompts, typically through
direct output or translation. For example, a twitter user claimed
to have discovered the prompt used by Bing Chat [90]. Earlier
studies involved manually constructing attacker prompts [91],
[92], while more recent work, such as PLeak, leverages output
feedback from a given prompt and introduces an incremental
search algorithm to optimize prompt retrieval [93]. Zhang
et al. present a framework for systematically evaluating the
effectiveness of these attacks. While these approaches exploit
the model‚Äôs vulnerability to adversarial prompts, our proposed
attacks take advantage of timing differences introduced in
LLM service deployment. As such, our attacks do not rely
on the specific details of any particular LLM.
Side channel attacks on LLM. Debenedetti et al. propose
system-level side channels within the deep learning lifecycle,
such as training data filtering, input preprocessing, output
monitoring, and query filtering. These side channels can
potentially be exploited to infer the training data or the
requests [94]. LLM keystroking attacks [95] are a type of
packet analysis based side channels. These attacks exploit the
length of response tokens, assuming the attacker has access to
encrypted network packets. Besides, Carlini et al. investigated
the privacy risks of timing attacks in multi-turn interactions
with LLM chatbots [96]. By comparison, we are the first to
study the timing leaks introduced by LLM serving system
optimizations, rather than relying on output data or token
packet sizes to recover requests.Micro-architectural side channel attacks on deep learn-
ing systems. Numerous studies have explored methods for
extracting deep learning models or structures, as well as fin-
gerprinting these models, by exploiting various side channels,
such as CPU [97]‚Äì[101], GPU [102], FPGA [103], power
and magnetic channels [104], [105], and PCIe traffic [106].
In comparison, our work focuses on leaking private prompts
rather than stealing model parameters. Additionally, our ap-
proach does not rely on the micro-architectural or power
characteristics of specific hardware; instead, it exploits timing
leaks inherent in LLM systems. As a result, our attacks are
applicable across CPU, GPU, and FPGA platforms, provided
they utilize KV cache or semantic cache sharing techniques.
VIII. C ONCLUSIONS
LLM inference is a resource-intensive process, prompting
numerous studies focused on reducing inference costs and
latency. These optimizations often involve the use of various
caches. When multiple users share the LLM system, these
optimizations can lead to interference between users. This
paper examines the side channels created by such interference,
identifying two types of leaks: one in the KV cache and
another in the semantic cache. We urge LLM system providers
to recognize this emerging threat and prioritize security in their
design choices.
REFERENCES
[1] ‚ÄúChatgpt ‚Äî openai,‚Äù https://openai.com/chatgpt/, 2024.
[2] ‚ÄúGemini,‚Äù https://deepmind.google/technologies/gemini/, 2024.
[3] ‚ÄúPerplexity ai,‚Äù https://www.perplexity.ai/, 2024.
[4] ‚ÄúGithub copilot ¬∑ your ai pair programmer,‚Äù https://github.com/features/
copilot/, 2024.
[5] Z. Yao, R. Yazdani Aminabadi, M. Zhang, X. Wu, C. Li, and Y . He,
‚ÄúZeroquant: Efficient and affordable post-training quantization for
large-scale transformers,‚Äù Advances in Neural Information Processing
Systems , vol. 35, pp. 27 168‚Äì27 183, 2022.
[6] X. Wei, Y . Zhang, X. Zhang, R. Gong, S. Zhang, Q. Zhang, F. Yu, and
X. Liu, ‚ÄúOutlier suppression: Pushing the limit of low-bit transformer
language models,‚Äù Advances in Neural Information Processing Systems ,
vol. 35, pp. 17 402‚Äì17 414, 2022.
[7] G. Xiao, J. Lin, M. Seznec, H. Wu, J. Demouth, and S. Han,
‚ÄúSmoothquant: Accurate and efficient post-training quantization for
large language models,‚Äù in International Conference on Machine
Learning . PMLR, 2023, pp. 38 087‚Äì38 099.
[8] E. Frantar, S. Ashkboos, T. Hoefler, and D. Alistarh, ‚ÄúGptq: Accu-
rate post-training quantization for generative pre-trained transformers,‚Äù
arXiv preprint arXiv:2210.17323 , 2022.
[9] Z. Liu, B. Oguz, C. Zhao, E. Chang, P. Stock, Y . Mehdad, Y . Shi, R. Kr-
ishnamoorthi, and V . Chandra, ‚ÄúLlm-qat: Data-free quantization aware
training for large language models,‚Äù arXiv preprint arXiv:2305.17888 ,
2023.
[10] W. Wang, W. Chen, Y . Luo, Y . Long, Z. Lin, L. Zhang, B. Lin,
D. Cai, and X. He, ‚ÄúModel compression and efficient inference for
large language models: A survey,‚Äù arXiv preprint arXiv:2402.09748 ,
2024.
[11] S. Park, J. Choi, S. Lee, and U. Kang, ‚ÄúA comprehensive sur-
vey of compression algorithms for language models,‚Äù arXiv preprint
arXiv:2401.15347 , 2024.
[12] X. Zhu, J. Li, Y . Liu, C. Ma, and W. Wang, ‚ÄúA survey on model com-
pression for large language models,‚Äù arXiv preprint arXiv:2308.07633 ,
2023.
[13] T. Dao, D. Fu, S. Ermon, A. Rudra, and C. R ¬¥e, ‚ÄúFlashattention: Fast
and memory-efficient exact attention with io-awareness,‚Äù Advances in
Neural Information Processing Systems , vol. 35, pp. 16 344‚Äì16 359,
2022.15
[14] W. Kwon, Z. Li, S. Zhuang, Y . Sheng, L. Zheng, C. H. Yu, J. Gonzalez,
H. Zhang, and I. Stoica, ‚ÄúEfficient memory management for large
language model serving with pagedattention,‚Äù in Proceedings of the
29th Symposium on Operating Systems Principles (SOSP) , 2023, pp.
611‚Äì626.
[15] G. Xiao, Y . Tian, B. Chen, S. Han, and M. Lewis, ‚ÄúEfficient
streaming language models with attention sinks,‚Äù arXiv preprint
arXiv:2309.17453 , 2023.
[16] Y . Zhao, D. Wu, and J. Wang, ‚ÄúAlisa: Accelerating large lan-
guage model inference via sparsity-aware kv caching,‚Äù arXiv preprint
arXiv:2403.17312 , 2024.
[17] Y . Song, Z. Mi, H. Xie, and H. Chen, ‚ÄúPowerinfer: Fast large
language model serving with a consumer-grade gpu,‚Äù arXiv preprint
arXiv:2312.12456 , 2023.
[18] G.-I. Yu, J. S. Jeong, G.-W. Kim, S. Kim, and B.-G. Chun, ‚ÄúOrca:
A distributed serving system for Transformer-Based generative mod-
els,‚Äù in 16th USENIX Symposium on Operating Systems Design and
Implementation (OSDI 22) , 2022, pp. 521‚Äì538.
[19] X. Miao, G. Oliaro, Z. Zhang, X. Cheng, H. Jin, T. Chen, and Z. Jia,
‚ÄúTowards efficient generative large language model serving: A survey
from algorithms to systems,‚Äù arXiv preprint arXiv:2312.15234 , 2023.
[20] L. Zheng, L. Yin, Z. Xie, J. Huang, C. Sun, C. H. Yu, S. Cao,
C. Kozyrakis, I. Stoica, J. E. Gonzalez et al. , ‚ÄúEfficiently programming
large language models using sglang,‚Äù arXiv preprint arXiv:2312.07104 ,
2023.
[21] L. Reynolds and K. McDonell, ‚ÄúPrompt programming for large lan-
guage models: Beyond the few-shot paradigm,‚Äù in Extended abstracts
of the 2021 CHI conference on human factors in computing systems ,
2021, pp. 1‚Äì7.
[22] L. Giray, ‚ÄúPrompt engineering with chatgpt: a guide for academic
writers,‚Äù Annals of biomedical engineering , vol. 51, no. 12, pp. 2629‚Äì
2633, 2023.
[23] ‚ÄúPrompt template,‚Äù https://python.langchain.com.cn/docs/modules/
model io/prompts/prompt templates/, 2024.
[24] ‚ÄúPrompt caching with claude,‚Äù https://www.anthropic.com/news/
prompt-caching, 2024.
[25] F. Bang, ‚ÄúGptcache: An open-source semantic cache for llm applica-
tions enabling faster answers and cost savings,‚Äù in Proceedings of the
3rd Workshop for Natural Language Processing Open Source Software
(NLP-OSS 2023) , 2023, pp. 212‚Äì218.
[26] ‚ÄúSglang is yet another fast serving framework for large language mod-
els and vision language models,‚Äù https://github.com/sgl-project/sglang,
2024.
[27] ‚ÄúLangchain,‚Äù https://www.langchain.com/, 2024.
[28] ‚ÄúGptcache : A library for creating semantic cache for llm queries,‚Äù
https://github.com/zilliztech/gptcache, 2024.
[29] G. Wu, Z. Zhang, Y . Zhang, W. Wang, J. Niu, Y . Wu, and Y . Zhang, ‚ÄúI
know what you asked: Prompt leakage via kv-cache sharing in multi-
tenant llm serving,‚Äù in 32nd Annual Network and Distributed System
Security Symposium, NDSS 2025, San Diego, California, USA , 2025.
[30] X. Zheng, H. Han, S. Shi, Q. Fang, Z. Du, Q. Guo, and X. Hu,
‚ÄúInputsnatch: Stealing input in llm services via timing side-channel
attacks,‚Äù arXiv preprint arXiv:2411.18191 , 2024.
[31] C. Gu, X. L. Li, R. Kuditipudi, P. Liang, and T. Hashimoto, ‚ÄúStanford
cs 191w senior project: Timing attacks on prompt caching in language
model apis,‚Äù 2024.
[32] K. Mei, Z. Li, S. Xu, R. Ye, Y . Ge, and Y . Zhang, ‚ÄúAios: Llm agent
operating system,‚Äù arXiv e-prints , pp. arXiv‚Äì2403, 2024.
[33] ‚ÄúLumos,‚Äù https://github.com/andrewnguonly/Lumos, 2024.
[34] ‚ÄúBingchat,‚Äù https://www.bing.com/chat, 2024.
[35] A. Vaswani, ‚ÄúAttention is all you need,‚Äù Advances in Neural Informa-
tion Processing Systems , 2017.
[36] B. Gao, Z. He, P. Sharma, Q. Kang, D. Jevdjic, J. Deng, X. Yang, Z. Yu,
and P. Zuo, ‚ÄúAttentionstore: Cost-effective attention reuse across multi-
turn conversations in large language model serving,‚Äù arXiv preprint
arXiv:2403.19708 , 2024.
[37] ‚ÄúTensorrt-llm,‚Äù https://github.com/NVIDIA/TensorRT-LLM, 2024.
[38] ‚ÄúLarge language model text generation inference,‚Äù https://github.com/
huggingface/text-generation-inference, 2024.
[39] ‚ÄúAnythingllm: The all-in-one desktop & docker ai application with
built-in rag, ai agents, and more.‚Äù https://github.com/Mintplex-Labs/
anything-llm, 2024.
[40] ‚ÄúGpt-4 document analysis ‚Äì klu,‚Äù https://klu.ai/use-cases/
document-analysis, 2024.
[41] M. Kurth, B. Gras, D. Andriesse, C. Giuffrida, H. Bos, and K. Razavi,
‚ÄúNetcat: Practical cache attacks from the network,‚Äù in 2020 IEEE
Symposium on Security and Privacy (SP) . IEEE, 2020, pp. 20‚Äì38.[42] M. Schwarz, M. Schwarzl, M. Lipp, J. Masters, and D. Gruss, ‚ÄúNet-
spectre: Read arbitrary memory over network,‚Äù in Computer Security‚Äì
ESORICS 2019: 24th European Symposium on Research in Computer
Security, Luxembourg, September 23‚Äì27, 2019, Proceedings, Part I 24 .
Springer, 2019, pp. 279‚Äì299.
[43] H. Naghibijouybari, A. Neupane, Z. Qian, and N. Abu-Ghazaleh, ‚ÄúRen-
dered insecure: Gpu side channel attacks are practical,‚Äù in Proceedings
of the 2018 ACM SIGSAC conference on computer and communications
security , 2018, pp. 2139‚Äì2153.
[44] Z. Zhou, W. Diao, X. Liu, Z. Li, K. Zhang, and R. Liu, ‚ÄúVulnerable gpu
memory management: towards recovering raw data from gpu,‚Äù arXiv
preprint arXiv:1605.06610 , 2016.
[45] H. Taneja, J. Kim, J. J. Xu, S. Van Schaik, D. Genkin, and Y . Yarom,
‚ÄúHot pixels: Frequency, power, and temperature attacks on {GPUs}and
arm{SoCs},‚Äù in 32nd USENIX Security Symposium (USENIX Security
23), 2023, pp. 6275‚Äì6292.
[46] ‚ÄúSamsung bans staff‚Äôs ai use after spotting chatgpt data
leak,‚Äù https://www.bloomberg.com/news/articles/2023-05-02/
samsung-bans-chatgpt-and-other-generative-ai-use-by-staff-after-leak.
[47] ‚ÄúSystem prompts in large language models,‚Äù https://promptengineering.
org/system-prompts-in-large-language-models/, 2024.
[48] ‚Äúvllm: A high-throughput and memory-efficient inference and serving
engine for llms,‚Äù https://github.com/vllm-project/vllm, 2024.
[49] ‚ÄúEstimate llm inference speed and vram usage quickly:
with a llama-7b case study,‚Äù https://www.jinghong-chen.net/
estimate-vram-usage-in-llm-inference/, 2024.
[50] Meta, ‚ÄúLlama-3.1-70b-instruct-gptq-int4,‚Äù https://huggingface.co/
hugging-quants/Meta-Llama-3.1-70B-Instruct-GPTQ-INT4, 2024.
[51] ‚ÄúLlamaindex is a data framework for your llm applications,‚Äù https:
//github.com/run-llama/llama index, 2024.
[52] ‚ÄúJpmorgan rolls out in-house genai-based chatbot to
employees,‚Äù https://www.financedirectoreurope.com/news/
jpmorgan-rolls-out-ai-based-chatbot/, 2024.
[53] ‚ÄúA large language model for healthcare,‚Äù https://aiforhealthcare.
substack.com/p/a-large-language-model-for-healthcare, 2024.
[54] ‚ÄúText generation and prompting,‚Äù https://platform.openai.com/docs/
guides/text?api-mode=responses, 2024.
[55] L. Zheng, W.-L. Chiang, Y . Sheng, S. Zhuang, Z. Wu, Y . Zhuang,
Z. Lin, Z. Li, D. Li, E. P. Xing, H. Zhang, J. E. Gonzalez, and I. Stoica,
‚ÄúJudging llm-as-a-judge with mt-bench and chatbot arena,‚Äù 2023.
[56] S. Wu, O. Irsoy, S. Lu, V . Dabravolski, M. Dredze, S. Gehrmann,
P. Kambadur, D. S. Rosenberg, and G. Mann, ‚ÄúBloomberggpt: A
large language model for finance,‚Äù CoRR , vol. abs/2303.17564, 2023.
[Online]. Available: https://doi.org/10.48550/arXiv.2303.17564
[57] Meta, ‚ÄúLlama-3.1-8b-instruct,‚Äù https://huggingface.co/meta-llama/
Llama-3.1-8B-Instruct, 2024.
[58] ‚ÄúSystem prompt leakage dataset,‚Äù https://huggingface.co/datasets/
gabrielchua/system-prompt-leakage, 2024.
[59] L. McInnes, J. Healy, and J. Melville, ‚ÄúUmap: Uniform manifold
approximation and projection for dimension reduction,‚Äù arXiv preprint
arXiv:1802.03426 , 2018.
[60] Z. Zhang, Y . Zhong, R. Ming, H. Hu, J. Sun, Z. Ge, Y . Zhu, and
X. Jin, ‚ÄúDisttrain: Addressing model and data heterogeneity with
disaggregated training for multimodal large language models,‚Äù 2024.
[Online]. Available: https://arxiv.org/abs/2408.04275
[61] S. Group, ‚Äúflush cache,‚Äù https://github.com/sgl-project/sglang/blob/
25e5d589e39b3b605296395e4f9c96ec42f09055/python/sglang/srt/
server.py#L164, 2024.
[62] ‚Äúllm side-channel demo,‚Äù https://sites.google.com/view/
early-bird-catches-the-leak, 2025.
[63] ‚ÄúLangchain applications,‚Äù https://lablab.ai/apps/tech/langchain/
langchain, 2024.
[64] ‚ÄúGptcache usage,‚Äù https://python.langchain.com/api reference/
community/cache/langchain community.cache.GPTCache.html, 2024.
[65] ‚ÄúMedquad,‚Äù https://huggingface.co/datasets/lavita/MedQuAD, 2019.
[66] P. Remy, ‚ÄúName dataset,‚Äù https://github.com/philipperemy/
name-dataset, 2021.
[67] D. community, ‚ÄúDistilbert,‚Äù https://huggingface.co/distilbert/
distilbert-base-uncased, 2024.
[68] ‚ÄúSummarizing documents with llms: A com-
prehensive guide,‚Äù https://www.linkedin.com/pulse/
summarizing-documents-llms-comprehensive-guide-sharat-kedari-4vdfc,
2024.
[69] ‚ÄúAi document summarization,‚Äù https://www.ibm.com/architectures/
hybrid/genai-document-summarization, 2024.
[70] ‚ÄúPrompt caching in the api,‚Äù https://openai.com/index/
api-prompt-caching/, 2024.16
[71] U. Shaham, M. Ivgi, A. Efrat, J. Berant, and O. Levy, ‚ÄúZeroscrolls:
A zero-shot benchmark for long text understanding,‚Äù arXiv preprint
arXiv:2305.14196 , 2023.
[72] ‚ÄúGroq,‚Äù https://groq.com/, 2024.
[73] ‚ÄúGet cached responses of azure openai api requests,‚Äù
https://learn.microsoft.com/en-us/azure/api-management/
azure-openai-semantic-cache-lookup-policy, 2024.
[74] ‚ÄúAmazon bedrock - build generative ai applications with foundation
models,‚Äù https://aws.amazon.com/bedrock/, 2024.
[75] ‚ÄúVertex ai with gemini 1.5 pro and gemini 1.5 flash,‚Äù https://cloud.
google.com/vertex-ai, 2024.
[76] ‚ÄúAlibaba platform for ai,‚Äù https://www.alibabacloud.com/help/en/pai/,
2024.
[77] ‚ÄúPresidio,‚Äù https://github.com/microsoft/presidio, 2022.
[78] T. Ristenpart, E. Tromer, H. Shacham, and S. Savage, ‚ÄúHey, you, get
off of my cloud: exploring information leakage in third-party compute
clouds,‚Äù in Proceedings of the 16th ACM conference on Computer and
communications security , 2009, pp. 199‚Äì212.
[79] W. Zhenyu, X. Zhang, and H. Wang, ‚ÄúWhispers in the hyper-space:
high-speed covert channel attacks in the cloud,‚Äù in USENIX Security
symposium , 2012, pp. 159‚Äì173.
[80] Y . Zhang, A. Juels, M. K. Reiter, and T. Ristenpart, ‚ÄúCross-tenant
side-channel attacks in paas clouds,‚Äù in Proceedings of the 2014 ACM
SIGSAC Conference on Computer and Communications Security , 2014,
pp. 990‚Äì1003.
[81] V . Varadarajan, Y . Zhang, T. Ristenpart, and M. Swift, ‚ÄúA placement
vulnerability study in Multi-Tenant public clouds,‚Äù in 24th USENIX
Security Symposium (USENIX Security 15) , 2015, pp. 913‚Äì928.
[82] C. Lin, Z. Han, C. Zhang, Y . Yang, F. Yang, C. Chen, and L. Qiu,
‚ÄúParrot: Efficient serving of llm-based applications with semantic
variable,‚Äù arXiv preprint arXiv:2405.19888 , 2024.
[83] R. Qin, Z. Li, W. He, M. Zhang, Y . Wu, W. Zheng, and X. Xu,
‚ÄúMooncake: Kimi‚Äôs kvcache-centric architecture for llm serving,‚Äù arXiv
preprint arXiv:2407.00079 , 2024.
[84] Z. Zheng, X. Ji, T. Fang, F. Zhou, C. Liu, and G. Peng, ‚ÄúBatchllm:
Optimizing large batched llm inference with global prefix sharing and
throughput-oriented token batching,‚Äù arXiv preprint arXiv:2412.03594 ,
2024.
[85] I. Gim, G. Chen, S.-s. Lee, N. Sarda, A. Khandelwal, and L. Zhong,
‚ÄúPrompt cache: Modular attention reuse for low-latency inference,‚Äù
Proceedings of Machine Learning and Systems , vol. 6, pp. 325‚Äì338,
2024.
[86] J. Yao, H. Li, Y . Liu, S. Ray, Y . Cheng, Q. Zhang, K. Du, S. Lu, and
J. Jiang, ‚ÄúCacheblend: Fast large language model serving with cached
knowledge fusion,‚Äù arXiv preprint arXiv:2405.16444 , 2024.
[87] Z. Ye, R. Lai, B.-R. Lu, C.-Y . Lin, S. Zheng, L. Chen, T. Chen,
and L. Ceze, ‚ÄúCascade inference: Memory bandwidth efficient
shared prefix batch decoding,‚Äù February 2024. [Online]. Available:
https://flashinfer.ai/2024/02/02/cascade-inference.html
[88] Y . Leviathan, M. Kalman, and Y . Matias, ‚ÄúFast inference from trans-
formers via speculative decoding,‚Äù in International Conference on
Machine Learning . PMLR, 2023, pp. 19 274‚Äì19 286.
[89] P. Kocher, J. Horn, A. Fogh, D. Genkin, D. Gruss, W. Haas, M. Ham-
burg, M. Lipp, S. Mangard, T. Prescher et al. , ‚ÄúSpectre attacks: Ex-
ploiting speculative execution,‚Äù Communications of the ACM , vol. 63,
no. 7, pp. 93‚Äì101, 2020.[90] ‚ÄúThe entire prompt of microsoft bing chat,‚Äù https://twitter.com/kliu128/
status/1623472922374574080, 2024.
[91] F. Perez and I. Ribeiro, ‚ÄúIgnore previous prompt: Attack techniques
for language models,‚Äù arXiv preprint arXiv:2211.09527 , 2022.
[92] Y . Zhang and D. Ippolito, ‚ÄúPrompts should not be seen as secrets:
Systematically measuring prompt extraction attack success,‚Äù arXiv
preprint arXiv:2307.06865 , 2023.
[93] B. Hui, H. Yuan, N. Gong, P. Burlina, and Y . Cao, ‚ÄúPleak: Prompt
leaking attacks against large language model applications,‚Äù arXiv
preprint arXiv:2405.06823 , 2024.
[94] E. Debenedetti, G. Severi, N. Carlini, C. A. Choquette-Choo, M. Jagiel-
ski, M. Nasr, E. Wallace, and F. Tram `er, ‚ÄúPrivacy side channels in
machine learning systems,‚Äù in 33rd USENIX Security Symposium , 2024.
[95] R. Weiss, D. Ayzenshteyn, G. Amit, and Y . Mirsky, ‚ÄúWhat was your
prompt? a remote keylogging attack on ai assistants,‚Äù arXiv preprint
arXiv:2403.09751 , 2024.
[96] N. Carlini and M. Nasr, ‚ÄúRemote timing attacks on efficient language
model inference,‚Äù arXiv preprint arXiv:2410.17175 , 2024.
[97] V . Duddu, D. Samanta, D. V . Rao, and V . E. Balas, ‚ÄúStealing neural
networks via timing side channels,‚Äù arXiv preprint arXiv:1812.11720 ,
2018.
[98] M. Yan, C. W. Fletcher, and J. Torrellas, ‚ÄúCache telepathy: Leveraging
shared resource attacks to learn {DNN}architectures,‚Äù in 29th USENIX
Security Symposium (USENIX Security 20) , 2020, pp. 2003‚Äì2020.
[99] A. S. Rakin, M. H. I. Chowdhuryy, F. Yao, and D. Fan, ‚ÄúDeepsteal:
Advanced model extractions leveraging efficient weight stealing in
memories,‚Äù in 2022 IEEE symposium on security and privacy (SP) .
IEEE, 2022, pp. 1157‚Äì1174.
[100] C. Gongye, Y . Fei, and T. Wahl, ‚ÄúReverse-engineering deep neural
networks using floating-point timing side-channels,‚Äù in 2020 57th
ACM/IEEE Design Automation Conference (DAC) . IEEE, 2020, pp.
1‚Äì6.
[101] S. Shukla, M. Alam, P. Mitra, and D. Mukhopadhyay, ‚ÄúStealing
the invisible: Unveiling pre-trained cnn models through adversarial
examples and timing side-channels,‚Äù arXiv preprint arXiv:2402.11953 ,
2024.
[102] J. Wei, Y . Zhang, Z. Zhou, Z. Li, and M. A. Al Faruque, ‚ÄúLeaky dnn:
Stealing deep-learning model secret with gpu context-switching side-
channel,‚Äù in 2020 50th Annual IEEE/IFIP International Conference on
Dependable Systems and Networks (DSN) . IEEE, 2020, pp. 125‚Äì137.
[103] Y . Zhang, R. Yasaei, H. Chen, Z. Li, and M. A. Al Faruque, ‚ÄúStealing
neural network structure through remote fpga side-channel analysis,‚Äù
IEEE Transactions on Information Forensics and Security , vol. 16, pp.
4377‚Äì4388, 2021.
[104] H. T. Maia, C. Xiao, D. Li, E. Grinspun, and C. Zheng, ‚ÄúCan one hear
the shape of a neural network?: Snooping the gpu via magnetic side
channel.‚Äù in USENIX Security Symposium , 2022, pp. 4383‚Äì4400.
[105] P. Horvath, L. Chmielewski, L. Weissbart, L. Batina, and Y . Yarom,
‚ÄúBarracuda: Bringing electromagnetic side channel into play to steal
the weights of neural networks from nvidia gpus,‚Äù arXiv preprint
arXiv:2312.07783 , 2023.
[106] Y . Zhu, Y . Cheng, H. Zhou, and Y . Lu, ‚ÄúHermes attack: Steal DNN
models with lossless inference accuracy,‚Äù in 30th USENIX Security
Symposium (USENIX Security 21) , 2021.