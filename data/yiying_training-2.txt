¬© 2025, Amazon Web Services, Inc. or its affiliates. All rights reserved. Amazon Confidential and Trademark.
HLAT: High-quality Large Language Model Pre-trained on AWS TrainiumHaozhengFan, Hao Zhou, GuangtaiHuang, Parameswaran Raman, XinweiFu, Gaurav Gupta, Dhananjay Ram, YidaWang, Luke Huan AWS AI LabsPaper: https://arxiv.org/abs/2404.10630Scripts (internal): https://github.com/awslabs/HLAT¬© 2025, Amazon Web Services, Inc. or its affiliates. All rights reserved. Amazon Confidential and Trademark.
Background
2‚Ä¢LLM is a major breakthrough for AGI, and pretraining is crucial‚Ä¢LLMs trained on different chips‚Ä¢Nvidia GPU: GPT, Llama, Mistral, ‚Ä¶‚Ä¢TPU: Gemini, T5, Open Llama, ‚Ä¶‚Ä¢AMD GPU: OLMo (from allenai), ‚Ä¶‚Ä¢Distributed training frameworks‚Ä¢GPU: Deepspeed, FSDP , Nemo, SageMaker (Rubik), ‚Ä¶‚Ä¢TPU: Jax, EasyLM, Tensorflow, ‚Ä¶‚Ä¢Trn: Neuronx-Distributed, Neuron NeMo, ‚Ä¶‚Ä¢There is no paper or open sourced LLM trained on AWS Trainium¬© 2025, Amazon Web Services, Inc. or its affiliates. All rights reserved. Amazon Confidential and Trademark.
Contributions
3‚Ä¢We pre-train HLAT (High-quality LLM pre-trained on AWS Trainium), a 7B model over 1.8T tokens on 64 trn1.32xlarge instances. ‚Ä¢We are currently training 70B model.‚Ä¢HLAT provides comparable performance with 7B models trained on GPU (LLaMA-1, LLaMA-2) and TPU (OpenLLaMA-1, OpenLLaMA-2). ‚Ä¢We provide some best practices of pre-training process, e.g., different sharding strategies, training precisions, and fault tolerance mechanism, on AWS Trainium.‚Ä¢We design a novel dataloader which performs both tokenization and example packing during training.¬© 2025, Amazon Web Services, Inc. or its affiliates. All rights reserved. Amazon Confidential and Trademark.
Background: distributed training on Trainium
4‚Ä¢AWS Trainium‚Ä¢Trn1.32xlarge contains 16 Trn accelerators, and 32 Neuron Cores‚Ä¢16GB memory per Neuron Core‚Ä¢3040 TFLOPS in FP16/BF1‚Ä¢Cost $21.50 vs. p4d.24xlarge $32.77‚Ä¢Neuron Distributed Training Library (NDTL, also called NeuronX-distributed)‚Ä¢Tensor, pipeline, data, and sequence parallelism‚Ä¢Zero-1 optimizer‚Ä¢Multiple training precision configurations‚Ä¢Automatic fault recovery‚Ä¢‚Ä¶¬© 2025, Amazon Web Services, Inc. or its affiliates. All rights reserved. Amazon Confidential and Trademark.
Training Setups
5¬© 2025, Amazon Web Services, Inc. or its affiliates. All rights reserved. Amazon Confidential and Trademark.
HLAT Model Architecture

6‚Ä¢HLAT adopts the decoder-only transformer architecture and applies same modifications used in LLaMA 7B‚Ä¢pre-normalization with RMSNorm‚Ä¢SwiGLU activation function‚Ä¢Rotary Embeddings‚Ä¢4K sequence length‚Ä¢Dataset‚Ä¢We use public datasets and train for 1.8T tokens¬© 2025, Amazon Web Services, Inc. or its affiliates. All rights reserved. Amazon Confidential and Trademark.
Training Hyperparameters
7‚Ä¢Cosine learning rate scheduler ‚Ä¢Maximum learning rate of 3ùëí‚àí4.‚Ä¢Minimum learning rate of 3ùëí‚àí5.‚Ä¢Linear warmup of 2000 steps.‚Ä¢AdamW optimizer with ùõΩ=[0.9,0.95].‚Ä¢Weight decay value of 0.1 for all parameters, including normalization weights.‚Ä¢Gradient-norm clipping of 1.0.‚Ä¢1.8 trillion tokens, ~450k steps¬© 2025, Amazon Web Services, Inc. or its affiliates. All rights reserved. Amazon Confidential and Trademark.
Orchestration
8‚Ä¢For model training, we utilize a cluster with 64 trn1.32xlarge instances (nodes) with totaling to 1024 AWS Trainium accelerators.‚Ä¢We manage the cluster orchestration using Amazon EKS, a fully-managed Kubernetes service. Amazon EKS simplifies the deployment of Kubernetes both on AWS and on-premises environments.‚Ä¢We utilize pre-flight burn-in tests and regular health check (nccom) to improve cluster stability‚Ä¢Neuron Persistent Cache on Local Worker:‚Ä¢All instances share a same file system using Amazon FSx for storing data, checkpoints, logs, etc. ‚Ä¢FSx may cause communication bottleneck because those cached graphs are frequently accessed ‚Ä¢We store Neuron Persistent Caches in file system of each local worker. ¬© 2025, Amazon Web Services, Inc. or its affiliates. All rights reserved. Amazon Confidential and Trademark.
Training Efficiency Optimization
9‚Ä¢Model Parallelism: shard the model into 8 TP/SP degrees‚Ä¢Selective Activation Checkpoint: increases throughput at the cost of minimum memory overhead‚Ä¢BF16 with Stochastic Rounding: AWS Trainium features BF16 with stochastic rounding (SR)‚Ä¢Coalescing Layers with Same Inputs: Coalesced linear layers with the same inputs‚Ä¢Constant Attention Mask: As a decoder-only model, HLAT pretraining uses a constant attention mask (lower-triangular) matrix.‚Ä¢Asynchronous Execution: We use NEURON_RT_ASYNC_EXEC_MAX_INFLIGHT_REQUESTS=3‚Ä¢Compiler Optimization:‚Ä¢compiler flag --distribution-strategy=llm-training. ‚Ä¢Compiler flag --model-type=transformer‚Ä¢NEURON_FUSE_SOFTMAX=1¬© 2025, Amazon Web Services, Inc. or its affiliates. All rights reserved. Amazon Confidential and Trademark.
Training Process
10¬© 2025, Amazon Web Services, Inc. or its affiliates. All rights reserved. Amazon Confidential and Trademark.
Training Curves
11¬© 2025, Amazon Web Services, Inc. or its affiliates. All rights reserved. Amazon Confidential and Trademark.
12Hardware and System StabilityThe pre-training process can be interrupted due to‚Ä¢hardware failures‚Ä¢communication timeouts‚Ä¢system upgrades‚Ä¢run out of disk spaceFault recovery automatically restarts training from latest saved checkpoints without manual intervention¬© 2025, Amazon Web Services, Inc. or its affiliates. All rights reserved. Amazon Confidential and Trademark.
Performance Evaluation
13¬© 2025, Amazon Web Services, Inc. or its affiliates. All rights reserved. Amazon Confidential and Trademark.
Evaluation Datasets
14‚Ä¢MMLU (Massive Multitask Language Understanding)‚Ä¢BIG-Bench Hard (BBH)‚Ä¢Commonsense Reasoning‚Ä¢PIQA, HellaSwag, WinoGrande, ARC easy and challenge, and OpenBookQA.‚Ä¢World Knowledge‚Ä¢NaturalQuestions, TriviaQA‚Ä¢Reading Comprehension: BoolQ‚Ä¢Math: GSM8K‚Ä¢Coding: HumanEval¬© 2025, Amazon Web Services, Inc. or its affiliates. All rights reserved. Amazon Confidential and Trademark.
Performance vs Opensource models
15
¬© 2025, Amazon Web Services, Inc. or its affiliates. All rights reserved. Amazon Confidential and Trademark.
Performance vs Opensource models, cont‚Äôd
16‚Ä¢On MMLU tasks, HLAT performs better than OpenLLaMA-1 and LLaMA-1, and is worse then LLaMA-2.‚Ä¢May due to the difference in training datasets.‚Ä¢On BBH, Commonsense Reasoning, and World Knowledge tasks, HLAT performs similar as OpenLLaMA-1 and OpenLLaMA-2 models.‚Ä¢HLAT excels in 19/29 tasks as compared with OpenLLaMA-1, and 15/29 tasks compared with OpenLLaMA-2.‚Ä¢On Math problems (GSM8K), HLAT performs significantly better than OpenLLaMA-1 and OpenLLaMA-2.‚Ä¢HLAT has a big improvement of Math ability in later training stage.‚Ä¢On Coding problems, HLAT performs better than OpenLLaMA-1, comparable with LLaMA-1, and worse than OpenLLaMA-2 and LLaMA-2.¬© 2025, Amazon Web Services, Inc. or its affiliates. All rights reserved. Amazon Confidential and Trademark.
Intermediate Performance
17
¬© 2025, Amazon Web Services, Inc. or its affiliates. All rights reserved. Amazon Confidential and Trademark.
Intermediate Performance, cont‚Äôd
18‚Ä¢For Commonsense Reasoning, the model accuracy improves quickly at beginning of training, and starts to saturate at later training stages. This is similar as the trends observed in other LLM model trainings‚Ä¢For Math task (GSM8K), the learning curve shows an exponentially increasing trend. It increase very gradually for the initial ~1 trillion tokens and begins to improve significantly during the later stages of training‚Ä¢For World Knowledge task, the performance increases almost linearly with number of training tokens‚Ä¢May related to close-book testing problem‚Ä¢Those observations indicate the necessity of a set of evaluation tasks covering a wide range of domains for LLM pre-training.¬© 2025, Amazon Web Services, Inc. or its affiliates. All rights reserved. Amazon Confidential and Trademark.
Truthfulness and Bias
19
¬© 2025, Amazon Web Services, Inc. or its affiliates. All rights reserved. Amazon Confidential and Trademark.
Cost and Scalability‚Ä¢On 64 nodes, training HLAT-7B model costs about 54% of the GPU baseline.
20‚Ä¢Plot shows normalized CPT for HLAT 7B and 70B models on AWS Trainium with various number of nodes. ‚Ä¢CPT refers to cost per 4-million tokens‚Ä¢CPT of GPU baseline is normalized to 100%. ‚Ä¢70B models ran into out-of-memory on 4 nodes.¬© 2025, Amazon Web Services, Inc. or its affiliates. All rights reserved. Amazon Confidential and Trademark.
Limitation and Best Practices
21¬© 2025, Amazon Web Services, Inc. or its affiliates. All rights reserved. Amazon Confidential and Trademark.
Limitation
22‚Ä¢HLAT suffers a set of limitations such as hallucinations, potential non-factual generations, biases, and toxicity.‚Ä¢Usage of publicly available datasets‚Ä¢Not went through a supervised finetuning and human preference alignment‚Ä¢Training stops after 1.8T tokens‚Ä¢As is suggested by intermediate checkpoint performance, HLAT may be able to further improve on certain tasks, such as math and world knowledge, with more training tokens‚Ä¢Neuron Distributed Training Library‚Ä¢TP degree is limited to 8 and 32‚Ä¢Pre-compilation takes time, needs re-compilation if dimension changes¬© 2025, Amazon Web Services, Inc. or its affiliates. All rights reserved. Amazon Confidential and Trademark.
Best Practices
23‚Ä¢Parallelism: To achieve the highest training throughput, parallelism configuration needs to be jointly optimized with choice of activation checkpointing method, gradient accumulation steps, and training precision. ‚Ä¢Training Precision: BF16 with SR may has drifting issues. Using master weights can alleviate the problem, but incurs higher memory usage. ‚Ä¢Choice of ùõΩ2 : ùõΩ!=0.99 may cause numerical instability for BF16 due to limited precision. ùõΩ!=0.95 alleviates the problem.  ‚Ä¢Weight decay: applying weight decay on normalization layers or not does not affect much performance.‚Ä¢Dataloader: Nemo dataloader tends to overfit due to repetition within sequence.¬© 2025, Amazon Web Services, Inc. or its affiliates. All rights reserved. Amazon Confidential and Trademark.
Best Practices
24‚Ä¢Parallelism: To achieve the highest training throughput, parallelism configuration needs to be jointly optimized with choice of activation checkpointing method, gradient accumulation steps, and training precision. ‚Ä¢Training Precision: BF16 with SR may has drifting issues. Using master weights can alleviate the problem, but incurs higher memory usage. ‚Ä¢Choice of ùõΩ2 : ùõΩ!=0.99 may cause numerical instability for BF16 due to limited precision. ùõΩ!=0.95 alleviates the problem.  ‚Ä¢Weight decay: applying weight decay on normalization layers or not does not affect much performance.‚Ä¢Dataloader: Nemo dataloader tends to overfit due to repetition within sequence.¬© 2025, Amazon Web Services, Inc. or its affiliates. All rights reserved. Amazon Confidential and Trademark.
Learnings and Next Steps
25¬© 2025, Amazon Web Services, Inc. or its affiliates. All rights reserved. Amazon Confidential and Trademark.
SR causes large loss variance
26
‚Ä¢Blue: SR/BF16‚Ä¢Orange: SR/master_weights‚Ä¢Green: RNE/master_weights‚Ä¢Red: RNE/master_weights/Fix TP Drift¬© 2025, Amazon Web Services, Inc. or its affiliates. All rights reserved. Amazon Confidential and Trademark.
Gradient instability can indicate bad precision setting
27‚Ä¢Gradient norm increases fast is bad‚Ä¢Loss decreases slow is bad
‚Ä¢Black: SR/BF16‚Ä¢Blue: RNE‚Ä¢Red: RNE/FP32 operators/FP32 cc‚Ä¢Yellow: RNE/GPU-compatible precision¬© 2025, Amazon Web Services, Inc. or its affiliates. All rights reserved. Amazon Confidential and Trademark.
Train loss goes to zero indicates something is wrong
28‚Ä¢Turns out to have bug in constant attention mask‚Ä¢black: baseline with information leak‚Ä¢Red/Yellow: Fix information leak‚Ä¢Take away: Information leak is not visible until 10k steps
¬© 2025, Amazon Web Services, Inc. or its affiliates. All rights reserved. Amazon Confidential and Trademark.
Bad initialization causes divergence 
29‚Ä¢Turns out to have bug in constant attention mask and gradient all-reduce‚Ä¢Green / Black: Baseline. Divergent at 22k/23k steps‚Ä¢Yellow: Enabled scaled initialization. Convergent.
¬© 2025, Amazon Web Services, Inc. or its affiliates. All rights reserved. Amazon Confidential and Trademark.
Some bugs are hard to observe from loss curve
30‚Ä¢We had a bug in gradient all-reduce (dark orange)‚Ä¢Pink is after bug fix
¬© 2025, Amazon Web Services, Inc. or its affiliates. All rights reserved. Amazon Confidential and Trademark.
Next Steps
31‚Ä¢Training with GPU-compatible precision on Trainium‚Ä¢Use RNE and mixed precision‚Ä¢Generate best model quality with moderate throughput‚Ä¢ETA mid Apr‚Ä¢Llama2 70B pretraining‚Ä¢Train Llama2 70B on 1T+ tokens from a mixture of public datasets‚Ä¢Compare evaluation results with open sourced LLM checkpoints‚Ä¢ETA mid June‚Ä¢Aspirational research topics on stochastic rounding‚Ä¢A potential option for decent model quality with best throughput‚Ä¢Needs experiments to verify the model quality¬© 2025, Amazon Web Services, Inc. or its affiliates. All rights reserved. Amazon Confidential and Trademark.
Thank you!Contact: Haozheng Fan (fanhaozh@), Yida Wang (wangyida@), Hao Zhou (zhuha@), Luke Huan (lukehuan@) ¬© 2025, Amazon Web Services, Inc. or its affiliates. All rights reserved. Amazon Confidential and Trademark.
Appendix
33¬© 2025, Amazon Web Services, Inc. or its affiliates. All rights reserved. Amazon Confidential and Trademark.
All intermediate performance
34
¬© 2025, Amazon Web Services, Inc. or its affiliates. All rights reserved. Amazon Confidential and Trademark.
Coalescing Layers with Same Inputs
35Coalescing Layers with Same Inputs: Coalesced linear layers with the same inputs‚Ä¢q_proj, k_proj, v_proj: we merge them into qkv_proj
‚Ä¢gate_proj, up_proj: we merge them into gate_up_proj
¬© 2025, Amazon Web Services, Inc. or its affiliates. All rights reserved. Amazon Confidential and Trademark.
Scaling Efficiency
36
The scaling efficiency for Llama2 7b:‚Ä¢87% on 32 nodes. MFU = 33.5%‚Ä¢72% on 64 nodes. MFU = 27.9%¬© 2025, Amazon Web Services, Inc. or its affiliates. All rights reserved. Amazon Confidential and Trademark.
Observations on training precision for 70B (preliminary)
37‚Ä¢Pure BF16 (without stochastic rounding) is usually bad (gradient can be unstable and explode)‚Ä¢This is observed for both 7B and 70B training. ‚Ä¢Using master weights usually has better accuracy with little throughput impact‚Ä¢But requires more memory, so may run into OOM issue‚Ä¢Always use Zero-1 with master weights‚Ä¢Using FP32 for certain operations can improve accuracy‚Ä¢E.g. gradient accumulation and all-reduce, softmax & cross-entropy calculation‚Ä¢Negatively impact throughput and memory footprint‚Ä¢Stochastic rounding may cause drift when parallelism dimension is large‚Ä¢Can be partially mitigated with zero-1