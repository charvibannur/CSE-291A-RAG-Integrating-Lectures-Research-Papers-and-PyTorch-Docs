query,split,raw_gold-text,num_gold_texts,num_retrieved,retrieved_docs,text_recall,text_precision,text_f1,num_gold_texts_matched,ctx_precision,ctx_recall,ctx_f1,chunk_overlap,min_chunk_overlap,max_chunk_overlap,chunk_overlap_count,bleu,text_supported@1,text_recall@1,text_supported@3,text_recall@3
Explain the importance of ImageNet in the works alexnet and googlenet.,multi,"('We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 dif- ferent classes.', 'We propose a deep convolutional neural network architecture codenamed Incep- tion, which was responsible for setting the new state of the art for classiÔ¨Åcation and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC14).')",2,6,"['as belonging to the class indicated by the image label. E ImageNet ImageNet is a dataset of millions of labeled images in thousands of categories. The images\nwere collected from the web and labelled by human labellers using Amazon‚Äôs Mechanical Turk\ncrowd-sourcing tool.', 'instance of a CIFAR-10 class, and that the object in the image be easily identiÔ¨Åable as belonging to the class indicated by the image label. E ImageNet\nImageNet is a dataset of millions of labeled images in thousands of categories.', '5 GoogLeNet We chose GoogLeNet as our team-name in the ILSVRC14 competition. This name is an homage to Yann LeCuns pioneering LeNet 5 network [10]. We also use GoogLeNet to refer to the particular\nincarnation of the Inception architecture used in our submission for the competition.', 'and bigger datasets to become available. 2 The Dataset ImageNet is a dataset of over 15 million labeled high-resolution images belonging to roughly 22,000 categories. The images were collected from the web and labeled by human labelers using Ama-\nzon‚Äôs Mechanical Turk crowd-sourcing tool.', 'of ten. Another difference is that the ImageNet images often contain multiple instances of ImageNet objects, simply due to the sheer number of object classes. For this reason, even a human would have difÔ¨Åculty approaching perfect accuracy on\nthis dataset.', 'layers and millions of parameters on the ImageNet dataset with 1 million training images. Since then, even larger and deeper networks have been trained [12]. The typical use of convolutional networks is on classi\x0ccation tasks, where\nthe output to an image is a single class label.', 'have ob- served consistent phenomena. To provide instances for dis- cussion, we describe two models for ImageNet as follows. Plain Network. Our plain baselines (Fig. 3, middle) are\nmainly inspired by the philosophy of VGG nets [41] (Fig. 3,\nleft).', 'image recognition (LeCun et al., 1998; Krizhevsky et al., 2012) and machine translation (Sutskever et al., 2014; Bahdanau et al., 2015; Wu et al., 2016). Along with this success is a paradigm shift from feature designing to architecture\ndesigning, i.e., from SIFT (Lowe, 1999), and HOG (Dalal & Triggs, 2005), to AlexNet (Krizhevsky\net al., 2012), VGGNet (Simonyan & Zisserman, 2014), GoogleNet (Szegedy et al., 2015), and\nResNet (He et al., 2016a).', 'networks with non-Inception architecture, however this requires careful manual design at this point. 5 GoogLeNet We chose GoogLeNet as our team-name in the ILSVRC14 competition. This name is an homage to\nYann LeCuns pioneering LeNet 5 network [10].', '2014. [13] K. He, X. Zhang, S. Ren, and J. Sun. Delving deep into rectiÔ¨Åers: Surpassing human-level performance on imagenet classiÔ¨Åcation. In ICCV , 2015. [14] G. E. Hinton, N. Srivastava, A. Krizhevsky, I. Sutskever, and\nR. R. Salakhutdinov.']",0.5,0.0,0.0,1,0.022727272727272728,0.5,0.04347826086956522,0.2635209838202437,0.2026726057906459,0.31141868512110726,10,2.8146288157849127e-07,0.0,0.0,0.0,0.0
How does auto-differentiation work in these frameworks?,single,"TensorFlow is an interface for expressing machine learning algorithms and an implementation for executing such algorithms ‚Ä¢PyTorch  is a programming framework for tensor computation, deep learning, and auto differentiation 6Deep Learning Programming Framework7Aspect PyTorch TensorFlow JAX NumPy Primary Use Deep learning Deep learning numerical and ML computing numerical computing Programming Paradigm Dynamic (eager execution)Static (Graph mode, or Eager)Functional transformations Procedural Auto grad dynamic comp graphstatic comp graph Functional -based with grad/jit",1,5,"[', vol. abs/2106.05974, 2021. [Online]. Available: https://arxiv.org/abs/2106.05974 [23] N. Shazeer, Y . Cheng, N. Parmar, D. Tran, A. V . et al B. A. Hechtman. [24] A. Paszke, S. Gross, S. Chintala, G. Chanan, E. Yang, Z. DeVito, Z. Lin,\nA. Desmaison, L. Antiga, and A. Lerer, ‚ÄúAutomatic Differentiation in\nPyTorch,‚Äù 2017.', 'Computation , In Press, 2018. Atilim Gunes Baydin, Barak A Pearlmutter, Alexey Andreyevich Radul, and Jeffrey Mark Siskind. Automatic differentiation in machine learning: a survey. Journal of machine learning research , 18\n(153):1‚Äì153, 2018.', 'Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in pytorch. 2017. [23] Noam Shazeer, Youlong Cheng, Niki Parmar, Dustin Tran, Ashish Vaswani, Penporn Koanan-\ntakool, Peter Hawkins, HyoukJoong Lee, Mingsheng Hong, Cliff Young, et al.', ', 2017. Dougal Maclaurin, David Duvenaud, and Ryan P Adams. Autograd: Reverse-mode differentiation of native Python. In ICML workshop on Automatic Machine Learning , 2015. Hongyuan Mei and Jason M Eisner.', 'autograd requires implementing a new Function subclass for each operation. Recall that Functions are what autograd uses to encode the operation history and compute gradients. The first part of this doc is focused on backward mode AD as it is the most widely used feature.', 'A Pearlmutter, Alexey Andreyevich Radul, and Jeffrey Mark Siskind. Automatic differentiation in machine learning: a survey. Journal of machine learning research , 18 (153):1‚Äì153, 2018. Rianne van den Berg, Leonard Hasenclever, Jakub M Tomczak, and Max Welling.', 'end of the iteration.) 2. Helps avoid certain reference cycles, (e.g., since the tensor output of the autograd.Function itself keeps a reference to the ctx). 3.', 'Pass (important for HW 1) 34Build the AutoDiff Graph 35Use AutoGrad ‚Ä¢use finite differences to check our gradient calculations ùúïùëì(ùë•1,ùë•2) ùúïùë•1=ùëìùë•1+‚Ñé,ùë•2‚àíùëì(ùë•1‚àí‚Ñé,ùë•2) 2‚Ñé ‚Ä¢Care the precision! oUse double precision (fp64)\noPick a small ‚Ñé=0.000001\noCompute the forward difference through the graph twice\n36How to check the correctness of gradient‚Ä¢Learning parameters of an NN needs gradient calculation\n‚Ä¢Computation Graph\noto perform computation: topological traversal along the DAG\n‚Ä¢Auto Differentiation\nobuilding backward computation graph for gradient calculation\n‚Ä¢https:// github.com /mattjj /autodidact/\n37Summary‚Ä¢Auto Diff survey, https://arxiv.org/abs/1502.05767  \n‚Ä¢The Elements of Differentiable Programming (Book), \nhttps://arxiv.org /abs/2403.14606  \n38Additional Reading‚Ä¢TensorFlow: A System for Large -Scale Machine Learning, \nOSDI 2016.', 'and Ryan P Adams. Autograd: Reverse-mode differentiation of native Python. In ICML workshop on Automatic Machine Learning , 2015. Hongyuan Mei and Jason M Eisner. The neural Hawkes process: A neurally self-modulating\nmultivariate point process.', 'Hoffman, Marcus Brubaker, Daniel Lee, Peter Li, and Michael Betan- court. The Stan math library: Reverse-mode automatic differentiation in c++. arXiv preprint arXiv:1509.07164 , 2015. Bo Chang, Lili Meng, Eldad Haber, Lars Ruthotto, David Begert, and Elliot Holtham.']",0.0,0.0,0.0,0,0.08771929824561403,1.0,0.16129032258064516,0.2821541846699104,0.18944844124700239,0.48125,10,1.6035037765703961e-07,0.0,0.0,0.0,0.0
"What are FlashMLA, DeepEP, and DeepGEMM, and what problems do they each solve?",single,"Accelerating Transformer Layers‚Ä¢FlashMLA  (released 2/24/2025) FlashMLA  is an efficient MLA decoding kernel for Hopper GPUs, optimized for variable -length sequences serving. DeepEP  (released 2/25/2025) oa communication library tailored for Mixture -of-Experts ( MoE) and expert parallelism (EP). It provides high -throughput and low -latency all -to-all GPU kernels, which are also as known as MoE dispatch and combine. DeepGEMM  (released 2/26/2025) oDeepGEMM  is a library designed for clean and efficient FP8 General Matrix Multiplications (GEMMs) with fine -grained scaling 3Deepseek  opensource libraries",1,7,"['and expert parallelism (EP). It provides high -throughput and low -latency all -to-all GPU kernels, which are also as known as MoE dispatch and combine. ‚Ä¢DeepGEMM  (released 2/26/2025)\noDeepGEMM  is a library designed for clean and efficient FP8 General Matrix \nMultiplications (GEMMs) with fine -grained scaling\n3Deepseek  opensource libraries\nhttps://github.com/deepseek -ai/ ‚Ä¢Overview of large -scale model training\n‚Ä¢Multi -GPU communication\n‚Ä¢Data Parallel Training via AllReduce\n4OutlineTransformerGPT1GPT2GPT3GopherPALMGPT4\nNemotron\nLLaMA3 -8BLLaMA3.1\nQwen2DeepSeek -v3\n001101001,00010,000\n2016 2017 2018 2019 2020 2021 2022 2023 2024 2025size(B)\n5Scale of LLMs\n671B1.8TGPT1GPT2GPT3 GopherPALMGPT4 LLaMA3.1\nQwen2DeepSeek -v3\n01101001,00010,000100,000\n2017 2018 2019 2020 2021 2022 2023 2024 2025 2026Tokens (B)\n6Scale of Training Data\n15T‚Ä¢Pretraining for Deepseek  V3 (671B)\no2,048 H800 GPUs\notrained for 2 months\noa total of 2.664 million H800 GPU hours\n‚Ä¢LLaMA  3.1(405B) \nousing 16,000 H100 GPUs\noa total of 30.84 million GPU hours\n7Large -scale Distribution TrainingStrategies for Scalable Training\n8\nPartition the data\nsingle node \ndata parallel\ndistributed \ndata parallel\nparameter \nserver\nPartition the Model\nModel \nparallel\nPipeline \nparallel\nTensor \nparallel9Classical Distributed Training: Parameter Server\nData\nworker worker worker worker\nlocal grad local grad local grad local grad\npush (worker to server)partition\nParameter server\naggregate grads, update parameterspull\n1\n2\n3\n 3\n 3\n 3\n4\n5‚Ä¢Overview of large -scale model training\n‚Ä¢Multi -GPU communication\n‚Ä¢Data Parallel Training via AllReduce\n10Outline\n‚Ä¢NCCL (Nvidia Collective Communication Library)\noprovides inter -GPU communication APIs\noboth collective and point -to-point send/receive primitives\nosupports various of interconnect technologies\n‚Ä¢PCIe\n‚Ä¢NVLink\n‚Ä¢InfiniBand\n‚Ä¢IP sockets\noOperations are tied to a CUDA stream.', 'accommodated within the GPU memory. Compatibility with FlashAttention [ 10].FlashAttention is an attention backend widely used in modern LLM serving system to accelerate attention computation. It avoids writing attention weights\nto HBM through kernel fusion, thus significantly reducing data movement between on-chip memory\nand HBM.', '51,60]. To reduce the memory footprint, both methods avoid storing the large attention matrix in the forward pass and recompute it in the backward pass. The Ô¨Årst major diÔ¨Äerence is that Rabe and Staats [66]focuses on the reducing the total memory footprint\n(maximum amount of GPU memory required) while FlashAttention focuses on reducing memory accesses\n(the number of memory reads/writes).', 'FlashAttention be- haves the same as the baseline implementation and the validation perplexity curves of the two implementations almost lie on top of each other. Long Document ClassiÔ¨Åcation.', 'FlashAttention ‚Äôs correctness, runtime, and memory requirement (proof in Appendix C). Theorem 1. Algorithm 1 returns O=softmax¬πQK>¬∫VwithùëÇ¬πùëÅ2ùëë¬∫FLOPs and requires ùëÇ¬πùëÅ¬∫additional memory beyond inputs and output. 3.2 Analysis: IO Complexity of FlashAttention\nWe analyze the IO complexity of FlashAttention , showing signiÔ¨Åcant reduction in HBM accesses compared\nto standard attention.', 'that FlashAttention has smaller total memory requirement compared to Rabe and Staats [66]. The Ô¨Ånal major diÔ¨Äerence is the way the backward pass is computed. Rabe and Staats [66]uses gradient\ncheckpointing to recompute the attention matrix and the temporary output of each block.', '1. Within this process, FLEXINFER kernel achieves 1.01 √óof native FlashAttention performance and 1.23 √óof Paged FlashAt- tention performance, proving its robustness and computation flexibility. 7.2.2 Prefix-prefilling Kernel Evaluation\nOn the left of Figure 8, we evaluate different prefix-prefilling\nkernels with varied batch sizes.', 'However, it focuses specifically on prefetching and does not address other opti- mizations such as unsharding or offloading, which leads to notable differences in design. In SimpleFSDP, communica-\ntion operations are inserted into the user-level code (Python)\nbefore compilation, whereas DeepCompile first compiles the\nmodel into a computation graph and then applies transforma-\ntions.', 'Flashattention-2: Faster attention with better parallelism and work partitioning. CoRR , abs/2307.08691, 2023. doi: 10.48550/ARXIV .2307.08691. URL https://doi.org/10. 48550/arXiv.2307.08691 . Tri Dao, Daniel Y . Fu, Stefano Ermon, Atri Rudra, and Christopher R√©.', 'corpus composed of encyclopedia data, web data, ebook data, etc. For DPMoE, we perform all the experiments with our implementation based on Megatron-LM8v2.5 and DeepSpeed9v0.5.10. For PPMoE, we build our codebase upon the implementation of Megatron-\nLM v2.6.']",1.0,0.1,0.18181818181818182,1,0.06060606060606061,0.5,0.10810810810810813,0.2590757846707296,0.13846153846153847,0.4368421052631579,10,0.07490978803373669,0.0,1.0,0.0,1.0
What are the challenges of theoretical distributed training (PipeDream) and what is the scaling efficiency of distributed real-world Trainium performance (HLAT)?,multi,"('Challenge 1: Stage Partitioning‚Ä¢How to partition model layers into the stages evenly? ‚Ä¢Throughput depends on the slowest stage in pipeline ‚Ä¢Solution: ‚Ä¢ProÔ¨Åle layer-wise perf and comm perf ‚Ä¢Allows a stage to be replicated (DP) ‚Ä¢Uses dynamic programming to Ô¨Ånd optimal partition and layer replication', 'Challenge 2: Work Scheduling‚Ä¢How to schedule forward and backward computation on a worker? ‚Ä¢Solution: 1F1B-RR ‚Ä¢Run one forward and one backward ‚Ä¢Round-robin across replicated stages', 'Challenge 3: Weight Versioning‚Ä¢How to ensure the same minibatch uses the same weight version across workers for forward and backward? ‚Ä¢Otherwise computation will be far oÔ¨Ä and training not able to converge ‚Ä¢Solution: Store multiple weight versions so that the backward and forward of the same minibatch ‚Ä¢Weights across workers can be diÔ¨Äerent!', 'The scaling efficiency for Llama2 7b:‚Ä¢87% on 32 nodes. MFU = 33.5%‚Ä¢72% on 64 nodes. MFU = 27.9%¬© 2025, Amazon Web Services, Inc. or its affiliates. All rights reserved. Amazon Confidential and Trademark. Observations on training precision for 70B (preliminary)')",4,7,"['recent years for distributed LLM training, including model parallelism, data parallelism, and pipeline parallelism [ 31;27;19]. However, a single parallelism strategy has limitations regarding scalability. For example, the performance of data parallelism with\nZeRO-3 will decrease dramatically when the number of GPUs increases to several thousands [21].', 'assume that workloads remain stable dur- ing training. Consequently, they fail to handle the pipeline stalls introduced by dynamic models, leading to reduced computational efficiency. Innovative designs of dynamic models aim to reduce compu-\ntational cost, but without effective load balancing, their benefits\npractically fail to translate into actual performance gains during\ndistributed training [ 4].', '[cs.LG] 23 Apr 2025Training large-scale MoE models, however, presents significant challenges. As the model size increases, efficient distributed training across thousands of GPUs becomes essential. Different paral-\nlelism strategies have been proposed in recent years for distributed LLM training, including model\nparallelism, data parallelism, and pipeline parallelism [ 31;27;19].', 'reducing memory footprint in distributed training o‚ûî enables training significantly larger models ‚Ä¢Key idea: partition optimizer states, gradients, and parameters. ‚Ä¢Pros: oLower memory usages significantly. oScalable, flexible, easy -to-use.', 'Figure 10 shows the training losses for both settings. Although some operators are non-deterministic and introduce subtle differ- ences, the loss curves were closely aligned. 6 Related Work\n6.1 Distributed Training Framework\nAs discussed in Section 2, various distributed training strate-\ngies have been proposed to scale deep learning models be-\nyond the capacity of a single GPU.', 'Volume 2 . 1112‚Äì1127. [53] Xupeng Miao, Yining Shi, Zhi Yang, Bin Cui, and Zhihao Jia. 2023. SDPipe: A Semi-Decentralized Framework for Heterogeneity-aware Pipeline-parallel Training. Proc. VLDB Endow. 16 (2023).', 'bottlenecks by hori- zontally scaling CPU nodes and leveraging a coordinated read mechanism to mitigate straggler issues caused by input size variability in distributed training. Specifically, it is comprised\nof four key components: a dispatcher, a pool of workers,\nclients, and an orchestrator.', 'distributed training o‚ûî enables training significantly larger models ‚Ä¢Key idea: partition optimizer states, gradients, and parameters. ‚Ä¢Pros: oLower memory usages significantly. oScalable, flexible, easy -to-use. ‚Ä¢Cons:\noSome stages introduce extra communication overhead, depending on \ninfrastructure (PCI -E / NVLink )\n75Summary‚Ä¢https://github.com/llmsystem/llmsys_code_examples/blob/m\nain/deepspeed_example/DeepSpeed -Example.ipynb  \n76Code Example', 'Distributed Training System. arXiv preprint arXiv:2203.14685 . Zheng, Z.; Yaqi, X.; Hulin, W.; Donglin, Y . ; Chuang, H.; Xiaobo, Z.; and Dazhao, C. 2024. MPMoE: Memory Effi-\ncient MoE for Pre-Trained Models With Adaptive Pipeline\nParallelism.', 'about future tokens [44]. Production distributed training frameworks typically apply static load balancing at the start of training and maintain the same distri- bution throughout. For example, Megatron-LM [ 50] evenly splits\ntransformer layers across accelerators.']",0.0,0.0,0.0,0,0.0,0.0,0.0,0.254194367612887,0.15848214285714285,0.35094339622641507,10,2.029116224770119e-07,0.0,0.0,0.0,0.0
What are the three core components of the TinyServe system?,single,"The system is organized around three core components: (1)Query-Aware KV Retriever: Dynamically selects relevant key-value blocks at decode time based on the current query vector and page-level metadata, reducing unnecessary mem- ory access. (2)Modular Scheduling Pipeline: A dispatch loop handles incoming queries and routes them through configurable plug- ins (e.g., entropy-based early exit, token-level pruning, ap- proximate attention). This modular design allows experimen- tation with different sparsity strategies without modifying the core model. (3)Sparse Attention Executor: Efficiently computes attention over selected KV pages using fused CUDA kernels, with support for FP16/INT8 KV formats and multi-GPU dispatch.",1,1,"['frameworks. 3 Methodology 3.1 System Overview: TinyServe TinyServe is a lightweight serving framework designed for serving tiny language models under tight memory and latency constraints. Rather than acting as a benchmarking tool, TinyServe serves as a\nreal-time serving environment that enables sparsity-aware atten-\ntion, modular token selection, and efficient KV-cache reuse.', 'than acting as a benchmarking tool, TinyServe serves as a real-time serving environment that enables sparsity-aware atten- tion, modular token selection, and efficient KV-cache reuse. The system is organized around three core components:\n(1)Query-Aware KV Retriever: Dynamically selects relevant\nkey-value blocks at decode time based on the current query\nvector and page-level metadata, reducing unnecessary mem-\nory access.', 'cache activation; - Memory-aware scheduling (e.g., prefetching selected pages); - Reduced HBM bandwidth pressure. System Implication. TinyServe enables dynamic query-aware sparsity without requiring architectural retraining. The modular\nimplementation integrates directly into TinyServe‚Äôs kernel loop\nand allows hardware-sensitive scheduling: e.g., keeping hot pages\nin shared memory or limiting K to match tensor core granularity.', '} KV load+ùúèattn(ùêæ¬∑ùëÜ) This structure-aware design ensures: - Query-dependent cache activation; - Memory-aware scheduling (e.g., prefetching selected pages); - Reduced HBM bandwidth pressure. System Implication. TinyServe enables dynamic query-aware\nsparsity without requiring architectural retraining.', 'models (e.g., 125M‚Äì350M parameters). TinyServe replicates core compo- nents of LLM serving‚Äîstreaming decoding, KV cache management, token routing, and quantization‚Äîin a fully controllable environ- ment. Crucially, it supports fine-grained instrumentation and plug-\nin modules such as entropy-based early exit, query-aware KV se-\nlection, and approximate attention.', 'core model. (3)Sparse Attention Executor: Efficiently computes attention over selected KV pages using fused CUDA kernels, with support for FP16/INT8 KV formats and multi-GPU dispatch. In TinyServe, each decode step activates the TinyServe pipeline:\nthe query vector is used to score KV pages, top-ranked pages are\nfetched, sparse attention is performed, and plug-in modules may\ntrigger pruning or early stopping.', 'exhibits bursty memory loads. TinyServe shows smoother and significantly lower access patterns, re- maining well below the HBM threshold, benefiting from query-aware page-level KV selection. Vertical dotted lines\nmark key transitions in token reuse or decoding stages.', 'integrates directly into TinyServe‚Äôs kernel loop and allows hardware-sensitive scheduling: e.g., keeping hot pages in shared memory or limiting K to match tensor core granularity. The kernel design for TinyServe can be found at algorithm 1.', 'to two bot- tlenecks: ‚Ä¢Memory movement : loading allùëòùëñ,ùë£ùëñfrom high-bandwidth memory (HBM); ‚Ä¢Unstructured access : attention requires full key scan with no cache prefetch pattern. To address this, TinyServe introduces a structured memory\nlayout via token grouping into fixed-size pages .', 'emphasize that carefully designed synthetic stressors and caching strategies are essential for both graph-based and language-based workloads, re- inforcing the importance of lightweight analysis frameworks. 3 Methodology\n3.1 System Overview: TinyServe\nTinyServe is a lightweight serving framework designed for serving\ntiny language models under tight memory and latency constraints.']",1.0,0.4,0.5714285714285715,1,0.1111111111111111,0.75,0.19354838709677416,0.34540838287094405,0.21577726218097448,0.5660377358490566,10,0.13238010034611578,0.0,0.0,0.3333333333333333,1.0
What are the trade-offs between simple post-training quantization and GPTQ?,multi,"('8CUDA APIs for Half Precision‚Ä¢Using lower precision oconverting parameters from FP32 to INT8 or INT4 operform all computation in lower prevision. Reduce model accuracy: oLoss of Precision ‚ûî accumulate quantization noise oRange mismatch ‚ûî values are clipped and lead to information loss oQuantization error ‚ûî rounding errors 9Direct Quantization Approach‚Ä¢Absmax  quant Zero -point quant 10Quantize a number', 'mixed strategy for 8 -bit quantization and 16 -bit (for outliers)2Recap of Basic Quantization Methods‚Ä¢Absmax  quant Zero -point quant 4Quantize a Number to Int8 5GPTQ scale to GPT -size LLMs maintain accuracy‚Ä¢Revisit  layer -wise quantization of weight matrices argmin', 'layer -wise quantization + compensation for errors + precompute oaccurately compress some of the largest publicly -available models down to 3 and 4 bits, and bring end -to-end speedups')",3,3,"['training ‚Ä¢Code Walkthrough 30Outline ‚Ä¢GPTQ is Post -Training Quantization (PTQ): converting the weights of an already trained model to a lower precision without any retraining. ‚Ä¢Quantization -Aware Training (QAT): integrates the weight \nconversion process during the training stage.', 'model to a lower precision without any retraining. ‚Ä¢Quantization -Aware Training (QAT): integrates the weight conversion process during the training stage. often superior model performance. ( QLoRA ) \n31Quantization -Aware TrainingQLoRA = Low -rank + Quantized training\n‚óèMajor innovations:\n‚óã4-bit Normal Float for storage\n‚óãDouble Quantization\n‚óãPage Optimizer\n‚óèBF16 for computation\n‚ûîReduces the average memory requirements of fine -tuning a 65B \nparameter model from 780GB of GPU memory to 48GB on a single \nGPU.', '-NeoX20B) ‚Ä¢At 1.3B scale, computation time is ~3 hours obut slower than GPTQ (x100 larger in ~4 hours) ‚Ä¢integrated in Deepspeed 22ZeroQuant Yao et al. ZeroQuant : Efficient and Affordable Post -Training Quantization for Large -Scale Transformers.', 'Round -to- nearest methods? ‚Ä¢How does GPT -Q speed up model inference in practical applications? ‚Ä¢Does GPT -Q even work for extreme 2 -bit quantization? 22Effectiveness of GPTQ?‚Ä¢Calibration data randomly sampled from C -4 dataset to \nensure GPTQ is not task -aware.', '‚Ä¢How is GPT -Q‚Äôs perf on large models compared with Round -to- nearest methods? ‚Ä¢How does GPT -Q speed up model inference in practical applications? ‚Ä¢Does GPT -Q even work for extreme 2 -bit quantization?', 'of an already trained model to a lower precision without any retraining. ‚Ä¢Quantization -Aware Training (QAT): integrates the weight conversion process during the training stage. often superior \nmodel performance.', '3h Does GPT -Q even work for extreme 2 -bit quantization? 28 How is GPT -Q‚Äôs perf on small models compared with accurate -but-expensive methods? 29\n Fastest prior method‚Ä¢https://github.com/qwopqwop200/GPTQ -for-LLaMa/  \n‚Ä¢GPTQ in \nohttps://github.com/qwopqwop200/GPTQ -for-\nLLaMa/blob/triton/gptq.py  \n31GPTQ for LLaMAGPTQ: Initialization\n32\n‚óèReshape weights from the \ninput layer\n‚óèInitialize Hessian matrixGPTQ: Hessian Matrix Update\n33‚óèUpdate Hessian matrix with \ninformation from a new \nbatch of the input and \noutput pairs GPTQ: Lazy Batch -Update\n34\n‚óèProcesses weight matrix W in blocks.', '-Q even work for extreme 2 -bit quantization? 22Effectiveness of GPTQ?‚Ä¢Calibration data randomly sampled from C -4 dataset to ensure GPTQ is not task -aware. ‚Ä¢Standard uniform per -row asymmetric quantization on the \nmin-max grid\n‚Ä¢Quantize on each transformer block (6 layers), with input X \nfrom last quantized block output.', 'methods? ‚Ä¢How does GPT -Q‚Äôs quantization time scale with model size? ‚Ä¢How is GPT -Q‚Äôs perf on large models compared with Round -to- nearest methods? ‚Ä¢How does GPT -Q speed up model inference in practical \napplications?', '25 How is GPT -Q‚Äôs perf on large models compared with Round -to-nearest methods? 26 How does GPT -Q‚Äôs quantization time scale with model size? 27* Measured on single A100ZeroQuant -LKD GPT-Q\n1.3B model - 3h\nDoes GPT -Q even work for extreme 2 -bit quantization?']",0.0,0.0,0.0,0,0.06451612903225806,0.25,0.10256410256410256,0.30220266142236174,0.18244406196213425,0.3793103448275862,10,2.503181325537413e-07,0.0,0.0,0.0,0.0
What does ‚ÄúIO-aware‚Äù mean in the context of FlashAttention?,single,"In this paper, we argue that a missing principle is making attention algorithms IO-aware [1]‚Äîthat is, carefully accounting for reads and writes to diÔ¨Äerent levels of fast and slow memory (e.g., between fast GPU on-chip SRAM and relatively slow GPU high bandwidth memory, or HBM [ 45], Figure 1 left).",1,7,"['FLASHATTENTION: fast and memory-efficient exact attention with IO- awareness. InProceedings of the 36th International Conference on Neural Informa- tion Processing Systems(New Orleans, LA, USA)(NIPS ‚Äô22). Curran Associates\nInc., Red Hook, NY, USA, Article 1189, 16 pages.', 'accommodated within the GPU memory. Compatibility with FlashAttention [ 10].FlashAttention is an attention backend widely used in modern LLM serving system to accelerate attention computation. It avoids writing attention weights\nto HBM through kernel fusion, thus significantly reducing data movement between on-chip memory\nand HBM.', 'Stefano Ermon, Atri Rudra, and Christopher R√©. 2022. Flashattention: Fast and memory-efficient exact attention with io-awareness. Advances in Neural Information Processing Systems 35 (2022), 16344‚Äì16359. [24] Jingzhi Fang, Yanyan Shen, Yue Wang, and Lei Chen.', 'FlashAttention ‚Äôs correctness, runtime, and memory requirement (proof in Appendix C). Theorem 1. Algorithm 1 returns O=softmax¬πQK>¬∫VwithùëÇ¬πùëÅ2ùëë¬∫FLOPs and requires ùëÇ¬πùëÅ¬∫additional memory beyond inputs and output. 3.2 Analysis: IO Complexity of FlashAttention\nWe analyze the IO complexity of FlashAttention , showing signiÔ¨Åcant reduction in HBM accesses compared\nto standard attention.', 'R√©. Flashattention: Fast and memory-efficient exact attention with io-awareness. In Sanmi Koyejo, S. Mo- hamed, A. Agarwal, Danielle Belgrave, K. Cho, and A. Oh (eds. ), Advances in Neural\nInformation Processing Systems 35: Annual Conference on Neural Information Process-\ning Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9,\n2022 , 2022.', 'programmer: Asset or liability? Journal of Systems and Software 203 (2023), 111734. [23] Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher R√©. 2022. Flashattention: Fast and memory-efficient\nexact attention with io-awareness.', 'Fu, S. Ermon, A. Rudra, and C. R ¬¥e, ‚ÄúFlashattention: Fast and memory-efficient exact attention with io-awareness,‚Äù Advances in Neural Information Processing Systems , vol. 35, pp. 16 344‚Äì16 359,\n2022.15\n[14] W. Kwon, Z. Li, S. Zhuang, Y .', 'abs/2307.08691, 2023. [16] Tri Dao, Daniel Y . Fu, Stefano Ermon, Atri Rudra, and Christopher R√©. Flashattention: Fast and memory- efficient exact attention with io-awareness. In NeurIPS ,\n2022.', 'on the reducing the total memory footprint (maximum amount of GPU memory required) while FlashAttention focuses on reducing memory accesses (the number of memory reads/writes). As mentioned in Section 2, the amount of memory access is the\nprimary determining factor of runtime.', 'Software 203 (2023), 111734. [23] Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher R√©. 2022. Flashattention: Fast and memory-efficient exact attention with io-awareness. Advances in Neural Information Processing Systems 35 (2022), 16344‚Äì16359.']",0.0,0.0,0.0,0,0.0,0.0,0.0,0.2561671436828894,0.22127659574468084,0.29381443298969073,10,1.7628544065748924e-07,0.0,0.0,0.0,0.0
"What is internal covariate shift, and how does it affect training?",single,"We deÔ¨Åne Internal Covariate Shift as the change in the distribution of network activations due to the change in network parameters during training. To improve the training, we seek to reduce the internal covariate shift. By Ô¨Åxing the distribution of the layer inputs xas the training progresses,we expect to improve the training speed.",1,1,"['ImageNetclassiÔ¨Åcation.2 Towards Reducing Internal CovariateShift We deÔ¨Åne Internal Covariate Shift as the change in the distribution of network activations due to the change in networkparametersduringtraining. Toimprovethetrain-\ning, we seek to reduce the internal covariate shift.', 'refer to the change in the distributions of internal nodes of a deep network, in the course of training, as In- ternal Covariate Shift . Eliminating it offers a promise of\nfaster training.', '(Shimodaira, 2000). This is typically handled via domain adaptation (Jiang, 2008). However, the notion of covariate shift can be extended beyond the learningsystemasawhole,toapplytoitsparts,suchasa sub-networkora layer. Considera networkcomputing\n‚Ñì=F2(F1(u,Œò1),Œò2)\nwhereF1andF2are arbitrary transformations, and the\nparameters Œò1,Œò2are to be learned so as to minimize\nthe loss‚Ñì.', 'internal nodes of a deep network, in the course of training, as In- ternal Covariate Shift . Eliminating it offers a promise of faster training. We propose a new mechanism, which we\ncallBatch Normalization , that takes a step towards re-\nducing internal covariate shift, and in doing so dramati-\ncally accelerates the training of deep neural nets.', 'inputs remains more stable as the network trains, then the optimizer would be less likely to get stuck in the saturated regime, and the trainingwouldaccelerate. We refer to the change in the distributions of internal\nnodes of a deep network, in the course of training, as In-\nternal Covariate Shift .', 'as the change in the distribution of network activations due to the change in networkparametersduringtraining. Toimprovethetrain- ing, we seek to reduce the internal covariate shift. By\nÔ¨Åxingthe distributionof the layer inputs xas the training\nprogresses,weexpecttoimprovethetrainingspeed.', 'to the change in networkparametersduringtraining. Toimprovethetrain- ing, we seek to reduce the internal covariate shift. By Ô¨Åxingthe distributionof the layer inputs xas the training progresses,weexpecttoimprovethetrainingspeed. Ithas\nbeen long known (LeCunetal., 1998b; Wiesler &Ney,\n2011) that the network training convergesfaster if its in-\nputsarewhitened‚Äìi.e.,linearlytransformedtohavezero\nmeansandunitvariances,anddecorrelated.', 'Improving predictive inference under covariate shift by weighting the log-likelihood function. JournalofStatisticalPlanningandInference , 90(2):227‚Äì244,October2000. Srivastava, Nitish, Hinton, Geoffrey, Krizhevsky, Alex, Sutskever, Ilya, and Salakhutdinov, Ruslan. Dropout:\nA simple way to preventneural networksfrom overÔ¨Åt-\nting.J.', 'experiments,we foundthis effect to be advantageous to the generalization of the network. Whereas Dropout (Srivastavaet al., 2014) is typically used to reduce over- Ô¨Åtting,inabatch-normalizednetworkwefoundthatitcan beeitherremovedorreducedinstrength. 4 Experiments\n4.1 Activationsovertime\nTo verify the effects of internal covariate shift on train-\ning, and the ability of Batch Normalization to combat it,\nweconsideredtheproblemofpredictingthedigitclasson\ntheMNISTdataset(LeCunetal.,1998a).', 'to continu- ously adapt to the new distribution. When the input dis- tributiontoalearningsystemchanges,itissaidtoexperi- encecovariateshift (Shimodaira, 2000). This is typically handled via domain adaptation (Jiang, 2008). However,\nthe notion of covariate shift can be extended beyond the\nlearningsystemasawhole,toapplytoitsparts,suchasa\nsub-networkora layer.']",1.0,0.5,0.6666666666666666,1,0.3548387096774194,0.6666666666666666,0.46315789473684216,0.4314166314509495,0.15877437325905291,0.9930795847750865,10,0.07983061371329453,1.0,1.0,0.6666666666666666,1.0
What is NVIDIA GPU Confidential Computing (CC) and how does it secure communication?,single,"NVIDIA CC ensures the confidentiality and integrity of communication between a CVM and a GPU via AES-GCM encryption [ 15]. A critical component of AES- GCM is the Initialization Vector (IV), a unique, non-repeating number (a nonce) required for each encryption session.",1,1,"['hardware GPU confidential computing. A closer look at NVIDIA CC. Although CVMs encrypt their memory, this encryption is separate from that used by NVIDIA CC. NVIDIA CC ensures the confidentiality and\nintegrity of communication between a CVM and a GPU via\nAES-GCM encryption [ 15].', 'Hard- ware GPU confidential computing has low performance over- head and is backward-compatible with existing applications. This paper focuses on studying hardware GPU confidential computing. A closer look at NVIDIA CC.', 'insecure GPU commands, but this adds substantial performance overhead due to extra runtime checks for indirect memory access, heavily used in systems like vLLM [25]. Unlike software-based solutions, NVIDIA Confidential\nComputing relies on hardware: NVIDIA H100 GPU is the\nfirst commercial implementation with confidential comput-\ning capability [ 15].', 'low performance over- head and is backward-compatible with existing applications. This paper focuses on studying hardware GPU confidential computing. A closer look at NVIDIA CC. Although CVMs encrypt\ntheir memory, this encryption is separate from that used\nby NVIDIA CC.', 'of this is the NVIDIA H100 GPU [ 36], which supports confidential computing inside the GPU to protect sensitive data and models from unauthorized access. Moreover, the data communication between the CVM\nand the GPU enclave is encrypted, further reinforcing the\nsecurity of I/O operations.', '[25]. Unlike software-based solutions, NVIDIA Confidential Computing relies on hardware: NVIDIA H100 GPU is the first commercial implementation with confidential comput- ing capability [ 15]. Working with CVMs, H100 could build a\nGPU enclave, allowing users to have exclusive control over\nthe GPU and rejecting any access from the host, such as\n= AES(keyCPU, IV=1,       ) aCPU\nGPUEncrypt\nDecryptDecrypt\nEncrypt= AES(keyCPU, IV=2,       ) b\n= AES(keyGPU, IV=5,       )\n= AES(keyGPU, IV=6,       ) dab\nab\ncdcd\ncCVM shared memory\nIV = 3\nIV = 7Figure 1: Workflow of encrypted data transfer in NVIDIA\nCC.', 'respectively. read/write GPU memory and modify the control flow. Hard- ware GPU confidential computing has low performance over- head and is backward-compatible with existing applications. This paper focuses on studying hardware GPU confidential\ncomputing.', 'and encryption [27]. Confidential Computing (CC) on GPUs. Beyond CPU- based CVMs, confidential computing on GPUs secures GPU computations such as LLM serving and training. Soter [40],\ndesigned for edge computing, uses CPU-side confidential\ncomputing to eliminate the trust on GPU hardware.', 'is an illustra- tion of how NVIDIA Confidential Computing and PipeLLM execute it. CPU GPU GPUCPUDecrypt Encrypt Saved Time1c 3c 1t1c 1t PipeLLMNVIDIA CC# 1. Swap\xa0 from GPU\n# - PCIe transfer\n# - CPU decryption\ndata = swap_from_gpu()\n# 2.', 'transfers of the NVIDIA CC. Consider the process of copying memory from the CPU to the GPU; the reverse process follows a simi- lar pattern. All CPU-side application data resides in the\nCVM‚Äôs private encrypted memory.']",1.0,0.2,0.33333333333333337,1,0.0625,0.5,0.1111111111111111,0.2757501407808912,0.15916955017301038,0.4357142857142857,10,0.044785085320257136,1.0,1.0,0.3333333333333333,1.0
What is the difference between torch.disttibuted and torch.distributed.pipelining?,multi,"('While promising for scaling, pipelining is often difficult to implement because it needs to partition the execution of a model in addition to model weights. The partitioning of execution often requires intrusive code changes to your model. Another aspect of complexity comes from scheduling micro-batches in a distributed environment, with data flow dependency considered.', 'The torch.distributed  package provides PyTorch support and communication primitives for multiprocess parallelism across several computation nodes running on one or more machines.')",2,1,"['has_loss_and_backward , loss_spec ) torch.distributed.pipelining. pipe_split ( ) pipe_split is a special operator that is used to mark the boundary between stages in a module. It is used to split the module into stages.', '2025 torch.distributed.pipelining is currently in alpha state and under development. API changes may be possible. It was migrated from the PiPPy project. Why Pipeline Parallel? Pipeline Parallelism is one of the primitive parallelism for deep learning.', 'Pipeline Parallelism\nCreated On: Jun 16, 2025 | Last Updated On: Jun 16, 2025\ntorch.distributed.pipelining  is currently in alpha state and under development. API\nchanges may be possible. It was migrated from the PiPPy project. Why Pipeline Parallel?', 'splitting the module. (default: None) Return type: A pipeline representation of class Pipe. class torch.distributed.pipelining. Pipe (split_gm , num_stages , has_loss_and_backward , loss_spec ) torch.distributed.pipelining. pipe_split ( )\npipe_split is a special operator that is used to mark the boundary between stages in a module.', 'a commonality that the computation per device cannot hide the communication of conventional parallelism, for example, the weight all-gather of FSDP. What is torch.distributed.pipelining ? While promising for scaling, pipelining is often difficult to implement because it needs to partition\nthe execution of a model in addition to model weights.', 'used by this stage group (Optional[dist.ProcessGroup]) ‚Äì the process group to be used by this stage Returns: a pipeline stage that can run with PipelineSchedules. Return type:\n_PipelineStage\nPipeline Schedules\nclass torch.distributed.pipelining.schedules.', 'a certain submodule in the forward function. torch.distributed.pipelining. pipeline (module, mb_args, mb_kwargs =None, split_spec =None, split_policy =None ) Split a module based on a specification. See Pipe for more details.', 'stages per rank. Uses the backward for weights to fill in the pipeline bubble. In particular this is implementing the ZB1P schedule in the paper. class\ntorch.distributed.pipelining.schedules.', 'The above scenarios share a commonality that the computation per device cannot hide the communication of conventional parallelism, for example, the weight all-gather of FSDP. What is torch.distributed.pipelining ?', 'certain submodule in the forward function. :ivar END: Represents adding a split point after the execution of a certain submodule in the forward function. torch.distributed.pipelining. pipeline (module, mb_args, mb_kwargs =None,\nsplit_spec =None, split_policy =None )\nSplit a module based on a specification.']",0.5,0.3,0.37499999999999994,1,0.16666666666666666,0.5,0.25,0.32455958071630964,0.2012012012012012,0.45348837209302323,10,0.07882878311156755,0.0,0.0,0.0,0.0
What optimizations does FlashAttention introduce compared to standard attention kernels implemented in PyTorch?,multi,"('FMHA targets BERT models, it only supports head dimension 64, and only runs on A100 GPUs. FMHA fuses the attention computation dropout¬πsoftmax¬πmask¬πQK>¬∫¬∫¬∫Vinto one CUDA kernel. In the forward pass, it stores the attention matrix softmax¬πmask¬πQKùëá¬∫¬∫to HBM to be used in gradient computation. As a result, it does not oÔ¨Äer substantial memory saving (though for shorter sequences memory footprint is often not a primary concern).', 'This module contains functions and classes that alter the behavior of torch.nn.functional.scaled_dot_product_attention', 'flex_attention This module implements the user facing API for flex_attention in PyTorch. bias Defines bias subclasses that work with scaled_dot_product_attention')",3,1,"['implementation and experimental setting in Tay et al. [80]and Xiong et al. [90].3Table 3 shows that FlashAt- tention achieves up 2.4\x02speed-up compared to standard attention. Block-sparse FlashAttention is\nfaster than all of the approximate attention methods that we have tested.', 'forward + backward pass of FlashAt- tention and block-sparse FlashAttention compared to the baselines in exact, approximate, and sparse attention (exact numbers in Appendix E). Runtime grows quadratically with sequence length, but FlashAt-\ntention runs signiÔ¨Åcantly faster than exact attention baselines, up to 3 \x02faster than the PyTorch\nimplementation.', 'that FlashAt- tention achieves up 2.4\x02speed-up compared to standard attention. Block-sparse FlashAttention is faster than all of the approximate attention methods that we have tested. Table 3: The performance of standard attention, FlashAttention , block-sparse FlashAttention , and approximate\nattention baselines on the Long-Range-Arena benchmarks.', 'arrows), loading them to SRAM, and writing the output of the attention computation back to HBM. Right:Speedup over the PyTorch implementation of attention on GPT-2. FlashAttention does not read and write the large ùëÅ\x02ùëÅattention matrix to HBM, resulting in an 7.6 \x02\nspeedup on the attention computation.', 'E). Runtime grows quadratically with sequence length, but FlashAt- tention runs signiÔ¨Åcantly faster than exact attention baselines, up to 3 \x02faster than the PyTorch implementation. The runtimes of many approximate/sparse attention mechanisms grow linearly with se-\nquence length, but FlashAttention still runs faster than approximate and sparse attention for short\nsequences due to fewer memory accesses.', 'Attention. FlashAttention is up to 3\x02faster than the standard attention implemen- tation across common sequence lengths from 128 to 2K and scales up to 64K. Up to sequence length of 512,\nFlashAttention is both faster and more memory-eÔ¨Écient than any existing attention method, whereas\nfor sequence length beyond 1K, some approximate attention methods (e.g., Linformer) start to become\nfaster.', '4], and more [ 40,85]. However, common Python interfaces to deep learning such as PyTorch and TensorÔ¨Çow do not allow Ô¨Åne-grained control of memory access. We propose FlashAttention , a new attention algorithm that computes exact attention with far fewer\nmemory accesses.', 'show that FlashAttention can serve as a useful primitive for realizing the potential of approximate attention algorithms by overcoming their issues with memory access overhead. As a proof of\nconcept, we implement block-sparse FlashAttention , a sparse attention algorithm that is 2-4 \x02faster than\nevenFlashAttention , scaling up to sequence length of 64k.', 'On the other hand, block-sparse FlashAttention is faster than all implementations of exact, sparse, and approximate attention that we know of, across all sequence lengths. Memory Footprint.', 'of FlashAttention and block-sparse FlashAttention compared to prior attention implementations. ‚Ä¢Faster Model Training. FlashAttention trains Transformer models faster in wall-clock time. We train BERT-large (seq. length 512) 15% faster than the training speed record in MLPerf 1.1 [ 58], GPT2\n(seq.']",0.0,0.0,0.0,0,0.06666666666666667,0.2857142857142857,0.10810810810810811,0.34082091191293007,0.27040816326530615,0.43386243386243384,10,4.9240727178564545e-05,0.0,0.0,0.0,0.0
What problem does the Model Context Protocol (MCP) solve?,single,MCP (Model Context Protocol) Connecting (N) LLMs to (M) external tools/resources used to be a NxM problem MCP standardizes the LLM-tool communication into a N->1->M process Build with a client-server model MCP client: the agent that needs to call tool/data MCP server: a service to expose external tools and data sources,1,7,"['Models . https://openreview.net/ forum?id=hHoK1kBPd9 [21] Dong Liu, Yanxuan Yu, Xuhong Wang, Ben Lengerich, and Ying Nian Wu. 2025. MKA: Memory-Keyed Attention for Efficient Long-Context Reasoning. In ICML\n2025 Workshop on Long-Context Foundation Models .', 'and few-shot works by giving Kexamples of context and completion, and then one Ô¨Ånal example of context, with the model expected to provide the completion. We\ntypically set Kin the range of 10 to 100 as this is how many examples can Ô¨Åt in the model‚Äôs context window\n(nctx= 2048 ).', 'of injecting new knowledge into the LLM should be minimized. In-context learning puts the new knowledge in the model‚Äôs input, rather than the model itself. The separation of knowl-\nedge and model serves as the key to modularity‚ÄîLLM service\nproviders can specify which knowledge to use and easily\ncompose different pieces of knowledge, which helps the LLM\nto avoid conflicting knowledge and improve the generation\nquality.', 'anatomy of MCMC-based maximum likelihood learning of energy-based models. arXiv preprint arXiv:1903.12370 , 2019. [42] Erik Nijkamp, Mitch Hill, Song-Chun Zhu, and Ying Nian Wu. Learning non-convergent non-persistent\nshort-run MCMC toward energy-based model.', '128K context, the model can retrieve the full context with 8 dense layers but fails to do so with only 2 and 4 dense layers. The results validate the long context capability of HHST design.', 'parallel folding up to 16x nodes with little MFU drops, especially for large-scale models like Llama3-8x70B, where the MFU only drops from 43.7% to 41.5%. Scaling with Context Length To evaluate the capability of our framework to train large scale\nMoE models with very long context lengths, we conducted context scaling experiments by increasing\nthe sequence length while keeping the total number of tokens per global batch constant.', 'Workloads such as GQA mechanism [ 2], that is commonly used in infer- ence models. The original MHA mechanism that is described in Section 2.4. GQA Extension:In this work, we extend SP for GQA\nmechanism for adapting SP to a diverse set of LLMs that\nare used in inference.', 'Nichol. VQ-DRAW: A sequential discrete V AE. arXiv preprint arXiv:2003.01599 , 2020. [41] Erik Nijkamp, Mitch Hill, Tian Han, Song-Chun Zhu, and Ying Nian Wu. On the anatomy of MCMC-based\nmaximum likelihood learning of energy-based models.', 'very long context lengths, we conducted context scaling experiments by increasing the sequence length while keeping the total number of tokens per global batch constant. As shown\nin Figure 4, our framework can train MoE models with high efficiency up to a context length of\n128K tokens, and the MFU only drops from 38.7% to 35.9% for Qwen-57B14A and 47.6% to 42.9%\nfor Mixtral-8x22B.', 'specify which knowledge to use and compose them easily. Second, the overhead ( e.g.,time, cost) of injecting new knowledge into the LLM should be minimized. In-context learning puts the new knowledge in the model‚Äôs\ninput, rather than the model itself.']",0.0,0.0,0.0,0,0.0,0.0,0.0,0.22541973386785424,0.1297071129707113,0.2857142857142857,10,8.044093791008557e-10,0.0,0.0,0.0,0.0
What search algorithm does AlphaZero use instead of alpha-beta search?,single,"Instead of an alpha-beta search with domain-speciÔ¨Åc enhancements, AlphaZero uses a general-purpose Monte-Carlo tree search (MCTS) algorithm. Each search consists of a series of simulated games of self-play that traverse a tree from root srootto leaf. Each simulation proceeds by selecting in each state sa moveawith low visit count, high move probability and high value (averaged over the leaf states of simulations that selected afroms) according to the current neural network.",1,1,"['are then used to guide its search. Instead of an alpha-beta search with domain-speciÔ¨Åc enhancements, AlphaZero uses a general- purpose Monte-Carlo tree search (MCTS) algorithm. Each search consists of a series of simu-\nlated games of self-play that traverse a tree from root srootto leaf.', ').AlphaZero uses a markedly different approach that averages over the position evalu- ations within a subtree, rather than computing the minimax evaluation of that subtree. How-\never, chess programs using traditional MCTS were much weaker than alpha-beta search pro-\ngrams, ( 4, 24 ); while alpha-beta programs based on neural networks have previously been un-\nable to compete with faster, handcrafted evaluation functions.', 'several decades. State-of- the-art programs are based on powerful engines that search many millions of positions, leverag- ing handcrafted domain expertise and sophisticated domain adaptations. AlphaZero is a generic\nreinforcement learning algorithm ‚Äì originally devised for the game of Go ‚Äì that achieved su-\nperior results within a few hours, searching a thousand times fewer positions, given no domain\n3The prevalence of draws in high-level chess tends to compress the Elo scale, compared to shogi or Go.', 'evaluating a large subtree. In contrast, alpha-beta search computes an explicit mini- max, which propagates the biggest approximation errors to the root of the subtree. Using MCTS\nmay allow AlphaZero to effectively combine its neural network representations with a powerful,\ndomain-independent search.', 'expected outcome zfrom position s, v\x19E[zjs].AlphaZero learns these move probabilities and value estimates entirely from self- play; these are then used to guide its search. Instead of an alpha-beta search with domain-speciÔ¨Åc enhancements, AlphaZero uses a general-\npurpose Monte-Carlo tree search (MCTS) algorithm.', 'eight games to Elmo (see Supplementary Ma- terial for several example games), as well as defeating the previous version of AlphaGo Zero (see Table 1). We also analysed the relative performance of AlphaZero ‚Äôs MCTS search compared to the\nstate-of-the-art alpha-beta search engines used by StockÔ¨Åsh andElmo .AlphaZero searches just\n80 thousand positions per second in chess and 40 thousand in shogi, compared to 70 million\nforStockÔ¨Åsh and 35 million for Elmo .AlphaZero compensates for the lower number of evalu-\nations by using its deep neural network to focus much more selectively on the most promising\nvariations ‚Äì arguably a more ‚Äúhuman-like‚Äù approach to search, as originally proposed by Shan-\nnon ( 27).', 'this new player. In contrast, AlphaZero simply maintains a sin- gle neural network that is updated continually, rather than waiting for an iteration to complete. 3Figure 1: Training AlphaZero for 700,000 steps.', 'The search returns a vector \x19representing a probability distribution over moves, either proportionally or greedily with respect to the visit counts at the root state. The parameters \x12of the deep neural network in AlphaZero are trained by self-play reinforce-\nment learning, starting from randomly initialised parameters \x12.', 'programs such as Deep Blue, have used very similar architectures ( 9,23) including the majority of the components described above, although 10important details vary considerably. None of the techniques described in this section are used by AlphaZero .', 'the majority of the components described above, although 10important details vary considerably. None of the techniques described in this section are used by AlphaZero . It is likely that\nsome of these techniques could further improve the performance of AlphaZero ; however, we\nhave focused on a pure self-play reinforcement learning approach and leave these extensions\nfor future research.']",1.0,0.3,0.4615384615384615,1,0.12,0.6666666666666666,0.20338983050847456,0.33005047878308147,0.1394422310756972,0.8680555555555556,10,0.05999710742592204,1.0,1.0,0.3333333333333333,1.0
Why can cProfile and the CPU-mode autograd profiler give incorrect timings on CUDA code?,single,"Due to the asynchronous nature of CUDA kernels, when running against CUDA code, the cProfile output and CPU-mode autograd profilers may not show correct timings: the reported CPU time includes only the time to launch kernels but not their GPU execution time unless synchronization occurs.",1,3,"['are profiling CUDA code, the first profiler that bottleneck runs (cProfile) will include the CUDA startup time (CUDA buffer allocation cost) in its time reporting. This\nshould not matter if your bottlenecks result in code much slower than the CUDA startup\ntime.', 'is very high and often gives a heavily skewed timeline. Similarly, Intel¬Æ VTune‚Ñ¢ Profiler helps to analyze performance on Intel platforms further with torch.autograd.profiler.emit_itt() . If you are profiling CUDA code, the first profiler that bottleneck  runs (cProfile) will\ninclude the CUDA startup time (CUDA buffer allocation cost) in its time reporting.', 'python -m torch.utils.bottleneck -h for more usage instructions. Because your script will be profiled, please ensure that it exits in a finite amount of time. Due to the asynchronous nature of CUDA kernels, when running against CUDA code, the\ncProfile output and CPU-mode autograd profilers may not show correct timings: the\nreported CPU time reports the amount of time used to launch the kernels but does not\ninclude the time the kernel spent executing on a GPU unless the operation does a\nsynchronize.', 'autograd profiler output to look at, you should first check if your script is CPU-bound (‚ÄúCPU total time is much greater than CUDA total time‚Äù). If it is CPU-bound, looking at the results of the CPU-mode autograd profiler\nwill help.', '(‚ÄúCPU total time is much greater than CUDA total time‚Äù). If it is CPU-bound, looking at the results of the CPU-mode autograd profiler will help. If on the other hand your script spends most of its time executing on the GPU,\nthen it makes sense to start looking for responsible CUDA operators in the output of the\nCUDA-mode autograd profiler.', 'its time executing on the GPU, then it makes sense to start looking for responsible CUDA operators in the output of the CUDA-mode autograd profiler. Of course the reality is much more complicated and your script might not be in one of\nthose two extremes depending on the part of the model you ºre evaluating.', '(CUDA buffer allocation cost) in its time reporting. This should not matter if your bottlenecks result in code much slower than the CUDA startup time. For more complicated uses of the profilers (like in a multi-GPU case), please see\nhttps://docs.python.org/3/library/profile.html or torch.autograd.profiler.profile()  for more\ninformation.', 'than the CUDA startup time. For more complicated uses of the profilers (like in a multi-GPU case), please see https://docs.python.org/3/library/profile.html or torch.autograd.profiler.profile() for more information. Previous NextRate this Page‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ\nSend FeedbackNote \uf05a\nWarning ‚ö†\nDocs\nAccess comprehensive\ndeveloper documentationTutorials\nGet in-depth tutorials for\nbeginners and advancedResources\nFind development\nresources and get yourTo analyze traffic and optimize your experience, we serve cookies on this site.', 'takes place, including the module initialization if needed and the parameter sharding. This should be spe to improve initialization speed if module is on CPU. If the default CUDA device was set (e.g.', 'in the CPU See the cufile api documentation for more details. These APIs can be used in versions greater than or equal to CUDA 12.6. In order to use these APIs,\none must ensure that their system is appropriately configured to use GPUDirect Storage per the\nGPUDirect Storage documentation.']",1.0,0.2,0.33333333333333337,1,0.1111111111111111,1.0,0.19999999999999998,0.2942794077994074,0.13592233009708737,0.510934393638171,10,0.05879699804713054,0.0,0.0,0.3333333333333333,1.0
Why can‚Äôt you perform data-dependent operations on meta tensors?,single,"Because meta tensors do not have real data, you cannot perform data-dependent operations like torch.nonzero()  or item() . In some cases, not all device types (e.g., CPU and CUDA) have exactly the same output metadata for an operation",1,4,"['performed on meta tensors, producing new meta tensors that describe what the result would have been if you performed the operation on a real tensor. You can use\nthis to perform abstract analysis without needing to spend time on compute or space to\nrepresent the actual tensors.', 'a real tensor. You can use this to perform abstract analysis without needing to spend time on compute or space to represent the actual tensors. Because meta tensors do not have real data, you cannot perform\ndata-dependent operations like torch.nonzero()  or item() .', 'or space to represent the actual tensors. Because meta tensors do not have real data, you cannot perform data-dependent operations like torch.nonzero() or item() . In some cases, not all device\ntypes (e.g., CPU and CUDA) have exactly the same output metadata for an operation; we\ntypically prefer representing the CUDA behavior faithfully in this situation.', 'loading the actual parameters into memory. This can be helpful if you need to make transformations on the model before you load the actual data. Most operations can be performed on meta tensors, producing new meta tensors that describe\nwhat the result would have been if you performed the operation on a real tensor.', 'Last Updated On: Jun 17, 2025 The ‚Äúmeta‚Äù device is an abstract device which denotes a tensor which records only metadata, but no actual data. Meta tensors have two primary use cases:\nModels can be loaded on the meta device, allowing you to load a representation of the model\nwithout actually loading the actual parameters into memory.', 'types (e.g., CPU and CUDA) have exactly the same output metadata for an operation; we typically prefer representing the CUDA behavior faithfully in this situation. Although in principle meta tensor computation should always be faster than an equivalent\nCPU/CUDA computation, many meta tensor implementations are implemented in Python\nand have not been ported to C++ for speed, so you may find that you get lower absolute\nframework latency with small CPU tensors.', ""expected to explicitly reinitialize the parameters manually: torch._subclasses.meta_utils contains undocumented utilities for taking an arbitrary Tensor and constructing an equivalent meta Tensor with high fidelity. These APIs are experimental and may>>> with torch.device('meta'):\n...     print(torch.randn(30, 30))\n...\ntensor(..., device='meta', size=(30, 30))\n>>> from torch.nn.modules  import Linear\n>>> with torch.device('meta'):\n...     print(Linear(20, 30))\n..."", 'are different communication patterns for different partitioned operators, depending on the semantics, e.g., whether it needs to accumulate partial results, or to rearrange data shards. According to our experience, manually handling these issues in the model\nrequires substantial amount of effort, given the fact that the frameworks like TensorFlow have a\nlarge sets of operators with ad-hoc semantics.', 'n-dimensional arrays featur- ing a rich set of data manipulation operations. Every Tensor object has an associated storage that is allocated on a specific device. When Tensor s only represent simple transformations such as reshape\nand split , they can share the same underlying storage.', 'efÔ¨Åcient pipelining of these communication and computation operations, and suffers from pipeline bubbles that reduce efÔ¨Åciency, or changes to the optimizer itself which impact accuracy. Distributed tensor computation is an orthogonal and more\ngeneral approach that partitions a tensor operation across\nmultiple devices to accelerate computation or increase\nmodel size.']",1.0,0.3,0.4615384615384615,1,0.18518518518518517,1.0,0.3125,0.2533595479592985,0.10054347826086957,0.6526610644257703,10,0.06765007629329677,0.0,0.0,0.6666666666666666,1.0
