query,split,raw_gold-text,num_gold_texts,num_retrieved,retrieved_docs,text_recall,text_precision,text_f1,num_gold_texts_matched,ctx_precision,ctx_recall,ctx_f1,chunk_overlap,min_chunk_overlap,max_chunk_overlap,chunk_overlap_count,bleu,text_supported@1,text_recall@1,text_supported@3,text_recall@3
Explain the importance of ImageNet in the works alexnet and googlenet.,multi,"('We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 dif- ferent classes.', 'We propose a deep convolutional neural network architecture codenamed Incep- tion, which was responsible for setting the new state of the art for classiÔ¨Åcation and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC14).')",2,3,"['and then use the same scaling coefÔ¨Åcients for all other models (step 2). 5. Experiments In this section, we will Ô¨Årst evaluate our scaling method on existing ConvNets and the new proposed EfÔ¨ÅcientNets. 5.1. Scaling Up MobileNets and ResNets As a proof of concept, we Ô¨Årst apply our scaling method to the widely-used MobileNets (Howard et al., 2017; San- dler et al., 2018) and ResNet (He et al., 2016). Table 3 shows the ImageNet results of scaling them in different ways. Compared to other single-dimension scaling methods, our compound scaling method improves the accuracy on all these models, suggesting the', 'in the challenge with no external data used for training. In addition to the training techniques aforementioned in this paper, we adopted a set of techniques during testing to obtain a higher performance, which we elaborate below. 1. We independently trained 7 versions of the same GoogLeNet model (including one wider version), and performed ensemble prediction with them. These models were trained with the same initialization (even with the same initial weights, mainly because of an oversight) and learning rate policies, and they only differ in sampling methodologies and the random order in which they see input images. 2. During', 'better than the constructed solution (or unable to do so in feasible time). In this paper, we address the degradation problem by introducing a deep residual learning framework. In- stead of hoping each few stacked layers directly Ô¨Åt a desired underlying mapping, we explicitly let these lay- ers Ô¨Åt a residual mapping. Formally, denoting the desired underlying mapping as H(x), we let the stacked nonlinear layers Ô¨Åt another mapping of F(x) :=H(x)\x00x. The orig- inal mapping is recast into F(x)+x. We hypothesize that it is easier to optimize the residual mapping than to optimize the original, unreferenced mapping. To the']",0.0,0.0,0.0,0,0.047619047619047616,0.5,0.08695652173913042,0.1357273036300329,0.06141732283464567,0.17363344051446947,3,3.5423071371884934e-07,0.0,0.0,0.0,0.0
How does auto-differentiation work in these frameworks?,single,"TensorFlow is an interface for expressing machine learning algorithms and an implementation for executing such algorithms ‚Ä¢PyTorch  is a programming framework for tensor computation, deep learning, and auto differentiation 6Deep Learning Programming Framework7Aspect PyTorch TensorFlow JAX NumPy Primary Use Deep learning Deep learning numerical and ML computing numerical computing Programming Paradigm Dynamic (eager execution)Static (Graph mode, or Eager)Functional transformations Procedural Auto grad dynamic comp graphstatic comp graph Functional -based with grad/jit",1,3,"['operations are implemented in low- latency C++ code underneath a thin Python interface, we have maintained low overhead while ensuring compatibility with Python-based DL frameworks. The performance improvements inherent in mixing commu- nication backends are consistent with the Ô¨Åndings of previous NCCL and MPI studies [37] and studies exploring the mixture of MPI with external runtimes in [20]. VIII. C ONCLUSION State-of-the-art deep learning (DL) models are pushing the boundaries of existing Ô¨Åelds while pioneering entirely new areas of study. However, such DL models are often impossible or impractical to train on single processors or small-scale workstations. Further work in', '-level details (no need to write cuda ) oautomatic differentiation (no need to derive gradient calculation manually) ‚Ä¢efficient in large -scale training and inference oautomatically scale to data and model size oautomatic hardware acceleration5Deep Learning Frameworks (also for LLMs)‚Ä¢Formulate machine learning computation using data flow graphs (data moving around a computation graph) ‚Ä¢TensorFlow is an interface for expressing machine learning algorithms and an implementation for executing such algorithms ‚Ä¢PyTorch is a programming framework for tensor computation, deep learning, and auto differentiation 6Deep Learning Programming Framework7Aspect PyTorch TensorFlow JAX NumPy Primary Use Deep learning Deep learningnumerical and ML computingnumerical computing Programming', '3Warp 4SM partition1‚Ä¢Basic CUDA operations omemory allocation odata movement ocreating threads and running on SMs ‚ñ™specifying number of threads and number of blocks in a grid oreferring to data in GPU memory within a thread ‚ñ™using building index variables to refer to the data ‚Ä¢Implementing parallel matrix operations on GPUs opartition the data and computation and assign to threads 3Recap‚Ä¢Learning algorithm for Neural Network ‚Ä¢Computation Graph ‚Ä¢Auto Differentiation 4Today‚Äôs Topic ‚Ä¢Neural network layers oEmbedding (lookup table) oLinear oRelu oAverage pooling oSoftmax 5A Simple Feedforward Neural Network LinearReluLinearSoftmax Embedding ‚ÄúIt is a good movie‚ÄùAvg‚Ä¢ùë•ùëõ,ùë¶ùëõ are data and label pairs for training']",1.0,0.3333333333333333,0.5,1,0.125,1.0,0.2222222222222222,0.31772809292028686,0.21067415730337077,0.5012820512820513,3,0.1420161590837385,0.0,0.0,0.3333333333333333,1.0
"What are FlashMLA, DeepEP, and DeepGEMM, and what problems do they each solve?",single,"Accelerating Transformer Layers‚Ä¢FlashMLA  (released 2/24/2025) FlashMLA  is an efficient MLA decoding kernel for Hopper GPUs, optimized for variable -length sequences serving. DeepEP  (released 2/25/2025) oa communication library tailored for Mixture -of-Experts ( MoE) and expert parallelism (EP). It provides high -throughput and low -latency all -to-all GPU kernels, which are also as known as MoE dispatch and combine. DeepGEMM  (released 2/26/2025) oDeepGEMM  is a library designed for clean and efficient FP8 General Matrix Multiplications (GEMMs) with fine -grained scaling 3Deepseek  opensource libraries",1,3,"['Operators that can be reused in Other Networks: Dropout, LayerNorm , Softmax , Cross Entropy hierachical auto-regressive search for large vocabulary converting sorting to parallel operations (max, filter, re -rank)Accelerating decodingRecap: Accelerating Transformer Layers‚Ä¢FlashMLA (released 2/24/2025) oFlashMLA is an efficient MLA decoding kernel for Hopper GPUs, optimized for variable -length sequences serving. ‚Ä¢DeepEP (released 2/25/2025) oa communication library tailored for Mixture -of-Experts ( MoE) and expert parallelism (EP). It provides high -throughput and low -latency all -to-all GPU kernels, which are also as known as MoE dispatch and combine. ‚Ä¢DeepGEMM (released 2/26/2025) oDeepGEMM is a library designed for clean and', 'parameters in Mixtral 8x7B model? ‚Ä¢56B? ‚Ä¢47B! osince only FFN layers act are experts, the other parameters (attention, embedding) are shared 11Parameters of MoEDanger of MoE over-fitting to small data 12 ‚Ä¢Encoder experts tend to specialize in token groups or shallow concepts (e.g., punctuation, proper nouns). ‚Ä¢Decoder experts exhibit less specialization. ‚Ä¢In multilingual setups, experts do not specialize in specific languages due to token routing and load balancing.What does an Expert network learn? 13‚Ä¢KeepTop1 with 3 routing experts (finding linear boundaries among expert centroids) 14Geometric Interpretation of Expert Routing‚Ä¢Transformer Mixture -of-Expert Model oSwitch Transformer architecture oShared -routed Experts ‚Ä¢Training and', 'task. At the concert, three agents are collaborating to perform in a band. Outdoors, two agents are discussing lantern-making, planning the required materials, and finances by selecting and using tools. Users can participate in any of these stages of this social activityThe use cases for LLM agents, or Language Model-based agents, are vast and diverse. These agents, powered by large language models (LLMs), can be used in various scenarios, including:1.Single-agent applications2.Multi-agent systems3.Human-Agent cooperationCategory https://gptpluginz.com/llm-agents/LLM agents can be utilized as personal assistants to assist users in breaking free from daily tasks and repetitive labor. They can analyze, plan, and solve problems']",1.0,0.3333333333333333,0.5,1,0.23529411764705882,1.0,0.38095238095238093,0.3288824422795987,0.08748317631224764,0.668918918918919,3,0.19984889314057436,1.0,1.0,0.3333333333333333,1.0
What are the challenges of theoretical distributed training (PipeDream) and what is the scaling efficiency of distributed real-world Trainium performance (HLAT)?,multi,"('Challenge 1: Stage Partitioning‚Ä¢How to partition model layers into the stages evenly? ‚Ä¢Throughput depends on the slowest stage in pipeline ‚Ä¢Solution: ‚Ä¢ProÔ¨Åle layer-wise perf and comm perf ‚Ä¢Allows a stage to be replicated (DP) ‚Ä¢Uses dynamic programming to Ô¨Ånd optimal partition and layer replication', 'Challenge 2: Work Scheduling‚Ä¢How to schedule forward and backward computation on a worker? ‚Ä¢Solution: 1F1B-RR ‚Ä¢Run one forward and one backward ‚Ä¢Round-robin across replicated stages', 'Challenge 3: Weight Versioning‚Ä¢How to ensure the same minibatch uses the same weight version across workers for forward and backward? ‚Ä¢Otherwise computation will be far oÔ¨Ä and training not able to converge ‚Ä¢Solution: Store multiple weight versions so that the backward and forward of the same minibatch ‚Ä¢Weights across workers can be diÔ¨Äerent!', 'The scaling efficiency for Llama2 7b:‚Ä¢87% on 32 nodes. MFU = 33.5%‚Ä¢72% on 64 nodes. MFU = 27.9%¬© 2025, Amazon Web Services, Inc. or its affiliates. All rights reserved. Amazon Confidential and Trademark. Observations on training precision for 70B (preliminary)')",4,3,"['All rights reserved. Amazon Confidential and Trademark. Intermediate Performance, cont‚Äôd 18‚Ä¢For Commonsense Reasoning, the model accuracy improves quickly at beginning of training, and starts to saturate at later training stages. This is similar as the trends observed in other LLM model trainings‚Ä¢For Math task (GSM8K), the learning curve shows an exponentially increasing trend. It increase very gradually for the initial ~1 trillion tokens and begins to improve significantly during the later stages of training‚Ä¢For World Knowledge task, the performance increases almost linearly with number of training tokens‚Ä¢May related to close-book testing problem‚Ä¢Those observations indicate the necessity of a set of', 'review the basics of GPU memory and execution hierarchy, and then introduce our Merge-Q technique, which significantly improves the kernel‚Äôs efficiency while allowing more fine-grained customization of the sparse attention. 3.1 P RELIMINARIES GPU threads have access to a hierarchy of different types of memory. Global high-bandwidth memory (HBM ) is the slowest but largest (roughly >100√óin latency and ‚àº6K√óin size). Shared memory (SRAM ) is physically on chip, thus has larger bandwidth and lower latency compared to HBM. Optimizing the computation of the SRAM and minimizing the IO between HBM and SRAM are crucial for improving the efficiency of', 'and then flattens and shards all of the parameters within each unit. The sharded parameters are communicated and recovered on-demand before computations, and then they are im- mediately discarded afterwards. This approach ensures that FSDP only needs to materialize parameters from one unit at a time, which significantly reduces peak memory consumption. The design and implementation of FSDP faces the following challenges. ‚Ä¢User Experience is critical for achieving broad adoption. When working on prior PyTorch distributed training fea- tures such as DistributeDataParallel (DDP) [ 14], we observed that aligning the user experience of distributed training with that of local training']",0.25,0.0,0.0,1,0.125,0.15384615384615385,0.13793103448275862,0.1546036221682112,0.13828238719068414,0.1633281972265023,3,0.031680055107902144,0.0,0.25,0.0,0.25
What are the three core components of the TinyServe system?,single,"The system is organized around three core components: (1)Query-Aware KV Retriever: Dynamically selects relevant key-value blocks at decode time based on the current query vector and page-level metadata, reducing unnecessary mem- ory access. (2)Modular Scheduling Pipeline: A dispatch loop handles incoming queries and routes them through configurable plug- ins (e.g., entropy-based early exit, token-level pruning, ap- proximate attention). This modular design allows experimen- tation with different sparsity strategies without modifying the core model. (3)Sparse Attention Executor: Efficiently computes attention over selected KV pages using fused CUDA kernels, with support for FP16/INT8 KV formats and multi-GPU dispatch.",1,3,"['training. These works emphasize that carefully designed synthetic stressors and caching strategies are essential for both graph-based and language-based workloads, re- inforcing the importance of lightweight analysis frameworks. 3 Methodology 3.1 System Overview: TinyServe TinyServe is a lightweight serving framework designed for serving tiny language models under tight memory and latency constraints. Rather than acting as a benchmarking tool, TinyServe serves as a real-time serving environment that enables sparsity-aware atten- tion, modular token selection, and efficient KV-cache reuse. The system is organized around three core components: (1)Query-Aware KV Retriever: Dynamically selects relevant key-value blocks at decode time based on the', 'only materializes unsharded parameters and gradients of one unit at a time, and otherwise, it keeps parameters and gradients sharded. Throughout the training loop, the optimizer states are kept sharded. The memory requirements for FSDP are proportional to the size of the sharded model plus the size of the largest fully-materialized FSDP unit. Figure 1 demonstrates the overall workflow using a simple six layer model. Suppose FSDP decomposes the model into three parts, namely, [layer0, layer3] ,[layer1, layer2] , and [layer4, layer5] . The decomposition behavior can be controlled by user-defined func- tions. FSDP then wraps each of these three', 'Language Model 2bridges corn0.6 0.02 ùëÉ(ùë•ùë°|ùë•<ùë°) Decoder [BOS] Pgh is a city ofPgh is a city ofTransformerGPT1GPT2GPT3GopherPALMGPT4 Nemotron LLaMA3 -8BLLaMA3.1 Qwen2DeepSeek -v3 001101001,00010,000 2016 2017 2018 2019 2020 2021 2022 2023 2024 2025size(B) 3Recap: Scaling of LLMs (the need for system optimization)‚Ä¢Programming model orelies on good abstraction ‚Ä¢Latency/Throughput oData movement oComputation ‚Ä¢Reliability ‚Ä¢Security 4Recap: Important Topics in LLM systems‚Ä¢Neural Network Layer and low -level operators ‚Ä¢Components of A GPU Server ‚Ä¢GPU Architecture ‚Ä¢Program Execution on GPU 5Outline Classifying the sentiment of online movie reviews. (Positive, negative, neutral) 6Text Classification Spider -Man is an almost -perfect extension of the experience of']",0.0,0.3333333333333333,0.0,0,0.07142857142857142,0.25,0.11111111111111112,0.18680819082215905,0.1694915254237288,0.2074468085106383,3,0.07640250495533615,1.0,0.0,0.3333333333333333,0.0
What are the trade-offs between simple post-training quantization and GPTQ?,multi,"('8CUDA APIs for Half Precision‚Ä¢Using lower precision oconverting parameters from FP32 to INT8 or INT4 operform all computation in lower prevision. Reduce model accuracy: oLoss of Precision ‚ûî accumulate quantization noise oRange mismatch ‚ûî values are clipped and lead to information loss oQuantization error ‚ûî rounding errors 9Direct Quantization Approach‚Ä¢Absmax  quant Zero -point quant 10Quantize a number', 'mixed strategy for 8 -bit quantization and 16 -bit (for outliers)2Recap of Basic Quantization Methods‚Ä¢Absmax  quant Zero -point quant 4Quantize a Number to Int8 5GPTQ scale to GPT -size LLMs maintain accuracy‚Ä¢Revisit  layer -wise quantization of weight matrices argmin', 'layer -wise quantization + compensation for errors + precompute oaccurately compress some of the largest publicly -available models down to 3 and 4 bits, and bring end -to-end speedups')",3,3,"['compared with accurate -but-expensive methods? 29 Fastest prior method‚Ä¢https://github.com/qwopqwop200/GPTQ -for-LLaMa/ ‚Ä¢GPTQ in ohttps://github.com/qwopqwop200/GPTQ -for- LLaMa/blob/triton/gptq.py 31GPTQ for LLaMAGPTQ: Initialization 32 ‚óèReshape weights from the input layer ‚óèInitialize Hessian matrixGPTQ: Hessian Matrix Update 33‚óèUpdate Hessian matrix with information from a new batch of the input and output pairs GPTQ: Lazy Batch -Update 34 ‚óèProcesses weight matrix W in blocks. ‚óèUpdates quantization parameters conditionally based on group size and static grouping settings.GPTQ: Lazy Batch -Update 35 ‚óèApplies quantization function quantize to weights and computes the loss due to quantization. ‚óèAdjusts remaining block weights based on quantization error to minimize the overall error.GPTQ:', 'and round the inputs X_quant = torch .clip((X * scale + zeropoint ).round (), -128, 127) # Dequantize X_dequant = (X_quant - zeropoint ) / scale return X_quant .to(torch .int8), X_dequanthttps://colab.research.google.com/drive/1DPr4mUQ92Cc - xf4GgAaB6dFcFnWIvqYi?usp=sharing 12Direct Quantization Colab13Today‚Äôs Topic ‚Ä¢Low precision numbers in computer ‚Ä¢Basic Quantization Methods Model Quantization Approaches 14Quantization during training post trainingexpensive re -training / finetuning Model Quantization Approaches 15Quantization during training post trainingpreserve accuracy scale to large parametersBRECQ (Li et al., 2021)AdaQuant (Hubara et al., 2021) OBQ (Frantar et al., 2022) ZeroQuant (Yao et al., 2022) LLM.int8() (Dettmers et al., 2022)Model Quantization Approaches 16Quantization during training post trainingpreserve', 'obtain TTFT values in the end-to-end scenario, the attacker first clears both the victim‚Äôs and the attacker‚Äôs requests from the cache. Next, the attacker measures the TTFT for their own request after the victim‚Äôs system prompt has been cached. Below, we describe how the TTFT is gathered and how caches are flushed. ‚Ä¢Timing measurement . As shown in Figure 5, the attacker begins by issuing a synthesized request containing the targeted system prompt. Once the end-of-sequence token is received in the POST response, a short delay is introduced, ensuring the system prompt resides in the KV cache. The attacker then']",0.0,0.0,0.0,0,0.0,0.0,0.0,0.14053149191631473,0.1040650406504065,0.16165413533834586,3,3.23732730710566e-07,0.0,0.0,0.0,0.0
What does ‚ÄúIO-aware‚Äù mean in the context of FlashAttention?,single,"In this paper, we argue that a missing principle is making attention algorithms IO-aware [1]‚Äîthat is, carefully accounting for reads and writes to diÔ¨Äerent levels of fast and slow memory (e.g., between fast GPU on-chip SRAM and relatively slow GPU high bandwidth memory, or HBM [ 45], Figure 1 left).",1,3,"['we plot of the validation perplexity throughout training of GPT-2 small/medium, using either HuggingFace implementation or our FlashAttention implementation. We see that FlashAttention be- haves the same as the baseline implementation and the validation perplexity curves of the two implementations almost lie on top of each other. Long Document ClassiÔ¨Åcation. For MIMIC-III and ECtHR, we follow the hyperparameters of Dai et al. [13]. 26100k 200k 300k Training steps1015202530Val perplexityGPT-2-small HuggingFace GPT-2-small FlashAttention GPT-2-medium HuggingFace GPT-2-medium FlashAttentionFigure 4: Validation perplexity of GPT-2 small/medium using two implementations. We conÔ¨Årm that FlashAttention yields the same validation curves as the baseline implementation from HuggingFace.', 'the dense model. This observation works as an initial validation of the context understanding ability of the HHST design. We‚Äôll further validate it in the long context continual training section. We also found adding two dense layers generally leads to a significantly higher performance. Within theHomogeneous group, we can observe adding attention sink can significantly boost training quality, compared to only using the sliding window (SWA). In the Heterogeneous & Incomplete group, the vertical stride size is bigger than the number of attention heads, making the context incomplete after the union. For the Heterogeneous & Complete group, we tune the', 'linear model. This approximation is reasonable as the time complexity of the linear transformation is proportional to the sequence length of request ùëñ. The coefficientùúåin Eq. 6 can be determined before activating the serving pipeline, involving a marginal preprocessing cost of approximately 30 seconds in practice. In this way, the request manager can estimate the extra cost by any amount of hidden cache usage during serving. Additionally, the quantification model incorporates an SLO-aware fallback mechanism. For a given request ùëñin the inference iteration ùëí, its tracked pending time pùëí ùëñmay exceed the latency SLOs. Including such a request in the']",0.0,0.0,0.0,0,0.0,0.0,0.0,0.13854949379582507,0.10518292682926829,0.15756035578144853,3,2.0989457062731966e-07,0.0,0.0,0.0,0.0
"What is internal covariate shift, and how does it affect training?",single,"We deÔ¨Åne Internal Covariate Shift as the change in the distribution of network activations due to the change in network parameters during training. To improve the training, we seek to reduce the internal covariate shift. By Ô¨Åxing the distribution of the layer inputs xas the training progresses,we expect to improve the training speed.",1,3,"['of internal covariate shift on train- ing, and the ability of Batch Normalization to combat it, weconsideredtheproblemofpredictingthedigitclasson theMNISTdataset(LeCunetal.,1998a). Weusedavery simple network, with a 28x28binary image as input, and 510K20K30K40K50K0.70.80.91 Without BN With BN ‚àí202 ‚àí202 (a) (b)WithoutBN (c)With BN Figure 1: (a) The test accuracy of the MNIST network trained with and without Batch Normalization, vs. the number of training steps. Batch Normalization helps the network train faster and achieve higher accuracy. (b, c)The evolution of input distributions to a typical sig- moid,overthecourseoftraining,shownas {15,50,85}th percentiles. Batch Normalization makes the distribution morestableandreducestheinternalcovariateshift. 3fully-connectedhiddenlayerswith100activationseach. Eachhiddenlayercomputes y =g(Wu+b)withsigmoid nonlinearity, and the weights Winitialized', 'to not predict stride or pooling, it can design a 15-layer architecture that achieves 5.50% error rate on the test set. This architecture has a good balance between accuracy and depth. In fact, it is the shallowest and perhaps the most inexpensive architecture among the top performing networks in this table. This architecture is shown in Appendix A, Figure 7. A notable feature of this architecture is that it has many rectangular Ô¨Ålters and it prefers larger Ô¨Ålters at the top layers. Like residual networks (He et al., 2016a), the architecture also has many one-step skip connections. This architecture is', '2007 rather than a difference in mAP. A caveat of this procedure is that it does not consider run-to-run variability when training the models, only variability arising from using a Ô¨Ånite sample of images for evaluation. B.8.2. R ESULTS WITH STANDARD RESNET The ResNet-50 ( 4\x02) results shown in Table 8 of the text show no clear advantage to the supervised or self-supervised models. With the narrower ResNet-50 architecture, however, supervised learning maintains a clear advantage over self-supervised learning. The supervised ResNet-50 model outperforms the self-supervised model on all datasets with linear evaluation, and most (10 of 12) datasets with']",1.0,0.0,0.0,1,0.0,0.0,0.0,0.13587791650779366,0.11450381679389313,0.17077175697865354,3,6.38266330004009e-05,0.0,1.0,0.0,1.0
What is NVIDIA GPU Confidential Computing (CC) and how does it secure communication?,single,"NVIDIA CC ensures the confidentiality and integrity of communication between a CVM and a GPU via AES-GCM encryption [ 15]. A critical component of AES- GCM is the Initialization Vector (IV), a unique, non-repeating number (a nonce) required for each encryption session.",1,3,"['the GPU, while ‚Äúc‚Äù and ‚Äúd‚Äù denote ciphertexts moved from the GPU back to the CPU. After the transfers, the current IV of CPU and GPU is 3 and 7, respectively. read/write GPU memory and modify the control flow. Hard- ware GPU confidential computing has low performance over- head and is backward-compatible with existing applications. This paper focuses on studying hardware GPU confidential computing. A closer look at NVIDIA CC. Although CVMs encrypt their memory, this encryption is separate from that used by NVIDIA CC. NVIDIA CC ensures the confidentiality and integrity of communication between a CVM and a GPU', 'Web Services, Inc. or its affiliates. All rights reserved. Amazon Confidential and Trademark. Observations on training precision for 70B (preliminary) 37‚Ä¢Pure BF16 (without stochastic rounding) is usually bad (gradient can be unstable and explode)‚Ä¢This is observed for both 7B and 70B training. ‚Ä¢Using master weights usually has better accuracy with little throughput impact‚Ä¢But requires more memory, so may run into OOM issue‚Ä¢Always use Zero-1 with master weights‚Ä¢Using FP32 for certain operations can improve accuracy‚Ä¢E.g. gradient accumulation and all-reduce, softmax & cross-entropy calculation‚Ä¢Negatively impact throughput and memory footprint‚Ä¢Stochastic rounding may cause drift when parallelism dimension is large‚Ä¢Can be partially mitigated with', 'Megatron-LM, its ùëáùëêùëúùëöùëö consists all-reduce operations, which makes its communication2ùõΩ(ùëù‚àí1)ùëèùë†‚Ñé ùëù, and its isoefficiency function ùëä‚àºùëù3, whereùõΩdenotes time to transfer a scalar. For Optimus, itsùëáùëêùëúùëöùëö consists broadcast and reduce operations, which makes its communication time2ùõΩùëèùë†‚Ñé2ùëûlogùëù ùëù, and its isoefficiency function ùëä‚àº(‚àöùëùlogùëù)3. For Tesseract, its ùëáùëêùëúùëöùëö consists broadcast and reduce operations as well. Tesseract reduces communication between GPU significantly as well, which is a huge drawback for Cannon‚Äôs Algorithm and 2.5D matrix multiplication. With GPU amount ùëù, Canon‚Äôs Algorithm requires 2‚àóùëù3 2‚àí2‚àóùëù1 2times of information transfer between GPU for a single matrix multiplication operation, 2.5D algorithm requires 2‚àóùëù‚àí2‚àóùëù1 3amount of transmission.']",1.0,0.0,0.0,1,0.05,0.5,0.09090909090909091,0.14825415375295795,0.14756258234519104,0.14959349593495935,3,0.05086066869136299,0.0,1.0,0.0,1.0
What is the difference between torch.disttibuted and torch.distributed.pipelining?,multi,"('While promising for scaling, pipelining is often difficult to implement because it needs to partition the execution of a model in addition to model weights. The partitioning of execution often requires intrusive code changes to your model. Another aspect of complexity comes from scheduling micro-batches in a distributed environment, with data flow dependency considered.', 'The torch.distributed  package provides PyTorch support and communication primitives for multiprocess parallelism across several computation nodes running on one or more machines.')",2,3,"['performs runtime shape/dtype inference automatically by propagating the outputs from stage0 to stage1 and so forth, in linear order. To bypass shape inference, pass th input_args and output_args to each PipelineStage instance. Parameters: submodule (nn.Module) ‚Äì The PyTorch module wrapped by this stage. stage_index (int) ‚Äì The ID of this stage. num_stages (int) ‚Äì The total number of stages. device (torch.device) ‚Äì The device where this stage is located. input_args (Union[torch.Tensor, Tuple[torch.tensor]], optional) ‚Äì The input arguments for the submodule. output_args (Union[torch.Tensor, Tuple[torch.tensor]], optional) ‚Äì The output arguments for the submodule. group (dist.ProcessGroup, optional) ‚Äì The process group for distributed', 'Li ‚Ä¢Model Parallel Training ‚Ä¢Pipeline Parallelism ‚Ä¢Tensor Parallelism 2Today‚Äôs TopicModel Parallelism 3Motivation: The size of models increases exponentially fast and large. It is no longer possible to fit these large models into the memory of a single GPU. TransformerGPT1GPT2GPT3GopherPALMGPT4 LLaMA3.1DeepSeek -v3 Qwen3 001101001,00010,000 2016 2017 2018 2019 2020 2021 2022 2023 2024 2025size(B)Model Parallel Training 4computation (forward/backward/update) of a model is distributed across multiple workers. Distributed layer -wise computation Distributed tensor computation F0F1F2F3 B0B1B2B3loss device 3, layer 3 device 2, layer 2 device 1, layer 1 device 0, layer 0 grad updatePipeline Parallelism 5Na√Øve Model Parallel: The model is distributed', 'GPU idleness between adjacent operations (i.e., bubble time). The primary reason for bub- bles is the computation time for different operations is less than the PyTorch scheduling latency. To address this issue, we employ CudaGraph [43, 58] to eliminate the gap between adjacent operations, thereby reducing the overall computation time. However, commonly-used on-device random number generator (RNG) feature is incompatible with CudaGraph. As a workaround, we utilize a fixed seed instead of random numbers to mimic the behavior of the RNG. 5 Evaluation This section provides detailed evaluation and benchmark re- sults of Domino. We first discuss the model and']",0.0,0.0,0.0,0,0.0,0.0,0.0,0.11402871213873667,0.0620782726045884,0.16314199395770393,3,5.243201202622297e-05,0.0,0.0,0.0,0.0
What optimizations does FlashAttention introduce compared to standard attention kernels implemented in PyTorch?,multi,"('FMHA targets BERT models, it only supports head dimension 64, and only runs on A100 GPUs. FMHA fuses the attention computation dropout¬πsoftmax¬πmask¬πQK>¬∫¬∫¬∫Vinto one CUDA kernel. In the forward pass, it stores the attention matrix softmax¬πmask¬πQKùëá¬∫¬∫to HBM to be used in gradient computation. As a result, it does not oÔ¨Äer substantial memory saving (though for shorter sequences memory footprint is often not a primary concern).', 'This module contains functions and classes that alter the behavior of torch.nn.functional.scaled_dot_product_attention', 'flex_attention This module implements the user facing API for flex_attention in PyTorch. bias Defines bias subclasses that work with scaled_dot_product_attention')",3,3,"['up to 2.4√óthroughput increase in end-to-end serving scenarios compared to vLLM. Moreover, it saves up to 80% (57 GB) GPU memory usage compared to vLLM. 97.1 Methodology Setup. We conduct all experiments on a server with eight NVIDIA A100(80GB) GPUs and an Intel Xeon Platinum 8369B CPU. These GPUs are interconnected via NVLink. Evaluation Scenarios. We evaluate FLEXINFER across multiple self-attention computation kernels and end-to-end serving scenarios on different LLMs. For end-to-end serving, we focus on single-generation and prefix-caching scenarios containing multi-turn chatbot and prefix-sharing scenarios. For kernel implementations, we focused on a self-attention mechanism, including decoding and prefix-prefilling kernels.', 'introduce sample-dependent sparsity patterns, making it computationally expensive to determine eviction timing during decoding. In contrast, our approach uses a fixed sparsity pattern across all samples, eliminating the overhead of deciding which tokens to evict. Besides, KV eviction approaches are post-hoc and often perform much poorly as compared to the original dense counterpart (Ge et al., 2024a). Our HHST , as we will soon see in the experiments, adapts to the sparse attention during training (pre-training or post-training) performs comparably to dense baselines while reducing the training overhead. 5 E XPERIMENT To evaluate HHST , we first study the pre-training', 'dynamically adjusts KV cache budgets according to the attention weight distributions of requests, minimizing the KV cache usage. Compared to InfiniGen, PSA reduces the KV cache ratio for attention by 1.8x on average for LWM-Text-7B. This result may seem counterintuitive, given InfiniGen uses fine-grained token-level selection. However, InfiniGen uses a uniform minimal attention score threshold to selects critical tokens, which fails to account for the varying ranges of attention scores across requests. Moreover, InfiniGen adopts weight matrices compression and inter-layer prefetching to speed up the KV cache selection and loading process, but this inadvertently results in accuracy degradation in attention']",0.0,0.0,0.0,0,0.0,0.0,0.0,0.16634239366277773,0.11094674556213018,0.2046070460704607,3,2.9927904704350356e-07,0.0,0.0,0.0,0.0
What problem does the Model Context Protocol (MCP) solve?,single,MCP (Model Context Protocol) Connecting (N) LLMs to (M) external tools/resources used to be a NxM problem MCP standardizes the LLM-tool communication into a N->1->M process Build with a client-server model MCP client: the agent that needs to call tool/data MCP server: a service to expose external tools and data sources,1,3,"['of models to choose from and distributes the tasks to expert models. LLM.∆îTask Execution:Expert models execute on the specific tasks and log results.∆îResponse Generation:LLM receives the execution results and provides summarized results to users. LLMs + APIs to expert models: HuggingGPTTALM: Tool Augmented Language ModelsTALM: Tool Augmented Language ModelsLLMs + training for tool use: TALMToolformer: Language Models Can Teach Themselves to Use Tools LLMs + training for tool use: ToolformerMCP (Model Context Protocol)‚óèConnecting (N) LLMs to (M) external tools/resources used to be a NxM problem ‚óèMCP standardizes the LLM-tool communication into a N->1->M process ‚óèBuild with a client-server model ‚óèMCP', 'Understanding LLM PerformanceYiying ZhangTool use: ∆îThe agent learns to call external APIsfor extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.What is LLM Agentshttps://gptpluginz.com/llm-agents/ https://lilianweng.github.io/posts/2023-06-23-agent/ MCP (Model Context Protocol)‚óèConnecting (N) LLMs to (M) external tools/resources used to be a NxM problem ‚óèMCP standardizes the LLM-tool communication into a N->1->M process ‚óèBuild with a client-server model ‚óèMCP client: the agent that needs to call tool/data ‚óèMCP server: a service to expose external tools and data sources AI Agent/Workflow Frameworks‚óèFrameworks initially proposed to', 'an AI workÔ¨Çow: Autonomous and can ‚Äúmorph‚ÄùHere is a famous picture from Lilian Weng (from OpenAI)What is LLM Agentshttps://gptpluginz.com/llm-agents/ https://lilianweng.github.io/posts/2023-06-23-agent/∆îLanguage Mastery:Their inherent capability to both comprehend and produce language ensures seamless user interaction.∆îDecision-making:LLMs are equipped to reason and decide, making them adept at solving intricate issues.∆îFlexibility:Their adaptability ensures they can be molded for diverse applications.∆îCollaborative Interactions:They can collaborate with humans or other agents, paving the way for multifaceted interactions.Why LLM Agents stand out? Equipping Agents: The Power of Tooling‚Ä¢Tools: external functions, APIs, or even another (utility) agent (e.g., MCP server) ‚Ä¢Agents can often decide when to call tools and what']",1.0,0.6666666666666666,0.8,1,0.3333333333333333,1.0,0.5,0.2671609355590188,0.10602409638554217,0.40125786163522015,3,0.12109313004235886,1.0,1.0,0.6666666666666666,1.0
What search algorithm does AlphaZero use instead of alpha-beta search?,single,"Instead of an alpha-beta search with domain-speciÔ¨Åc enhancements, AlphaZero uses a general-purpose Monte-Carlo tree search (MCTS) algorithm. Each search consists of a series of simulated games of self-play that traverse a tree from root srootto leaf. Each simulation proceeds by selecting in each state sa moveawith low visit count, high move probability and high value (averaged over the leaf states of simulations that selected afroms) according to the current neural network.",1,3,"['reinforcement learning algorithm. Instead of a handcrafted evaluation function and move ordering heuristics, AlphaZero utilises a deep neural network (p;v) =f\x12(s)with parameters \x12. This neural network takes the board po- sitionsas an input and outputs a vector of move probabilities pwith components pa=Pr(ajs) 2for each action a, and a scalar value vestimating the expected outcome zfrom position s, v\x19E[zjs].AlphaZero learns these move probabilities and value estimates entirely from self- play; these are then used to guide its search. Instead of an alpha-beta search with domain-speciÔ¨Åc enhancements, AlphaZero uses a general- purpose Monte-Carlo tree search (MCTS) algorithm. Each search consists of', '500 cycle 500 cycle 1 cycle Loading data takes more time than actual computation!10CUDA Device Memory Model Variable declaration Memory Scope Lifetime int var; Register Thread Grid int varArr[N]; Local Thread Grid __device__ __shared__ int SharedVar ;Shared Block Grid __device__ int GlobalVar ;Global Grid Application __device__ __constant__ int constVar ;Constant Grid Application 11Access Device MemoryOpportunity to speedup: Reuse loaded data 12NNùê∂=ùê¥√óùêµ AB CN NN Nthreads in the thread block may use the same dataOpportunity to speedup: Reuse loaded data 13NNùê∂=ùê¥√óùêµ AB CN NN Nthreads in the thread block may use the same dataStep 1 (simultaneously) load the first tile of', 'values of the skewed query and key matrices, then add these two matrices together. This helps us calculate the sum of each column and perform top- koperation only once while accommodating the outlier columns of both query and key matrices. We then sum the elements in each column and select the top- kcolumns in the matrix; we choose 30% of the columns in our work. Using the sum of column values captures the global trend of each column while minimizing the effect of variance in each row. The selected columns better approximate the attention pattern because of the use of']",1.0,0.3333333333333333,0.5,1,0.18181818181818182,0.6666666666666666,0.28571428571428575,0.21034303629300546,0.17682020802377416,0.23295454545454544,3,0.06888448856670774,1.0,1.0,0.3333333333333333,1.0
Why can cProfile and the CPU-mode autograd profiler give incorrect timings on CUDA code?,single,"Due to the asynchronous nature of CUDA kernels, when running against CUDA code, the cProfile output and CPU-mode autograd profilers may not show correct timings: the reported CPU time includes only the time to launch kernels but not their GPU execution time unless synchronization occurs.",1,3,"['instructions. Because your script will be profiled, please ensure that it exits in a finite amount of time. Due to the asynchronous nature of CUDA kernels, when running against CUDA code, the cProfile output and CPU-mode autograd profilers may not show correct timings: the reported CPU time reports the amount of time used to launch the kernels but does not include the time the kernel spent executing on a GPU unless the operation does a synchronize. Ops that do synchronize appear to be extremely expensive under regular CPU-mode profilers. In these case where timings are incorrect, the CUDA-mode autograd profiler', 'will handle any potential autocast logic and redispatch down. The next feature in line will be autograd that will properly create the autograd graph and then redispatch down. Finally, we reach the backend kernel for CUDA which will launch the right CUDA kernel and return the final result. On the way out, autograd will attach the graph to the output and, finally, autocast will have a chance to do any update it needs on exit. One configuration of the dispatcher is the order in which all these feature and backend keys are called. The latest list and their order can', 'codeWhat can LLMs do?‚Ä¢Text Generation: Writing emails, articles, poems, and even code. ‚Ä¢Translation: Translating languages with impressive accuracy. ‚Ä¢Summarization: Condensing long documents into key points. ‚Ä¢Question Answering: Providing answers to a wide range of questions. ‚Ä¢Conversational AI: Powering chatbots and virtual assistants.What can‚Äôt LLM do (well)?‚Ä¢Hallucination: can generate seemingly correct but incorrect information. ‚Ä¢Bias: can reÔ¨Çect biases present in their training data. ‚Ä¢Real-time info: no access to data present after their training ‚Ä¢Real-time info: no access to data present after their training ‚Ä¢Privacy: your personal info can be part of training data ‚Ä¢Security: LLMs can be manipulated to give/not give']",1.0,0.3333333333333333,0.5,1,0.1111111111111111,1.0,0.19999999999999998,0.24920967335392727,0.12311901504787962,0.4255663430420712,3,0.10232892464329685,1.0,1.0,0.3333333333333333,1.0
Why can‚Äôt you perform data-dependent operations on meta tensors?,single,"Because meta tensors do not have real data, you cannot perform data-dependent operations like torch.nonzero()  or item() . In some cases, not all device types (e.g., CPU and CUDA) have exactly the same output metadata for an operation",1,3,"['allowing you to load a representation of the model without actually loading the actual parameters into memory. This can be helpful if you need to make transformations on the model before you load the actual data. Most operations can be performed on meta tensors, producing new meta tensors that describe what the result would have been if you performed the operation on a real tensor. You can use this to perform abstract analysis without needing to spend time on compute or space to represent the actual tensors. Because meta tensors do not have real data, you cannot perform data-dependent operations', 'opt_einsum (https://optimized-einsum.readthedocs.io/en/stable/path_finding.html) if available to calculate an optimal path of contraction for faster performance. If opt_einsum is not available, torch.einsum will fall back to the default contraction path of left to right. torch.backends.opt_einsum. strategy A str that specifies which strategies to try when torch.backends.opt_einsum.enabled is True. By default, torch.einsum will try the ‚Äúauto‚Äù strategy, but the ‚Äúgreedy‚Äù and ‚Äúoptimal‚Äù strategies are also supported. Note that the ‚Äúoptimal‚Äù strategy is factorial on the number of inputs as it tries all possible paths. See more details in opt_einsum ºs docs (https://optimized- einsum.readthedocs.io/en/stable/path_finding.html). torch.backends.xeon Previous Meta deviceNext torch.exportRate this Page‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ ¬© Copyright PyTorch Contributors.Send', 'that the rate limiter‚Äôs effectiveness is not consistent, as it does not attain any speedups in the RegNet experiments, and even impedes the DeepViT ones. This behavior is expected since throttling the communications can only boost training if the fast CPU thread aggressively allocates GPU memory blocks and causes defragmentations. If it is difficult to identify with certainty from latency measurements or profiled traces, CUDA malloc retry can serve as a helpful indicator, which can be obtained from the num_alloc_retries key in the torch.cuda.memory_stats() dictionary. The experiments conducted with T5 models have demonstrated that the rate limiter technique can greatly']",1.0,0.3333333333333333,0.5,1,0.058823529411764705,0.5,0.10526315789473684,0.11981943159824814,0.10545023696682465,0.1461794019933555,3,0.04515306086594271,0.0,1.0,0.0,1.0
