{
  "queries": [
    {
      "query": "What optimizations does FlashAttention introduce compared to standard attention kernels implemented in PyTorch?",
      "results": [
        {
          "doc": "23_flashattention_2205_14135",
          "filename": "23_flashattention_2205_14135",
          "text": "tention and block-sparse FlashAttention compared to the baselines in exact, approximate, and sparse\nattention (exact numbers in Appendix E). Runtime grows quadratically with sequence length, but FlashAt-\ntention runs signi\ufb01cantly faster than exact attention baselines, up to 3 \u0002faster than the PyTorch\nimplementation. The runtimes of many approximate/sparse attention mechanisms grow linearly with se-\nquence length, but FlashAttention still runs faster than approximate and sparse attention for short",
          "score": 4.376280307769775,
          "rank": 1
        },
        {
          "doc": "23_flashattention_2205_14135",
          "filename": "23_flashattention_2205_14135",
          "text": "HBM. We implement FlashAttention in CUDA to achieve \ufb01ne-grained control over memory access and\nfuse all the attention operations into one GPU kernel. Even with the increased FLOPs due to recomputation,\nour algorithm both runs faster (up to 7.6x on GPT-2 [ 67], Figure 1 right) and uses less memory \u2014linear\nin sequence length\u2014than standard attention, thanks to the massively reduced amount of HBM access.\nWe analyze the IO complexity [ 1] ofFlashAttention , proving that it requires \ud835\udc42\u00b9\ud835\udc412\ud835\udc512\ud835\udc40\u00001\u00baHBM",
          "score": 3.4309189319610596,
          "rank": 2
        },
        {
          "doc": "23_flashattention_2205_14135",
          "filename": "23_flashattention_2205_14135",
          "text": "computation back to HBM. Right:Speedup over the PyTorch implementation of attention on GPT-2.\nFlashAttention does not read and write the large \ud835\udc41\u0002\ud835\udc41attention matrix to HBM, resulting in an 7.6 \u0002\nspeedup on the attention computation.\nGPUs, compute speed has out-paced memory speed [ 61,62,63], and most operations in Transformers are\nbottlenecked by memory accesses [ 43]. IO-aware algorithms have been critical for similar memory-bound",
          "score": 3.3795762062072754,
          "rank": 3
        },
        {
          "doc": "23_flashattention_2205_14135",
          "filename": "23_flashattention_2205_14135",
          "text": "time of all models. Each task has a di\ufb00erent sequence length varying between 1024 and 4096. We follow the\nimplementation and experimental setting in Tay et al. [80]and Xiong et al. [90].3Table 3 shows that FlashAt-\ntention achieves up 2.4\u0002speed-up compared to standard attention. Block-sparse FlashAttention is\nfaster than all of the approximate attention methods that we have tested.\nTable 3: The performance of standard attention, FlashAttention , block-sparse FlashAttention , and approximate",
          "score": 2.9396629333496094,
          "rank": 4
        },
        {
          "doc": "23_flashattention_2205_14135",
          "filename": "23_flashattention_2205_14135",
          "text": "wall-clock time. We analyze its IO complexity, showing that our method requires much fewer HBM accesses\ncompared to standard attention. We further show that FlashAttention can serve as a useful primitive by\nextending it to handle block-sparse attention.\nWe focus here on the forward pass for ease of exposition; Appendix B contains details for the backward.\n3.1 An E\ufb03cient Attention Algorithm With Tiling and Recomputation",
          "score": 2.86691951751709,
          "rank": 5
        },
        {
          "doc": "23_flashattention_2205_14135",
          "filename": "23_flashattention_2205_14135",
          "text": "We analyze the IO complexity [ 1] ofFlashAttention , proving that it requires \ud835\udc42\u00b9\ud835\udc412\ud835\udc512\ud835\udc40\u00001\u00baHBM\naccesses where \ud835\udc51is the head dimension and \ud835\udc40is the size of SRAM, as compared to \u03a9\u00b9\ud835\udc41\ud835\udc51\u00b8\ud835\udc412\u00baof standard\nattention. For typical values of \ud835\udc51and\ud835\udc40,FlashAttention requires many times fewer HBM accesses\ncompared to standard attention (up to 9 \u0002fewer, as shown in Fig. 2). Moreover, we provide a lower bound,\nshowing that no exact attention algorithm can asymptotically improve on the number of HBM accesses over\nall SRAM sizes.",
          "score": 2.8076295852661133,
          "rank": 6
        },
        {
          "doc": "S2-Attention_ Hardware-Aware Context Sharding Among Attention Heads",
          "filename": "S2-Attention_ Hardware-Aware Context Sharding Among Attention Heads",
          "text": "make them efficiency in practice. They inspired a novel hybrid sparse attention architecture that\nmeets several desiderata that we find crucial for achieving both practical efficiency gains and strong\naccuracy on downstream tasks, called as Head-Heterogenous Strided Transformer ( HHST ). We will\nopen-source our kernel library and make it a plug-in-and-play alternative for FlashAttention-2 module\nin popular training frameworks like Megatron and Pytorch. We also integrated S2-A TTENTION into",
          "score": 1.8372914791107178,
          "rank": 7
        },
        {
          "doc": "23_flashattention_2205_14135",
          "filename": "23_flashattention_2205_14135",
          "text": "In Fig. 2 (left), we see that even though FlashAttention has higher FLOP count compared to standard\nattention (due to recomputation in the backward pass), it has much fewer HBM accesses, resulting in much\nfaster runtime. In Fig. 2 (middle), we vary the block size \ud835\udc35\ud835\udc50ofFlashAttention , which results in di\ufb00erent\namounts of HBM accesses, and measure the runtime of the forward pass. As block size increases, the number",
          "score": 1.6964211463928223,
          "rank": 8
        },
        {
          "doc": "S2-Attention_ Hardware-Aware Context Sharding Among Attention Heads",
          "filename": "S2-Attention_ Hardware-Aware Context Sharding Among Attention Heads",
          "text": "its theoretical reduction in FLOPs has rarely translated into wall-clock speed-up\nover its dense attention counterparts, mainly due to the lack of hardware-level\noptimizations like FlashAttention (Dao, 2023). Meanwhile, it remains unclear\nwhether sparse attention can maintain the model\u2019s quality at the scale of today\u2019s\nlarge language models (LLMs), and how this can be achieved. This paper presents\nSparsely-Sharded Attention ( S2-A TTENTION ), an optimized Triton kernel library",
          "score": 1.2094358205795288,
          "rank": 9
        },
        {
          "doc": "23_flashattention_2205_14135",
          "filename": "23_flashattention_2205_14135",
          "text": "an approximate attention algorithm that is faster than any existing approximate attention method.\nFlashAttention trains Transformers faster than existing baselines: 15% end-to-end wall-clock speedup\non BERT-large (seq. length 512) compared to the MLPerf 1.1 training speed record, 3 \u0002speedup on\nGPT-2 (seq. length 1K), and 2.4 \u0002speedup on long-range arena (seq. length 1K-4K). FlashAttention\nand block-sparse FlashAttention enable longer context in Transformers, yielding higher quality models",
          "score": 1.085585594177246,
          "rank": 10
        }
      ]
    },
    {
      "query": "What are the trade-offs between simple post-training quantization and GPTQ?",
      "results": [
        {
          "doc": "cmu_llmsys-18-peft",
          "filename": "cmu_llmsys-18-peft",
          "text": "28\n\u2022Overview of Parameter Efficient Fine -Tuning\n\u2022LoRA : Low -rank Adaptation (Counter -interference adapter, \nCIAT)\n\u2022QLoRA : Quantization + Low -rank training\n\u2022Code Walkthrough\n30Outline\n\u2022GPTQ is Post -Training Quantization (PTQ): converting the \nweights of an already trained model to a lower precision \nwithout any retraining. \n\u2022Quantization -Aware Training (QAT): integrates the weight \nconversion process during the training stage. often superior \nmodel performance. ( QLoRA ) ",
          "score": 2.925814628601074,
          "rank": 1
        },
        {
          "doc": "cmu_llmsys-17-quantization2",
          "filename": "cmu_llmsys-17-quantization2",
          "text": "error  incurred by quantizing a single weight\n6Overall idea of GPTQ\nOptimal Brain Compression: A framework for accurate post -training quantization and pruning (2022)Optimal Brain Surgeon and General Network Pruning (1993)GPTQ: Accurate Post -Training Quantization for Generative Pre -trained Transformers. Frantar  et al. ICLR 2023. 1.Pre-compute Cholesky decomposition of the Hessian \ninverse for input data X of current (Linear) layer\n2.Iteratively handle one batch of columns of weight matrix W",
          "score": 0.9343751072883606,
          "rank": 2
        },
        {
          "doc": "cmu_llmsys-16-quantization",
          "filename": "cmu_llmsys-16-quantization",
          "text": "\u2022The scalability is verified up to 20B models (GPT -NeoX20B)\n\u2022At 1.3B scale, computation time is ~3 hours\nobut slower than GPTQ (x100 larger in ~4 hours)\n\u2022integrated in Deepspeed\n22ZeroQuant\nYao et al. ZeroQuant : Efficient and Affordable Post -Training Quantization for Large -Scale Transformers. Neurips  2022.\u2022Using 8 -bit quantization for \nmatrix multiplications\n\u2022But, extreme outliers in \nfeatures (activation values)\noneed for wider numerical ranges \noQuantize all parameters without ",
          "score": 0.8492363095283508,
          "rank": 3
        },
        {
          "doc": "The Early Bird Catches the Leak_ Unveiling Timing Side Channels in LLM   Serving",
          "filename": "The Early Bird Catches the Leak_ Unveiling Timing Side Channels in LLM   Serving",
          "text": "language models,\u201d Advances in Neural Information Processing Systems ,\nvol. 35, pp. 17 402\u201317 414, 2022.\n[7] G. Xiao, J. Lin, M. Seznec, H. Wu, J. Demouth, and S. Han,\n\u201cSmoothquant: Accurate and efficient post-training quantization for\nlarge language models,\u201d in International Conference on Machine\nLearning . PMLR, 2023, pp. 38 087\u201338 099.\n[8] E. Frantar, S. Ashkboos, T. Hoefler, and D. Alistarh, \u201cGptq: Accu-\nrate post-training quantization for generative pre-trained transformers,\u201d",
          "score": -0.25159013271331787,
          "rank": 4
        },
        {
          "doc": "cmu_llmsys-17-quantization2",
          "filename": "cmu_llmsys-17-quantization2",
          "text": "19Insight of Arbitrary Update Order for OBQ\nGPTQ: Accurate Post -Training Quantization for Generative Pre -trained Transformers. Frantar  et al. ICLR 2023. \u2022Na\u00efve column update is not fast in practice \nolow compute -to-memory -access ratio\nocannot highly utilize GPUs compute.\n\u2022Observation: \noRounding decisions for col i only affected \nby updates on this col\noUpdates to later columns are irrelevant at \nthis point in the process.\n\u2022Efficient update\n20Lazy Batch Updates",
          "score": -0.42450520396232605,
          "rank": 5
        },
        {
          "doc": "cmu_llmsys-16-quantization",
          "filename": "cmu_llmsys-16-quantization",
          "text": "15Quantization   during training\npost trainingpreserve accuracy\nscale to large \nparametersBRECQ (Li et al., 2021)AdaQuant (Hubara et al., 2021)\nOBQ (Frantar et al., 2022)\nZeroQuant (Yao et al., 2022)\nLLM.int8() (Dettmers et al., 2022)Model Quantization Approaches\n16Quantization  during training\npost trainingpreserve \naccuracy\n(by quantizing each \nindividual / consecutive \nlayers)\nscale to large \nparametersBRECQ (Li et al., 2021)AdaQuant (Hubara et al., 2021)\nOBQ (Frantar et al., 2022)",
          "score": -1.9952819347381592,
          "rank": 6
        },
        {
          "doc": "cmu_llmsys-16-quantization",
          "filename": "cmu_llmsys-16-quantization",
          "text": "P rec@5 85.7 % 5.6 % 75.7 %\n19Is Quantization Accurate?20Why is Quantizing LLMs Difficult?\nQuantization  during training\npost trainingpreserve accuracy\nscale to large \nparameters\n(by rounding weights to the \nnearest quantization level)BRECQ (Li et al., 2021)AdaQuant (Hubara et al., 2021)\nOBQ (Frantar et al., 2022)\nZeroQuant (Yao et al., 2022)\nLLM.int8() (Dettmers et al., 2022)accuracy loss when lower -bit \nprecision (ex. 3, 4 bits per \nparameter)\u2022Layer -by-layer knowledge distillation ",
          "score": -2.208678722381592,
          "rank": 7
        },
        {
          "doc": "The Early Bird Catches the Leak_ Unveiling Timing Side Channels in LLM   Serving",
          "filename": "The Early Bird Catches the Leak_ Unveiling Timing Side Channels in LLM   Serving",
          "text": "rate post-training quantization for generative pre-trained transformers,\u201d\narXiv preprint arXiv:2210.17323 , 2022.\n[9] Z. Liu, B. Oguz, C. Zhao, E. Chang, P. Stock, Y . Mehdad, Y . Shi, R. Kr-\nishnamoorthi, and V . Chandra, \u201cLlm-qat: Data-free quantization aware\ntraining for large language models,\u201d arXiv preprint arXiv:2305.17888 ,\n2023.\n[10] W. Wang, W. Chen, Y . Luo, Y . Long, Z. Lin, L. Zhang, B. Lin,\nD. Cai, and X. He, \u201cModel compression and efficient inference for",
          "score": -2.6952579021453857,
          "rank": 8
        },
        {
          "doc": "cmu_llmsys-17-quantization2",
          "filename": "cmu_llmsys-17-quantization2",
          "text": "1.3B model - 3h\nDoes GPT -Q even work for extreme 2 -bit quantization?\n28\nHow is GPT -Q\u2019s perf on small models compared with \naccurate -but-expensive methods?\n29\n Fastest prior method\u2022https://github.com/qwopqwop200/GPTQ -for-LLaMa/  \n\u2022GPTQ in \nohttps://github.com/qwopqwop200/GPTQ -for-\nLLaMa/blob/triton/gptq.py  \n31GPTQ for LLaMAGPTQ: Initialization\n32\n\u25cfReshape weights from the \ninput layer\n\u25cfInitialize Hessian matrixGPTQ: Hessian Matrix Update\n33\u25cfUpdate Hessian matrix with \ninformation from a new ",
          "score": -2.788555860519409,
          "rank": 9
        },
        {
          "doc": "cmu_llmsys-16-quantization",
          "filename": "cmu_llmsys-16-quantization",
          "text": "return  X_quant .to(torch .int8), X_dequanthttps://colab.research.google.com/drive/1DPr4mUQ92Cc -\nxf4GgAaB6dFcFnWIvqYi?usp=sharing  \n12Direct Quantization Colab13Today\u2019s Topic\n\u2022Low precision numbers in computer\n\u2022Basic Quantization Methods\nModel Quantization Approaches\n14Quantization during training\npost trainingexpensive re -training / finetuning\nModel Quantization Approaches\n15Quantization   during training\npost trainingpreserve accuracy\nscale to large ",
          "score": -2.817843437194824,
          "rank": 10
        }
      ]
    },
    {
      "query": "What are the challenges of theoretical distributed training (PipeDream) and what is the scaling efficiency of distributed real-world Trainium performance (HLAT)?",
      "results": [
        {
          "doc": "Pipeline MoE_ A Flexible MoE Implementation with Pipeline Parallelism",
          "filename": "Pipeline MoE_ A Flexible MoE Implementation with Pipeline Parallelism",
          "text": "(parameter count) are further increased to an incredible 530 billion (Megatron-Turing-NLG Smith\net al. (2022)) and 540 billion (PaLM Chowdhery et al. (2022)), because the scaling law Henighan\net al. (2020) is still working.\nEf\ufb01cient Distributed Model Training. Scaling model training to tens of or hundreds of billion\nparameters is a complicated task, which requires a lot of algorithmic innovations and engineer-\ning optimization. One of the most critical challenges is that the model cannot \ufb01t into one single",
          "score": 1.956183671951294,
          "rank": 1
        },
        {
          "doc": "MoE Parallel Folding_ Heterogeneous Parallelism Mappings for Efficient   Large-S",
          "filename": "MoE Parallel Folding_ Heterogeneous Parallelism Mappings for Efficient   Large-S",
          "text": "increases, efficient distributed training across thousands of GPUs becomes essential. Different paral-\nlelism strategies have been proposed in recent years for distributed LLM training, including model\nparallelism, data parallelism, and pipeline parallelism [ 31;27;19]. However, a single parallelism\nstrategy has limitations regarding scalability. For example, the performance of data parallelism with\nZeRO-3 will decrease dramatically when the number of GPUs increases to several thousands [21].",
          "score": -0.2299526035785675,
          "rank": 2
        },
        {
          "doc": "Balanced and Elastic End-to-end Training of Dynamic LLMs",
          "filename": "Balanced and Elastic End-to-end Training of Dynamic LLMs",
          "text": "Devanur, Greg Ganger, and Phil Gibbons. 2018. Pipedream: Fast and efficient\npipeline parallel dnn training.arXiv preprint arXiv:1806.03377(2018).\n[18] Chaoyang He, Shen Li, Mahdi Soltanolkotabi, and Salman Avestimehr. 2021.\nPipetransformer: Automated elastic pipelining for distributed training of trans-\nformers.arXiv preprint arXiv:2102.03161(2021).\n[19] Jiaao He, Jidong Zhai, Tiago Antunes, Haojie Wang, Fuwen Luo, Shangfeng\nShi, and Qin Li. 2022. FasterMoE: Modeling and Optimizing Training of Large-",
          "score": -1.3023953437805176,
          "rank": 3
        },
        {
          "doc": "yiying_training-2",
          "filename": "yiying_training-2",
          "text": "Background: distributed training on Trainium",
          "score": -1.5990253686904907,
          "rank": 4
        },
        {
          "doc": "24_fsdp_2304_11277",
          "filename": "24_fsdp_2304_11277",
          "text": "training framework becomes increasingly imperative for applica-\ntions built on top of PyTorch. This section elucidates the trajectory\nof PyTorch\u2019s distributed training capabilities.\n2.1 Model Replication\nModel replication approaches are designed to tackle high-volume\ndatasets by scaling out and distributing computations across multi-\nple devices. DistributedDataParallel (DDP) [ 14] is the first end-to-end\ndistributed training feature in PyTorch that falls into this category.",
          "score": -1.6479536294937134,
          "rank": 5
        },
        {
          "doc": "Balanced and Elastic End-to-end Training of Dynamic LLMs",
          "filename": "Balanced and Elastic End-to-end Training of Dynamic LLMs",
          "text": "distributed training [ 4]. To address this gap, we introduce DynMo ,\nan elastic load-balancing framework tailored for dynamic models. It\nis the first work to study pipeline stalls caused by training dynamic\nmodels. DynMo ensures balanced pipeline stages by dynamically\nredistributing workloads across accelerators whenever imbalance\narises, thereby improving computational efficiency and reducing\ntraining costs. The framework incorporates two different dynamic",
          "score": -2.1752195358276367,
          "rank": 6
        },
        {
          "doc": "MoE Parallel Folding_ Heterogeneous Parallelism Mappings for Efficient   Large-S",
          "filename": "MoE Parallel Folding_ Heterogeneous Parallelism Mappings for Efficient   Large-S",
          "text": "effectively. Conventional distributed training methods include TP, DP, CP and PP. TP divides the\ncomputations of neural network layers across multiple devices, allowing for parallel processing\nof tensors within layers[ 31]. TP can significantly reduce the memory consumption of each model\nrank but introduces some intra-layer communication overhead. DP distributes batches of data\nacross replicas of the model on different devices, aggregating gradients during training[ 35]. Zero",
          "score": -3.075972080230713,
          "rank": 7
        },
        {
          "doc": "yiying_training-2",
          "filename": "yiying_training-2",
          "text": "Background: distributed training on Trainium\n4\u2022AWS Trainium\u2022Trn1.32xlarge contains 16 Trn accelerators, and 32 Neuron Cores\u202216GB memory per Neuron Core\u20223040 TFLOPS in FP16/BF1\u2022Cost $21.50 vs. p4d.24xlarge $32.77\u2022Neuron Distributed Training Library (NDTL, also called NeuronX-distributed)\u2022Tensor, pipeline, data, and sequence parallelism\u2022Zero-1 optimizer\u2022Multiple training precision configurations\u2022Automatic fault recovery\u2022\u2026\u00a9 2025, Amazon Web Services, Inc. or its affiliates. All rights reserved. Amazon Confidential and Trademark.",
          "score": -3.3283848762512207,
          "rank": 8
        },
        {
          "doc": "distributed_pipelining",
          "filename": "distributed_pipelining",
          "text": "What is torch.distributed.pipelining ?\nWhile promising for scaling, pipelining is often difficult to implement because it needs to partition\nthe execution of a model in addition to model weights. The partitioning of execution often requires\nintrusive code changes to your model. Another aspect of complexity comes from scheduling\nmicro-batches in a distributed environment, with data flow dependency considered.\nThe pipelining  package provides a toolkit that does said things automatically which allows easy",
          "score": -3.7863259315490723,
          "rank": 9
        },
        {
          "doc": "MCR-DL_ Mix-and-Match Communication Runtime for Deep Learning",
          "filename": "MCR-DL_ Mix-and-Match Communication Runtime for Deep Learning",
          "text": "being applied to other domains such as vision [22]. DLRM\n[3] has scaled beyond a trillion parameters with 4D parallelism\ntechniques [9]. We demonstrate that our work further improves\nthe scaling behavior of these complex parallel DL models.\nIII. B ACKGROUND\nA. DL Training\nDistributed DL can take several forms: data-parallelism,\nmodel-parallelism, and hybrid-parallelism. Data-parallism\nplaces a full model replica on each processor, and splits\nthe training data among processors. Model parallelism splits",
          "score": -3.8104987144470215,
          "rank": 10
        }
      ]
    },
    {
      "query": "What is the difference between torch.disttibuted and torch.distributed.pipelining?",
      "results": [
        {
          "doc": "distributed_pipelining",
          "filename": "distributed_pipelining",
          "text": "What is torch.distributed.pipelining ?\nWhile promising for scaling, pipelining is often difficult to implement because it needs to partition\nthe execution of a model in addition to model weights. The partitioning of execution often requires\nintrusive code changes to your model. Another aspect of complexity comes from scheduling\nmicro-batches in a distributed environment, with data flow dependency considered.\nThe pipelining  package provides a toolkit that does said things automatically which allows easy",
          "score": 4.678384780883789,
          "rank": 1
        },
        {
          "doc": "distributed_pipelining",
          "filename": "distributed_pipelining",
          "text": "splitting the module. (default: None)\nReturn type:\nA pipeline representation of class Pipe.\nclass torch.distributed.pipelining. Pipe (split_gm , num_stages ,\nhas_loss_and_backward , loss_spec )\ntorch.distributed.pipelining. pipe_split ( )\npipe_split is a special operator that is used to mark the boundary between stages in a module.\nIt is used to split the module into stages. It is a no-op if your annotated module is run eagerly.\nExample\nThe above example will be split into two stages.\nMicrobatch Utilities",
          "score": 3.0660109519958496,
          "rank": 2
        },
        {
          "doc": "distributed_pipelining",
          "filename": "distributed_pipelining",
          "text": "model to be partitioned such that multiple micro-batches can execute different parts of the mode\ncode concurrently. Pipeline parallelism can be an effective technique for:\nlarge-scale training\nbandwidth-limited clusters\nlarge model inference\nThe above scenarios share a commonality that the computation per device cannot hide the\ncommunication of conventional parallelism, for example, the weight all-gather of FSDP.\nWhat is torch.distributed.pipelining ?",
          "score": 1.981663465499878,
          "rank": 3
        },
        {
          "doc": "distributed_pipelining",
          "filename": "distributed_pipelining",
          "text": "First, the pipeline  API turns our model into a directed acyclic graph (DAG) by tracing the model. \ntraces the model using torch.export  \u2013 a PyTorch 2 full-graph capturing tool.\nThen, it groups together the operations and parameters needed by a stage into a reconstructed\nsubmodule: submod_0, submod_1, \u2026from torch.distributed.pipelining  import build_stage\nfrom torch.nn.parallel  import DistributedDataParallel\ndp_mod = DistributedDataParallel (stage_mod )\ninfo = pipe.info()",
          "score": 0.403606653213501,
          "rank": 4
        },
        {
          "doc": "distributed_pipelining",
          "filename": "distributed_pipelining",
          "text": "Pipeline Parallelism\nCreated On: Jun 16, 2025 | Last Updated On: Jun 16, 2025\ntorch.distributed.pipelining  is currently in alpha state and under development. API\nchanges may be possible. It was migrated from the PiPPy project.\nWhy Pipeline Parallel?\nPipeline Parallelism is one of the primitive parallelism for deep learning. It allows the execution of \nmodel to be partitioned such that multiple micro-batches can execute different parts of the mode",
          "score": 0.36964425444602966,
          "rank": 5
        },
        {
          "doc": "distributed_pipelining",
          "filename": "distributed_pipelining",
          "text": "Policy.10/10/25, 3:04 PM Pipeline Parallelism \u2014 PyTorch 2.8 documentation\nhttps://docs.pytorch.org/docs/stable/distributed.pipelining.html 9/17[source\n[sourceLogging\nYou can turn on additional logging using the TORCH_LOGS  environment variable from torch._logging\nTORCH_LOGS=+pp  will display logging.DEBUG  messages and all levels above it.\nTORCH_LOGS=pp  will display logging.INFO  messages and above.\nTORCH_LOGS=-pp  will display logging.WARNING  messages and above.\nAPI Reference\nModel Split APIs",
          "score": -0.9634859561920166,
          "rank": 6
        },
        {
          "doc": "distributed_pipelining",
          "filename": "distributed_pipelining",
          "text": "navigating, you agree to allow our usage of cookies. As the current maintainers of this site,\nFacebook\u02bcs Cookies Policy applies. Learn more, including about available controls: Cookies\nPolicy.10/10/25, 3:04 PM Pipeline Parallelism \u2014 PyTorch 2.8 documentation\nhttps://docs.pytorch.org/docs/stable/distributed.pipelining.html 12/17[source\n[sourceclass torch.distributed.pipelining.stage. PipelineStage (submodule ,\nstage_index , num_stages , device, input_args =None, output_args =None,",
          "score": -1.3188668489456177,
          "rank": 7
        },
        {
          "doc": "distributed",
          "filename": "distributed",
          "text": "Distributed communication package -\ntorch.distributed\nCreated On: Jul 12, 2017 | Last Updated On: Jul 14, 2025\nPlease refer to PyTorch Distributed Overview for a brief introduction to all features related\nto distributed training.\nBackends\ntorch.distributed  supports three built-in backends, each with different capabilities. The table\nbelow shows which functions are available for use with CPU / CUDA tensors. MPI supports CUDA\nonly if the implementation used to build PyTorch supports it.\nBackend gloo mpi nccl",
          "score": -1.7709364891052246,
          "rank": 8
        },
        {
          "doc": "distributed_pipelining",
          "filename": "distributed_pipelining",
          "text": "Policy.10/10/25, 3:04 PM Pipeline Parallelism \u2014 PyTorch 2.8 documentation\nhttps://docs.pytorch.org/docs/stable/distributed.pipelining.html 7/17Alternatively, if you would like to build the stage runtime later after some modification to the\nstage_mod, you can use a functional version of the build_stage  API. For example:\nThe pipeline  frontend uses a tracer (torch.export) to capture your model into a single\ngraph. If your model is not full-graph\u02bcable, you can use our manual frontend below.",
          "score": -2.533738136291504,
          "rank": 9
        },
        {
          "doc": "distributed_pipelining",
          "filename": "distributed_pipelining",
          "text": "navigating, you agree to allow our usage of cookies. As the current maintainers of this site,\nFacebook\u02bcs Cookies Policy applies. Learn more, including about available controls: Cookies\nPolicy.10/10/25, 3:04 PM Pipeline Parallelism \u2014 PyTorch 2.8 documentation\nhttps://docs.pytorch.org/docs/stable/distributed.pipelining.html 3/17the Transformer class defined in Torchtitan shows a pattern of building an easily partitionable\nmodel.",
          "score": -3.288337469100952,
          "rank": 10
        }
      ]
    },
    {
      "query": "Explain the importance of ImageNet in the works alexnet and googlenet.",
      "results": [
        {
          "doc": "10_efficientnet_1905_11946",
          "filename": "10_efficientnet_1905_11946",
          "text": "ference. Compared to the widely used ResNet-50 (He et al.,\n2016), our Ef\ufb01cientNet-B4 improves the top-1 accuracy\nfrom 76.3% to 83.0% (+6.7%) with similar FLOPS. Besides\nImageNet, Ef\ufb01cientNets also transfer well and achieve state-of-the-art accuracy on 5 out of 8 widely used datasets, while\nreducing parameters by up to 21x than existing ConvNets.\n2. Related Work\nConvNet Accuracy: Since AlexNet (Krizhevsky et al.,\n2012) won the 2012 ImageNet competition, ConvNets have",
          "score": 0.3131418824195862,
          "rank": 1
        },
        {
          "doc": "14_simclr_2002_05709",
          "filename": "14_simclr_2002_05709",
          "text": "ImageNet only in the choice of data augmentation, the use of\na nonlinear head at the end of the network, and the loss func-\ntion. The strength of this simple framework suggests that,\ndespite a recent surge in interest, self-supervised learning\nremains undervalued.\nAcknowledgements\nWe would like to thank Xiaohua Zhai, Rafael M\u00fcller and\nYani Ioannou for their feedback on the draft. We are also\ngrateful for general support from Google Research teams in\nToronto and elsewhere.\nReferences",
          "score": -0.0018902719020843506,
          "rank": 2
        },
        {
          "doc": "14_simclr_2002_05709",
          "filename": "14_simclr_2002_05709",
          "text": "composition of two transformations (applied sequentially). The\nlast column re\ufb02ects the average over the row.\nTo understand the effects of individual data augmentations\nand the importance of augmentation composition, we in-\nvestigate the performance of our framework when applying\naugmentations individually or in pairs. Since ImageNet\nimages are of different sizes, we always apply crop and re-\nsize images (Krizhevsky et al., 2012; Szegedy et al., 2015),\nwhich makes it dif\ufb01cult to study other augmentations in",
          "score": -1.241518497467041,
          "rank": 3
        },
        {
          "doc": "06_dropout_1207_0580",
          "filename": "06_dropout_1207_0580",
          "text": "as belonging to the class indicated by the image label.\nE ImageNet\nImageNet is a dataset of millions of labeled images in thousands of categories. The images\nwere collected from the web and labelled by human labellers using Amazon\u2019s Mechanical Turk\ncrowd-sourcing tool. In 2010, a subset of roughly 1000 images in each of 1000 classes was the\nbasis of an object recognition competition, a part of the Pascal Visual Object Challenge. This",
          "score": -2.2180066108703613,
          "rank": 4
        },
        {
          "doc": "04_googlenet_1409_4842",
          "filename": "04_googlenet_1409_4842",
          "text": "keeping the computational budget constant. To optimize quality, the architectural\ndecisions were based on the Hebbian principle and the intuition of multi-scale\nprocessing. One particular incarnation used in our submission for ILSVRC14 is\ncalled GoogLeNet, a 22 layers deep network, the quality of which is assessed in\nthe context of classi\ufb01cation and detection.\n1 Introduction\nIn the last three years, mainly due to the advances of deep learning, more concretely convolutional",
          "score": -2.469942331314087,
          "rank": 5
        },
        {
          "doc": "02_resnet_1512_03385",
          "filename": "02_resnet_1512_03385",
          "text": "ity. An ensemble of these residual nets achieves 3.57% error\non the ImageNet testset. This result won the 1st place on the\nILSVRC 2015 classi\ufb01cation task. We also present analysis\non CIFAR-10 with 100 and 1000 layers.\nThe depth of representations is of central importance\nfor many visual recognition tasks. Solely due to our ex-\ntremely deep representations, we obtain a 28% relative im-\nprovement on the COCO object detection dataset. Deep\nresidual nets are foundations of our submissions to ILSVRC",
          "score": -2.6481130123138428,
          "rank": 6
        },
        {
          "doc": "06_dropout_1207_0580",
          "filename": "06_dropout_1207_0580",
          "text": "and there are 1000 categories instead of ten. Another difference is that the ImageNet images\noften contain multiple instances of ImageNet objects, simply due to the sheer number of object\nclasses. For this reason, even a human would have dif\ufb01culty approaching perfect accuracy on\nthis dataset. For our experiments we resized all images to 256\u0002256pixels.\nF Convolutional Neural Networks\nOur models for CIFAR-10 and ImageNet are deep, feed-forward convolutional neural networks",
          "score": -2.7800137996673584,
          "rank": 7
        },
        {
          "doc": "01_alexnet_imagenet_2012",
          "filename": "01_alexnet_imagenet_2012",
          "text": "ImageNet Classi\ufb01cation with Deep Convolutional\nNeural Networks\nAlex Krizhevsky\nUniversity of Toronto\nkriz@cs.utoronto.caIlya Sutskever\nUniversity of Toronto\nilya@cs.utoronto.caGeoffrey E. Hinton\nUniversity of Toronto\nhinton@cs.utoronto.ca\nAbstract\nWe trained a large, deep convolutional neural network to classify the 1.2 million\nhigh-resolution images in the ImageNet LSVRC-2010 contest into the 1000 dif-\nferent classes. On the test data, we achieved top-1 and top-5 error rates of 37.5%",
          "score": -3.2132701873779297,
          "rank": 8
        },
        {
          "doc": "04_googlenet_1409_4842",
          "filename": "04_googlenet_1409_4842",
          "text": "of it. We have found that all the included the knobs and levers allow for a controlled balancing of\ncomputational resources that can result in networks that are 2\u00003\u0002faster than similarly performing\nnetworks with non-Inception architecture, however this requires careful manual design at this point.\n5 GoogLeNet\nWe chose GoogLeNet as our team-name in the ILSVRC14 competition. This name is an homage to\nYann LeCuns pioneering LeNet 5 network [10]. We also use GoogLeNet to refer to the particular",
          "score": -3.2259721755981445,
          "rank": 9
        },
        {
          "doc": "04_googlenet_1409_4842",
          "filename": "04_googlenet_1409_4842",
          "text": "1. We independently trained 7 versions of the same GoogLeNet model (including one wider\nversion), and performed ensemble prediction with them. These models were trained with\nthe same initialization (even with the same initial weights, mainly because of an oversight)\nand learning rate policies, and they only differ in sampling methodologies and the random\norder in which they see input images.\n2. During testing, we adopted a more aggressive cropping approach than that of Krizhevsky et",
          "score": -3.825533151626587,
          "rank": 10
        }
      ]
    },
    {
      "query": "What search algorithm does AlphaZero use instead of alpha-beta search?",
      "results": [
        {
          "doc": "18_alphazero_1712_01815",
          "filename": "18_alphazero_1712_01815",
          "text": "v\u0019E[zjs].AlphaZero learns these move probabilities and value estimates entirely from self-\nplay; these are then used to guide its search.\nInstead of an alpha-beta search with domain-speci\ufb01c enhancements, AlphaZero uses a general-\npurpose Monte-Carlo tree search (MCTS) algorithm. Each search consists of a series of simu-\nlated games of self-play that traverse a tree from root srootto leaf. Each simulation proceeds by\nselecting in each state sa moveawith low visit count, high move probability and high value",
          "score": 9.524613380432129,
          "rank": 1
        },
        {
          "doc": "18_alphazero_1712_01815",
          "filename": "18_alphazero_1712_01815",
          "text": "out when evaluating a large subtree. In contrast, alpha-beta search computes an explicit mini-\nmax, which propagates the biggest approximation errors to the root of the subtree. Using MCTS\nmay allow AlphaZero to effectively combine its neural network representations with a powerful,\ndomain-independent search.\nDomain Knowledge\n1. The input features describing the position, and the output features describing the move,\nare structured as a set of planes; i.e. the neural network architecture is matched to the",
          "score": 5.914522171020508,
          "rank": 2
        },
        {
          "doc": "18_alphazero_1712_01815",
          "filename": "18_alphazero_1712_01815",
          "text": "An approach based on training dual policy and value networks using AlphaZero -like policy\niteration was successfully applied to improve on the state-of-the-art in Hex ( 3).\n11MCTS and Alpha-Beta Search\nFor at least four decades the strongest computer chess programs have used alpha-beta search\n(18, 23 ).AlphaZero uses a markedly different approach that averages over the position evalu-\nations within a subtree, rather than computing the minimax evaluation of that subtree. How-",
          "score": 5.877591133117676,
          "rank": 3
        },
        {
          "doc": "18_alphazero_1712_01815",
          "filename": "18_alphazero_1712_01815",
          "text": "(see Table 1).\nWe also analysed the relative performance of AlphaZero \u2019s MCTS search compared to the\nstate-of-the-art alpha-beta search engines used by Stock\ufb01sh andElmo .AlphaZero searches just\n80 thousand positions per second in chess and 40 thousand in shogi, compared to 70 million\nforStock\ufb01sh and 35 million for Elmo .AlphaZero compensates for the lower number of evalu-\nations by using its deep neural network to focus much more selectively on the most promising",
          "score": 5.766561985015869,
          "rank": 4
        },
        {
          "doc": "18_alphazero_1712_01815",
          "filename": "18_alphazero_1712_01815",
          "text": "ations within a subtree, rather than computing the minimax evaluation of that subtree. How-\never, chess programs using traditional MCTS were much weaker than alpha-beta search pro-\ngrams, ( 4, 24 ); while alpha-beta programs based on neural networks have previously been un-\nable to compete with faster, handcrafted evaluation functions.\nAlphaZero evaluates positions using non-linear function approximation based on a deep",
          "score": 5.0849289894104,
          "rank": 5
        },
        {
          "doc": "18_alphazero_1712_01815",
          "filename": "18_alphazero_1712_01815",
          "text": "AlphaZero evaluates positions using non-linear function approximation based on a deep\nneural network, rather than the linear function approximation used in typical chess programs.\nThis provides a much more powerful representation, but may also introduce spurious approxi-\nmation errors. MCTS averages over these approximation errors, which therefore tend to cancel\nout when evaluating a large subtree. In contrast, alpha-beta search computes an explicit mini-",
          "score": 4.410152912139893,
          "rank": 6
        },
        {
          "doc": "18_alphazero_1712_01815",
          "filename": "18_alphazero_1712_01815",
          "text": "and a tabula rasa reinforcement learning algorithm.\nInstead of a handcrafted evaluation function and move ordering heuristics, AlphaZero utilises\na deep neural network (p;v) =f\u0012(s)with parameters \u0012. This neural network takes the board po-\nsitionsas an input and outputs a vector of move probabilities pwith components pa=Pr(ajs)\n2for each action a, and a scalar value vestimating the expected outcome zfrom position s,\nv\u0019E[zjs].AlphaZero learns these move probabilities and value estimates entirely from self-",
          "score": 4.188767910003662,
          "rank": 7
        },
        {
          "doc": "18_alphazero_1712_01815",
          "filename": "18_alphazero_1712_01815",
          "text": "the-art programs are based on powerful engines that search many millions of positions, leverag-\ning handcrafted domain expertise and sophisticated domain adaptations. AlphaZero is a generic\nreinforcement learning algorithm \u2013 originally devised for the game of Go \u2013 that achieved su-\nperior results within a few hours, searching a thousand times fewer positions, given no domain\n3The prevalence of draws in high-level chess tends to compress the Elo scale, compared to shogi or Go.",
          "score": 3.915585994720459,
          "rank": 8
        },
        {
          "doc": "18_alphazero_1712_01815",
          "filename": "18_alphazero_1712_01815",
          "text": "ons ( 5). These programs use a similar algorithm to computer chess programs, again based on a\nhighly optimised alpha-beta search engine with many domain-speci\ufb01c adaptations.\nGo is well suited to the neural network architecture used in AlphaGo because the rules of\nthe game are translationally invariant (matching the weight sharing structure of convolutional\nnetworks), are de\ufb01ned in terms of liberties corresponding to the adjacencies between points",
          "score": 3.089620590209961,
          "rank": 9
        },
        {
          "doc": "18_alphazero_1712_01815",
          "filename": "18_alphazero_1712_01815",
          "text": "scaled more effectively with thinking time than either Stock\ufb01sh orElmo , calling into question\nthe widely held belief ( 4, 11 ) that alpha-beta search is inherently superior in these domains.3\nFinally, we analysed the chess knowledge discovered by AlphaZero . Table 2 analyses the\nmost common human openings (those played more than 100,000 times in an online database\nof human chess games ( 1)). Each of these openings is independently discovered and played",
          "score": 2.8457324504852295,
          "rank": 10
        }
      ]
    },
    {
      "query": "What is internal covariate shift, and how does it affect training?",
      "results": [
        {
          "doc": "05_batchnorm_1502_03167",
          "filename": "05_batchnorm_1502_03167",
          "text": "ratesandcarefulparameterinitialization,andmakesitno -\ntoriously hard to train models with saturating nonlineari-\nties. We refer to this phenomenon as internal covariate\nshift, and address the problem by normalizing layer in-\nputs. Ourmethoddrawsitsstrengthfrommakingnormal-\nizationapartofthemodelarchitectureandperformingthe\nnormalization for each training mini-batch . Batch Nor-\nmalizationallowsustousemuchhigherlearningratesand\nbe less careful about initialization. It also acts as a regu-",
          "score": 4.656845569610596,
          "rank": 1
        },
        {
          "doc": "05_batchnorm_1502_03167",
          "filename": "05_batchnorm_1502_03167",
          "text": "that the distribution of nonlinearity inputs remains more\nstable as the network trains, then the optimizer would be\nless likely to get stuck in the saturated regime, and the\ntrainingwouldaccelerate.\nWe refer to the change in the distributions of internal\nnodes of a deep network, in the course of training, as In-\nternal Covariate Shift . Eliminating it offers a promise of\nfaster training. We propose a new mechanism, which we\ncallBatch Normalization , that takes a step towards re-",
          "score": 4.615631580352783,
          "rank": 2
        },
        {
          "doc": "05_batchnorm_1502_03167",
          "filename": "05_batchnorm_1502_03167",
          "text": "that we can match its performance using only 7% of the\ntraining steps, and can further exceed its accuracy by a\nsubstantial margin. Using an ensemble of such networks\ntrained with Batch Normalization, we achieve the top-5\nerror rate that improves upon the best known results on\nImageNetclassi\ufb01cation.2 Towards Reducing Internal\nCovariateShift\nWe de\ufb01ne Internal Covariate Shift as the change in the\ndistribution of network activations due to the change in\nnetworkparametersduringtraining. Toimprovethetrain-",
          "score": 3.7508273124694824,
          "rank": 3
        },
        {
          "doc": "05_batchnorm_1502_03167",
          "filename": "05_batchnorm_1502_03167",
          "text": "callBatch Normalization , that takes a step towards re-\nducing internal covariate shift, and in doing so dramati-\ncally accelerates the training of deep neural nets. It ac-\ncomplishes this via a normalization step that \ufb01xes the\nmeansandvariancesoflayerinputs. BatchNormalization\nalso has a bene\ufb01cial effect on the gradient \ufb02ow through\nthe network, by reducing the dependence of gradients\non the scale of the parameters or of their initial values.\nThis allows us to use much higher learning rates with-",
          "score": 3.536703586578369,
          "rank": 4
        },
        {
          "doc": "05_batchnorm_1502_03167",
          "filename": "05_batchnorm_1502_03167",
          "text": "networkparametersduringtraining. Toimprovethetrain-\ning, we seek to reduce the internal covariate shift. By\n\ufb01xingthe distributionof the layer inputs xas the training\nprogresses,weexpecttoimprovethetrainingspeed. Ithas\nbeen long known (LeCunetal., 1998b; Wiesler &Ney,\n2011) that the network training convergesfaster if its in-\nputsarewhitened\u2013i.e.,linearlytransformedtohavezero\nmeansandunitvariances,anddecorrelated. Aseachlayer\nobservestheinputsproducedbythelayersbelow,itwould",
          "score": 3.044081211090088,
          "rank": 5
        },
        {
          "doc": "02_resnet_1512_03385",
          "filename": "02_resnet_1512_03385",
          "text": "computation , 9(8):1735\u20131780, 1997.\n[16] S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep\nnetwork training by reducing internal covariate shift. In ICML , 2015.\n[17] H. Jegou, M. Douze, and C. Schmid. Product quantization for nearest\nneighbor search. TPAMI , 33, 2011.\n[18] H. Jegou, F. Perronnin, M. Douze, J. Sanchez, P. Perez, and\nC. Schmid. Aggregating local image descriptors into compact codes.\nTPAMI , 2012.\n[19] Y . Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick,",
          "score": 1.9597810506820679,
          "rank": 6
        },
        {
          "doc": "14_simclr_2002_05709",
          "filename": "14_simclr_2002_05709",
          "text": "network training by reducing internal covariate shift. arXiv\npreprint arXiv:1502.03167 , 2015.\nJi, X., Henriques, J. F., and Vedaldi, A. Invariant information\nclustering for unsupervised image classi\ufb01cation and segmenta-\ntion. In Proceedings of the IEEE International Conference on\nComputer Vision , pp. 9865\u20139874, 2019.\nKingma, D. P. and Welling, M. Auto-encoding variational bayes.\narXiv preprint arXiv:1312.6114 , 2013.\nKolesnikov, A., Zhai, X., and Beyer, L. Revisiting self-supervised",
          "score": 1.878291130065918,
          "rank": 7
        },
        {
          "doc": "05_batchnorm_1502_03167",
          "filename": "05_batchnorm_1502_03167",
          "text": "\ufb01tting,inabatch-normalizednetworkwefoundthatitcan\nbeeitherremovedorreducedinstrength.\n4 Experiments\n4.1 Activationsovertime\nTo verify the effects of internal covariate shift on train-\ning, and the ability of Batch Normalization to combat it,\nweconsideredtheproblemofpredictingthedigitclasson\ntheMNISTdataset(LeCunetal.,1998a). Weusedavery\nsimple network, with a 28x28binary image as input, and\n510K20K30K40K50K0.70.80.91\n  \nWithout BN\nWith BN\n\u2212202\n\u2212202\n(a) (b)WithoutBN (c)With BN",
          "score": 1.5827454328536987,
          "rank": 8
        },
        {
          "doc": "09_vit_2010_11929",
          "filename": "09_vit_2010_11929",
          "text": "and Aaron van den Oord. Data-ef\ufb01cient image recognition with contrastive predictive coding. In\nICML , 2020.\n10Published as a conference paper at ICLR 2021\nSergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by\nreducing internal covariate shift. 2015.\nDiederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR , 2015.\nAlexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan Puigcerver, Jessica Yung, Sylvain Gelly,",
          "score": 0.7784285545349121,
          "rank": 9
        },
        {
          "doc": "05_batchnorm_1502_03167",
          "filename": "05_batchnorm_1502_03167",
          "text": "detailsofensembleandmulticropinferencearesimilarto\n(Szegedyet al., 2014).\nWe demonstrate in Fig. 4 that batch normalization al-\nlowsusto set new state-of-the-artby a healthymarginon\ntheImageNetclassi\ufb01cationchallengebenchmarks.\n5 Conclusion\nWe have presented a novel mechanism for dramatically\naccelerating the training of deep networks. It is based on\nthe premise that covariate shift, which is known to com-\nplicate the trainingof machine learning systems, also ap-",
          "score": -0.5672375559806824,
          "rank": 10
        }
      ]
    },
    {
      "query": "What does \u201cIO-aware\u201d mean in the context of FlashAttention?",
      "results": [
        {
          "doc": "23_flashattention_2205_14135",
          "filename": "23_flashattention_2205_14135",
          "text": "aware\u2014accounting for reads and writes between levels of GPU memory. We propose FlashAttention ,\nan IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes\nbetween GPU high bandwidth memory (HBM) and GPU on-chip SRAM. We analyze the IO complexity\nofFlashAttention , showing that it requires fewer HBM accesses than standard attention, and is\noptimal for a range of SRAM sizes. We also extend FlashAttention to block-sparse attention, yielding",
          "score": 5.13558292388916,
          "rank": 1
        },
        {
          "doc": "The Early Bird Catches the Leak_ Unveiling Timing Side Channels in LLM   Serving",
          "filename": "The Early Bird Catches the Leak_ Unveiling Timing Side Channels in LLM   Serving",
          "text": "pression for large language models,\u201d arXiv preprint arXiv:2308.07633 ,\n2023.\n[13] T. Dao, D. Fu, S. Ermon, A. Rudra, and C. R \u00b4e, \u201cFlashattention: Fast\nand memory-efficient exact attention with io-awareness,\u201d Advances in\nNeural Information Processing Systems , vol. 35, pp. 16 344\u201316 359,\n2022.15\n[14] W. Kwon, Z. Li, S. Zhuang, Y . Sheng, L. Zheng, C. H. Yu, J. Gonzalez,\nH. Zhang, and I. Stoica, \u201cEfficient memory management for large\nlanguage model serving with pagedattention,\u201d in Proceedings of the",
          "score": 3.7598190307617188,
          "rank": 2
        },
        {
          "doc": "23_flashattention_2205_14135",
          "filename": "23_flashattention_2205_14135",
          "text": "Transformers are slow and memory-hungry on long sequences, since the time and memory complexity\nof self-attention are quadratic in sequence length. Approximate attention methods have attempted\nto address this problem by trading o\ufb00 model quality to reduce the compute complexity, but often do\nnot achieve wall-clock speedup. We argue that a missing principle is making attention algorithms IO-\naware\u2014accounting for reads and writes between levels of GPU memory. We propose FlashAttention ,",
          "score": 2.313486337661743,
          "rank": 3
        },
        {
          "doc": "Progressive Sparse Attention_ Algorithm and System Co-design for   Efficient Att",
          "filename": "Progressive Sparse Attention_ Algorithm and System Co-design for   Efficient Att",
          "text": "[10] Tri Dao, Daniel Y . Fu, Stefano Ermon, Atri Rudra, and Christopher R\u00e9. Flashattention:\nFast and memory-efficient exact attention with io-awareness. In Sanmi Koyejo, S. Mo-\nhamed, A. Agarwal, Danielle Belgrave, K. Cho, and A. Oh, editors, Advances in Neu-\nral Information Processing Systems 35: Annual Conference on Neural Information Pro-\ncessing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - Decem-\nber 9, 2022 , 2022. URL http://papers.nips.cc/paper_files/paper/2022/hash/",
          "score": 2.2399744987487793,
          "rank": 4
        },
        {
          "doc": "S2-Attention_ Hardware-Aware Context Sharding Among Attention Heads",
          "filename": "S2-Attention_ Hardware-Aware Context Sharding Among Attention Heads",
          "text": "48550/arXiv.2307.08691 .\nTri Dao, Daniel Y . Fu, Stefano Ermon, Atri Rudra, and Christopher R\u00e9. Flashattention:\nFast and memory-efficient exact attention with io-awareness. In Sanmi Koyejo, S. Mo-\nhamed, A. Agarwal, Danielle Belgrave, K. Cho, and A. Oh (eds.), Advances in Neural\nInformation Processing Systems 35: Annual Conference on Neural Information Process-\ning Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9,\n2022 , 2022. URL http://papers.nips.cc/paper_files/paper/2022/hash/",
          "score": 2.021533966064453,
          "rank": 5
        },
        {
          "doc": "23_flashattention_2205_14135",
          "filename": "23_flashattention_2205_14135",
          "text": "computation back to HBM. Right:Speedup over the PyTorch implementation of attention on GPT-2.\nFlashAttention does not read and write the large \ud835\udc41\u0002\ud835\udc41attention matrix to HBM, resulting in an 7.6 \u0002\nspeedup on the attention computation.\nGPUs, compute speed has out-paced memory speed [ 61,62,63], and most operations in Transformers are\nbottlenecked by memory accesses [ 43]. IO-aware algorithms have been critical for similar memory-bound",
          "score": 1.8948215246200562,
          "rank": 6
        },
        {
          "doc": "23_flashattention_2205_14135",
          "filename": "23_flashattention_2205_14135",
          "text": "FlashAttention : Fast and Memory-E\ufb03cient Exact Attention\nwith IO-Awareness\nTri Daoy, Daniel Y. Fuy, Stefano Ermony, Atri Rudraz, and Christopher R\u00e9y\nyDepartment of Computer Science, Stanford University\nzDepartment of Computer Science and Engineering, University at Bu\ufb00alo, SUNY\n{trid,danfu}@cs.stanford.edu ,ermon@stanford.edu ,atri@buffalo.edu ,\nchrismre@cs.stanford.edu\nJune 24, 2022\nAbstract\nTransformers are slow and memory-hungry on long sequences, since the time and memory complexity",
          "score": 1.7668567895889282,
          "rank": 7
        },
        {
          "doc": "S2-Attention_ Hardware-Aware Context Sharding Among Attention Heads",
          "filename": "S2-Attention_ Hardware-Aware Context Sharding Among Attention Heads",
          "text": "the attention computation into smaller block-wise computation to reduce the IO between SRAM and\nthe high bandwidth memory (HBM). The hardware implementation of FlashAttention family (Dao\net al., 2022; Dao, 2023) make them the most widely-adopted attention acceleration framework. It\nremains unclear whether we can implement various sparse self-attention in such hardware-aware way,\nso that the training speed can be further boosted over FlashAttention.\n2.2 I SSUES WITH PLUG-IN-AND -PLAY KV E VICTION METHODS",
          "score": 1.56980299949646,
          "rank": 8
        },
        {
          "doc": "23_flashattention_2205_14135",
          "filename": "23_flashattention_2205_14135",
          "text": "wall-clock time. We analyze its IO complexity, showing that our method requires much fewer HBM accesses\ncompared to standard attention. We further show that FlashAttention can serve as a useful primitive by\nextending it to handle block-sparse attention.\nWe focus here on the forward pass for ease of exposition; Appendix B contains details for the backward.\n3.1 An E\ufb03cient Attention Algorithm With Tiling and Recomputation",
          "score": 0.5867414474487305,
          "rank": 9
        },
        {
          "doc": "23_flashattention_2205_14135",
          "filename": "23_flashattention_2205_14135",
          "text": "We analyze the IO complexity [ 1] ofFlashAttention , proving that it requires \ud835\udc42\u00b9\ud835\udc412\ud835\udc512\ud835\udc40\u00001\u00baHBM\naccesses where \ud835\udc51is the head dimension and \ud835\udc40is the size of SRAM, as compared to \u03a9\u00b9\ud835\udc41\ud835\udc51\u00b8\ud835\udc412\u00baof standard\nattention. For typical values of \ud835\udc51and\ud835\udc40,FlashAttention requires many times fewer HBM accesses\ncompared to standard attention (up to 9 \u0002fewer, as shown in Fig. 2). Moreover, we provide a lower bound,\nshowing that no exact attention algorithm can asymptotically improve on the number of HBM accesses over\nall SRAM sizes.",
          "score": 0.38462162017822266,
          "rank": 10
        }
      ]
    },
    {
      "query": "Why can cProfile and the CPU-mode autograd profiler give incorrect timings on CUDA code?",
      "results": [
        {
          "doc": "bottleneck",
          "filename": "bottleneck",
          "text": "torch.utils.bottleneck  -h for more usage instructions.\nBecause your script will be profiled, please ensure that it exits in a finite amount of time.\nDue to the asynchronous nature of CUDA kernels, when running against CUDA code, the\ncProfile output and CPU-mode autograd profilers may not show correct timings: the\nreported CPU time reports the amount of time used to launch the kernels but does not\ninclude the time the kernel spent executing on a GPU unless the operation does a",
          "score": 7.062397480010986,
          "rank": 1
        },
        {
          "doc": "bottleneck",
          "filename": "bottleneck",
          "text": "include the time the kernel spent executing on a GPU unless the operation does a\nsynchronize. Ops that do synchronize appear to be extremely expensive under regular\nCPU-mode profilers. In these case where timings are incorrect, the CUDA-mode autograd\nprofiler may be helpful.python  -m  torch .utils .bottleneck  /path /to /source /script .py  [args]\nWarning \u26a0\nWarning \u26a0\nTo analyze traffic and optimize your experience, we serve cookies on this site. By clicking or",
          "score": 5.313739776611328,
          "rank": 2
        },
        {
          "doc": "bottleneck",
          "filename": "bottleneck",
          "text": "Similarly, Intel\u00ae VTune\u2122 Profiler  helps to analyze performance on Intel platforms\nfurther with torch.autograd.profiler.emit_itt() .\nIf you are profiling CUDA code, the first profiler that bottleneck  runs (cProfile) will\ninclude the CUDA startup time (CUDA buffer allocation cost) in its time reporting. This\nshould not matter if your bottlenecks result in code much slower than the CUDA startup\ntime.\nFor more complicated uses of the profilers (like in a multi-GPU case), please see",
          "score": 2.41756010055542,
          "rank": 3
        },
        {
          "doc": "bottleneck",
          "filename": "bottleneck",
          "text": "should first check if your script is CPU-bound (\u201cCPU total time is much greater than CUDA\ntotal time\u201d). If it is CPU-bound, looking at the results of the CPU-mode autograd profiler\nwill help. If on the other hand your script spends most of its time executing on the GPU,\nthen it makes sense to start looking for responsible CUDA operators in the output of the\nCUDA-mode autograd profiler.\nOf course the reality is much more complicated and your script might not be in one of",
          "score": 2.0239150524139404,
          "rank": 4
        },
        {
          "doc": "checkpoint",
          "filename": "checkpoint",
          "text": "Consequently, if any checkpointed functions involve randomness, this may result in\nincorrect gradients. (Note that if CUDA devices are among the devices detected, it will be\nprioritized; otherwise, the first device encountered will be selected.) If there are no CPU-\ntensors, the default device type state (default value is cuda, and it could be set to other\ndevice by DefaultDeviceType) will be saved and restored. However, the logic has no way",
          "score": -0.49958229064941406,
          "rank": 5
        },
        {
          "doc": "bottleneck",
          "filename": "bottleneck",
          "text": "To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or\nnavigating, you agree to allow our usage of cookies. As the current maintainers of this site,\nFacebook\u02bcs Cookies Policy applies. Learn more, including about available controls: Cookies\nPolicy.10/10/25, 3:03 PM torch.utils.bottleneck \u2014 PyTorch 2.8 documentation\nhttps://docs.pytorch.org/docs/stable/bottleneck.html 1/3To decide which (CPU-only-mode or CUDA-mode) autograd profiler output to look at, you",
          "score": -2.546280860900879,
          "rank": 6
        },
        {
          "doc": "24_fsdp_2304_11277",
          "filename": "24_fsdp_2304_11277",
          "text": "fast CPU thread aggressively allocates GPU memory blocks and\ncauses defragmentations. If it is difficult to identify with certainty\nfrom latency measurements or profiled traces, CUDA malloc retry\ncan serve as a helpful indicator, which can be obtained from the\nnum_alloc_retries key in the torch.cuda.memory_stats() dictionary.\nThe experiments conducted with T5 models have demonstrated\nthat the rate limiter technique can greatly benefit training efficiency,611M 2.28B 11.3B",
          "score": -3.398329734802246,
          "rank": 7
        },
        {
          "doc": "extending",
          "filename": "extending",
          "text": "down. The next feature in line will be autograd that will properly create the autograd graph and then\nredispatch down. Finally, we reach the backend kernel for CUDA which will launch the right CUDA\nkernel and return the final result. On the way out, autograd will attach the graph to the output and,\nfinally, autocast will have a chance to do any update it needs on exit.\nOne configuration of the dispatcher is the order in which all these feature and backend keys are",
          "score": -3.5956103801727295,
          "rank": 8
        },
        {
          "doc": "Domino_ Eliminating Communication in LLM Training via Generic Tensor   Slicing a",
          "filename": "Domino_ Eliminating Communication in LLM Training via Generic Tensor   Slicing a",
          "text": "havior to overlap with gradient computation is challenging\nbecause PyTorch automatically generates the gradient com-\nputation graph [69]. To precisely control communication\nstart/end time, our initial attempt to manually implement cus-\ntomized backward pass leads to poor throughput performance\ndue to triggering less efficient kernels than torch.autograd().\nTo tackle this issue, we developed a no-operation module.\nThis module receives the communication handle during the",
          "score": -4.172203063964844,
          "rank": 9
        },
        {
          "doc": "Domino_ Eliminating Communication in LLM Training via Generic Tensor   Slicing a",
          "filename": "Domino_ Eliminating Communication in LLM Training via Generic Tensor   Slicing a",
          "text": "and less on-device memory copy overhead. Taking GPT-3\n13B training as an example, with sequence length of 512 and\nmicro-batch size of 4, we notice significant training iteration\ntime reduction (around 10-15%) if we switch from cudaGraph\noff mode to on mode, which shows the benefits of reducing\nidle time between adjacent compute kernels. On the other\nhand, if we increase the micro-batch size to 16, enabling cud-\naGraph leads to 5-10% longer iteration time than disabling",
          "score": -4.716276168823242,
          "rank": 10
        }
      ]
    },
    {
      "query": "How does auto-differentiation work in these frameworks?",
      "results": [
        {
          "doc": "func",
          "filename": "func",
          "text": "torch.func  has auto-differentiation transforms (grad(f) returns a function that computes\nthe gradient of f), a vectorization/batching transform (vmap(f) returns a function that\ncomputes f over batches of inputs), and others.\nThese function transforms can compose with each other arbitrarily. For example, composing\nvmap(grad(f))  computes a quantity called per-sample-gradients that stock PyTorch cannot\nefficiently compute today.\nWhy composable function transforms?",
          "score": 1.5840634107589722,
          "rank": 1
        },
        {
          "doc": "cmu_llmsys-05-dl-framework",
          "filename": "cmu_llmsys-05-dl-framework",
          "text": "Deep Learning Framework \nDesign\nLei Li\n\u2022Learning parameters of an NN needs gradient calculation\n\u2022Computation Graph\noto perform computation: topological traversal along the DAG\n\u2022Auto Differentiation\nobuilding backward computation graph for gradient calculation\n2Recap3y=x1 +  exp(1.5 * x1 + 2.0 * x2) x1= 3, x2=0.5\n\ud835\udc651\n\ud835\udc653\n\ud835\udc655\ud835\udc641\n=1.5\n*\n+\n\ud835\udc656exp(.)\n\ud835\udc657\ud835\udc652\n\ud835\udc654\ud835\udc642\n=2.0\n*\n+\n\ud835\udc657\n=1\ud835\udc656\ud835\udc654\n\ud835\udc655\nexp(.)\nid\ud835\udc655\u21926 *id\ud835\udc642*\ud835\udc652\n*\n\ud835\udc653id\ud835\udc641*\ud835\udc651\n\ud835\udc651\u21923*+Backward Computation Graph\u2022How to design a deep learning framework\noDesign ideas in TensorFlow",
          "score": 0.9561557769775391,
          "rank": 2
        },
        {
          "doc": "cmu_llmsys-04-autodiff",
          "filename": "cmu_llmsys-04-autodiff",
          "text": "\u2022Auto Differentiation\n4Today\u2019s Topic\n\u2022Neural network layers\noEmbedding (lookup table)\noLinear \noRelu\noAverage pooling\noSoftmax\n5A Simple Feedforward Neural Network\nLinearReluLinearSoftmax\nEmbedding\n\u201cIt is a good movie\u201dAvg\u2022\ud835\udc65\ud835\udc5b,\ud835\udc66\ud835\udc5b are data and label pairs for training\n\u2022Cross entropy\n\u2112(\ud835\udf03)=1\n\ud835\udc41\u2211\n\ud835\udc5b=1\ud835\udc41\n\u2212log\ud835\udc53 (\ud835\udc65\ud835\udc5b)\ud835\udc66\ud835\udc5b\n\u2022Pytorch  CrossEntropyLoss  is implemented as\noNegative Likelihood on logits\n6Training Loss for Classificationloss = nn.CrossEntropyLoss ()\noutput  = loss( input_logits , target_labels )",
          "score": -1.9747225046157837,
          "rank": 3
        },
        {
          "doc": "16_neural_ode_1806_07366",
          "filename": "16_neural_ode_1806_07366",
          "text": "The results above extend those of Stapor et al. (2018, section 2.4.2). An extended version of\nAlgorithm 1 including derivatives w.r.t. t0andt1can be found in Appendix C. Detailed derivations\nare provided in Appendix B. Appendix D provides Python code which computes all derivatives for\nscipy.integrate.odeint by extending the autograd automatic differentiation package. This\ncode also supports all higher-order derivatives. We have since released a PyTorch (Paszke et al.,",
          "score": -2.6133270263671875,
          "rank": 4
        },
        {
          "doc": "16_neural_ode_1806_07366",
          "filename": "16_neural_ode_1806_07366",
          "text": "Automatic differentiation in machine learning: a survey. Journal of machine learning research , 18\n(153):1\u2013153, 2018.\nRianne van den Berg, Leonard Hasenclever, Jakub M Tomczak, and Max Welling. Sylvester\nnormalizing \ufb02ows for variational inference. arXiv preprint arXiv:1803.05649 , 2018.\nBob Carpenter, Matthew D Hoffman, Marcus Brubaker, Daniel Lee, Peter Li, and Michael Betan-\ncourt. The Stan math library: Reverse-mode automatic differentiation in c++. arXiv preprint\narXiv:1509.07164 , 2015.",
          "score": -2.6854562759399414,
          "rank": 5
        },
        {
          "doc": "cmu_llmsys-05-dl-framework",
          "filename": "cmu_llmsys-05-dl-framework",
          "text": "Differentiation (recall previous lecture)\n17Gradient Computation\ntrain_step  = tf.train.GradientDescentOptimizer (0.5).minimize( cross_entropy )\u2022How to design a deep learning framework\noDesign ideas in TensorFlow\n\u25aaAbadi et al., \u201cTensorFlow: A System for Large -Scale Machine Learning\u201d, \nOSDI 2016\noBasic Graph node types in Tensorflow /Pytorch\noOverall design principles\n\u2022Hands -on practice to implement a mini -tensorflow\n\u2022Execution in Tensorflow\n18Today\u2019s Topic\n\u2022All nodes return tensors",
          "score": -2.9027035236358643,
          "rank": 6
        },
        {
          "doc": "func",
          "filename": "func",
          "text": "coverage over PyTorch operations.\nIf you have suggestions on the API or use-cases you\u02bcd like to be covered, please open a\nGitHub issue or reach out. We\u02bcd love to hear about how you\u02bcre using the library.\nWhat are composable function transforms?\nA \u201cfunction transform\u201d is a higher-order function that accepts a numerical function and returns\na new function that computes a different quantity.\ntorch.func  has auto-differentiation transforms (grad(f) returns a function that computes",
          "score": -2.9403443336486816,
          "rank": 7
        },
        {
          "doc": "16_neural_ode_1806_07366",
          "filename": "16_neural_ode_1806_07366",
          "text": "In contrast, by providing a generic vector-Jacobian product, we allow an ODE solver to be trained\nend-to-end with any other differentiable model components. While use of vector-Jacobian products\nfor solving the adjoint method has been explored in optimal control (Andersson, 2013; Andersson\net al., In Press, 2018), we highlight the potential of a general integration of black-box ODE solvers\ninto automatic differentiation (Baydin et al., 2018) for deep learning and generative modeling.\n8 Conclusion",
          "score": -3.6443119049072266,
          "rank": 8
        },
        {
          "doc": "16_neural_ode_1806_07366",
          "filename": "16_neural_ode_1806_07366",
          "text": "Yiping Lu, Aoxiao Zhong, Quanzheng Li, and Bin Dong. Beyond \ufb01nite layer neural networks:\nBridging deep architectures and numerical differential equations. arXiv preprint arXiv:1710.10121 ,\n2017.\nDougal Maclaurin, David Duvenaud, and Ryan P Adams. Autograd: Reverse-mode differentiation of\nnative Python. In ICML workshop on Automatic Machine Learning , 2015.\nHongyuan Mei and Jason M Eisner. The neural Hawkes process: A neurally self-modulating",
          "score": -3.6626224517822266,
          "rank": 9
        },
        {
          "doc": "16_neural_ode_1806_07366",
          "filename": "16_neural_ode_1806_07366",
          "text": "The main technical dif\ufb01culty in training continuous-depth networks is performing reverse-mode\ndifferentiation (also known as backpropagation) through the ODE solver. Differentiating through\nthe operations of the forward pass is straightforward, but incurs a high memory cost and introduces\nadditional numerical error.\nWe treat the ODE solver as a black box, and compute gradients using the adjoint sensitivity\nmethod (Pontryagin et al., 1962). This approach computes gradients by solving a second, aug-",
          "score": -4.256925582885742,
          "rank": 10
        }
      ]
    },
    {
      "query": "What are FlashMLA, DeepEP, and DeepGEMM, and what problems do they each solve?",
      "results": [
        {
          "doc": "cmu_llmsys-13-distributed-training",
          "filename": "cmu_llmsys-13-distributed-training",
          "text": "search for large vocabulary\nconverting sorting to \nparallel operations (max, \nfilter, re -rank)Accelerating decodingRecap: Accelerating Transformer Layers\u2022FlashMLA  (released 2/24/2025)\noFlashMLA  is an efficient MLA decoding kernel for Hopper GPUs, optimized \nfor variable -length sequences serving. \n\u2022DeepEP  (released 2/25/2025)\noa communication library tailored for Mixture -of-Experts ( MoE) and expert \nparallelism (EP). It provides high -throughput and low -latency all -to-all GPU ",
          "score": 2.0442116260528564,
          "rank": 1
        },
        {
          "doc": "cmu_llmsys-13-distributed-training",
          "filename": "cmu_llmsys-13-distributed-training",
          "text": "parallelism (EP). It provides high -throughput and low -latency all -to-all GPU \nkernels, which are also as known as MoE dispatch and combine.\n\u2022DeepGEMM  (released 2/26/2025)\noDeepGEMM  is a library designed for clean and efficient FP8 General Matrix \nMultiplications (GEMMs) with fine -grained scaling\n3Deepseek  opensource libraries\nhttps://github.com/deepseek -ai/ \u2022Overview of large -scale model training\n\u2022Multi -GPU communication\n\u2022Data Parallel Training via AllReduce",
          "score": -0.01608206331729889,
          "rank": 2
        },
        {
          "doc": "cmu_llmsys-11-transformer-acc",
          "filename": "cmu_llmsys-11-transformer-acc",
          "text": "Alternative Model Structures: Linformer, Reformer\nTraining Strategy: Shallow to Deep, Layer Dropout\nE\ufb03cient Computation: LAMB, Quantization, Hardware Optimization\n65\ncode is available at \nhttps://github.com/bytedance/lightseqStay tuned for Deepseek\u2019s FlashMLA\u2022High-performance decoding kernel optimized for Multi-head Latent Attention (MLA) on Hopper GPUs\n\u2022https://github.com/deepseek-ai/FlashMLA\n66Reading for Next\u2022PyTorch Distributed: Experiences on Accelerating Data Parallel Training",
          "score": -2.5054831504821777,
          "rank": 3
        },
        {
          "doc": "Speculative MoE_ Communication Efficient Parallel MoE Inference with   Speculati",
          "filename": "Speculative MoE_ Communication Efficient Parallel MoE Inference with   Speculati",
          "text": "math word problems. arXiv preprint arXiv:2110.14168 ,\n2021.\nDai, D., Deng, C., Zhao, C., Xu, R. X., Gao, H., Chen,\nD., Li, J., Zeng, W., Yu, X., Wu, Y ., Xie, Z., Li, Y . K.,Huang, P., Luo, F., Ruan, C., Sui, Z., and Liang, W.\nDeepseekmoe: Towards ultimate expert specialization in\nmixture-of-experts language models, 2024.\nDeepSeek-AI. Deepseek-v2: A strong, economical, and\nefficient mixture-of-experts language model, 2024a.\nDeepSeek-AI. Deepseek-v3 technical report, 2024b. URL",
          "score": -6.201092720031738,
          "rank": 4
        },
        {
          "doc": "Speculative MoE_ Communication Efficient Parallel MoE Inference with   Speculati",
          "filename": "Speculative MoE_ Communication Efficient Parallel MoE Inference with   Speculati",
          "text": "configurations and report the one DeepSpeed-MoE performs best.\nprevailing open-source LLM inference framework, adopt-\ning a bunch of optimization including but not limited to\ncontinuous batching, paged attention, flash attention, radix-\nattention, advanced quantization, etc. SGLang declares op-\ntimizations for MoE models with high-performance, fused\ntriton kernels (OpenAI) to implement TP-based paralleliza-\ntion for both attention and expert layers.\nMetrics: Throughput is the number of tokens (tokens/s)",
          "score": -7.108766078948975,
          "rank": 5
        },
        {
          "doc": "vTensor_ Flexible Virtual Tensor Management for Efficient LLM Serving",
          "filename": "vTensor_ Flexible Virtual Tensor Management for Efficient LLM Serving",
          "text": "Wang, Zihui Gu, Zilin Li, and Ziwei Xie. Deepseek-v2:\nA strong, economical, and efficient mixture-of-experts\nlanguage model, 2024.\n[18] Hugging Face. Text Generation Inference. https:\n//huggingface.co/text-generation-inference ,\n2024.\n[19] Hao Fei, Han Zhang, Bin Wang, Lizi Liao, Qian Liu,\nand Erik Cambria. Empathyear: An open-source avatar\nmultimodal empathetic chatbot, 2024.\n[20] flashInfer.ai. flashinfer. https://github.com/\nflashinfer-ai/flashinfer , 2023.",
          "score": -7.51978063583374,
          "rank": 6
        },
        {
          "doc": "DeepCompile_ A Compiler-Driven Approach to Optimizing Distributed Deep   Learnin",
          "filename": "DeepCompile_ A Compiler-Driven Approach to Optimizing Distributed Deep   Learnin",
          "text": "8\u00d77B MoE models. DeepCompile achieves up to 1.28 \u00d7and\n1.54\u00d7performance improvements over ZeRO-3 and FSDP\nbaselines, respectively, and up to a 7.01 \u00d7throughput increase\nin settings with limited GPU resources, using offloading.\nKeywords\ndeep learning, distributed training\n\u2217Work done while at Microsoft. Now at AMD.1 Introduction\nThe rapid growth of deep learning has led to the emergence\nof increasingly large models. Modern architectures often\ncontain billions of parameters, resulting in immense com-",
          "score": -7.648191452026367,
          "rank": 7
        },
        {
          "doc": "Speculative MoE_ Communication Efficient Parallel MoE Inference with   Speculati",
          "filename": "Speculative MoE_ Communication Efficient Parallel MoE Inference with   Speculati",
          "text": "Parallel) and DP (Data Parallelism). However,\nour analysis shows DeepSpeed-MoE\u2019s inference\nefficiency is largely bottlenecked by EP, which is\nimplemented with costly all-to-all collectives to\nroute token activation. Our work aims to boost\nDeepSpeed-MoE by strategically reducing EP\u2019s\ncommunication overhead with a technique named\nSpeculative MoE. Speculative MoE has two spec-\nulative parallelization schemes, speculative token\nshuffling and speculative expert grouping, which",
          "score": -7.7520856857299805,
          "rank": 8
        },
        {
          "doc": "Speculative MoE_ Communication Efficient Parallel MoE Inference with   Speculati",
          "filename": "Speculative MoE_ Communication Efficient Parallel MoE Inference with   Speculati",
          "text": "(FFN) modules by dynamically selecting the best backend\nimplementation of GroupGemm and DenseGemm.\nSpeculative MoE Inference - offloading and prefetch-\ning.Existing work on speculative MoE inference primarily\nfocuses on prefetching offloaded experts and strategically\nsaving GPU memories (Yi et al., 2023; Xue et al., 2024;\nZhong et al., 2024), though offloading can extend infer-\nence latency and is rarely used in latency-critical serving\nscenarios. In contrast, s-MoE focuses on the speculative",
          "score": -8.620702743530273,
          "rank": 9
        },
        {
          "doc": "Speculative MoE_ Communication Efficient Parallel MoE Inference with   Speculati",
          "filename": "Speculative MoE_ Communication Efficient Parallel MoE Inference with   Speculati",
          "text": "DBRX (Mosaic-Research), Arctic (arc) DeepSeek-MoE-\n16B (Dai et al., 2024), DeepSeek V2 (DeepSeek-AI, 2024a),\nV3 (DeepSeek-AI, 2024b), R1 (DeepSeek-AI, 2025), Grok-\n1 (X-AI), QWen-MoE (Qwen-Team, 2024), etc.\nNevertheless, at inference time, massive MoEs models\u2019\nvoluminous expert and attention blocks still require huge\namounts of GPU/NPU1cores, memory and bandwidth to\ncompute, stash and load expert parameters for the forward\npass. Existing MoE frameworks achieve inference scala-",
          "score": -8.777458190917969,
          "rank": 10
        }
      ]
    },
    {
      "query": "What problem does the Model Context Protocol (MCP) solve?",
      "results": [
        {
          "doc": "yiying_llm-perf",
          "filename": "yiying_llm-perf",
          "text": "https://lilianweng.github.io/posts/2023-06-23-agent/\nMCP (Model Context Protocol)\u25cfConnecting (N) LLMs to (M) external tools/resources used to be a NxM problem \u25cfMCP standardizes the LLM-tool communication into a N->1->M process \u25cfBuild with a client-server model \u25cfMCP client: the agent that needs to call tool/data \u25cfMCP server: a service to expose external tools and data sources",
          "score": 5.85311222076416,
          "rank": 1
        },
        {
          "doc": "yiying_ai-agents",
          "filename": "yiying_ai-agents",
          "text": "LLMs + training for tool use: ToolformerMCP (Model Context Protocol)\u25cfConnecting (N) LLMs to (M) external tools/resources used to be a NxM problem \u25cfMCP standardizes the LLM-tool communication into a N->1->M process \u25cfBuild with a client-server model \u25cfMCP client: the agent that needs to call tool/data \u25cfMCP server: a service to expose external tools and data sources",
          "score": 3.1608967781066895,
          "rank": 2
        },
        {
          "doc": "MoE Parallel Folding_ Heterogeneous Parallelism Mappings for Efficient   Large-S",
          "filename": "MoE Parallel Folding_ Heterogeneous Parallelism Mappings for Efficient   Large-S",
          "text": "communication overhead. And the largest model Llama3-8x70B could not be trained using only\nTP+EP due to memory constraints. (4) MCore framework leverages pipeline parallelism (PP) in\naddition to TP, EP, and DP, achieves a better balance between communication and computation. This\nresults in higher MFU values, reaching 46.3% on Mixtral-8x22B and 35.3% on Qwen-2-57B. By\neffectively partitioning the model across pipeline stages, MCore reduces the memory footprint per",
          "score": -0.42497485876083374,
          "rank": 3
        },
        {
          "doc": "MoE Parallel Folding_ Heterogeneous Parallelism Mappings for Efficient   Large-S",
          "filename": "MoE Parallel Folding_ Heterogeneous Parallelism Mappings for Efficient   Large-S",
          "text": "superior performance. (2) ETP in the MoE layer introduces substantially higher communication\noverhead compared to EP, with this effect being particularly pronounced in fine-grained MoE models.\n(3) Fine-grained MoE models exhibit notably lower computation-to-communication ratios. When\nETPxEP exceeds 8, necessitating inter-node communication, communication overhead dominates,\nFigure 4: Context-scaling experiments by increasing context length and number of GPUs up to 128K\nand 1024.\n9(a) Mixtral 8x22B model",
          "score": -2.7999556064605713,
          "rank": 4
        },
        {
          "doc": "yiying_llm-perf",
          "filename": "yiying_llm-perf",
          "text": "AI Agent/Workflow Frameworks\u25cfFrameworks initially proposed to standardize AI workflows, provide some out-of-box design patterns and abstractions \u25cfSome examples \u25cfLangChain: Came out the earliest, probably the most popular and hardest to use \u25cfLlamaIndex: Good RAG support \u25cfCrewAI and Camel: multi-agent framework for more complex tasks \u25cfBut a lot of unnecessary, added complexity for agents, harder to customize \u25cfMy experience of what\u2019s the easiest and sufficient for many tasks \u25cfNo framework (pure Python) \u25cfNo MCP (can just write your own functions or hooks) \u25cfNo A2A (no need for multi-agent)What is AI Agent Infra?\u25cfAgent testing and evaluation \u25cbUnit + e2e test, metrics, benchmarks, human-in-the-loop \u25cfAgent autotuning and optimization \u25cbAutomated prompt tuning, model selection, tool selection, workflow optimization \u25cfAgent hosting \u25cbServerless or long-running?  \u25cbStateful or stateless? \u25cfTooling, memory, dataDemo Time: Eigent Computer-Use Agent performing a Discord summarization taskOutline\u25cfTransformer primer \u25cfIntroduction oriented for LLM infra (perf problems), not the theory \u25cfLLM performanceSelf Attention",
          "score": -4.342851638793945,
          "rank": 5
        },
        {
          "doc": "MoNTA_ Accelerating Mixture-of-Experts Training with   Network-Traffc-Aware Para",
          "filename": "MoNTA_ Accelerating Mixture-of-Experts Training with   Network-Traffc-Aware Para",
          "text": "(b)Backward Computation.\nFigure 11: Communication Conflict\nmunication, conflicts caused by asynchronous communica-\ntion between EP and DP/PP can be reduced by setting priori-\nties for different communication primitives: EP\u00bfPP\u00bfCP\u00bfDP,\nensuring the efficiency of EP AllToAll communication.\nCluster Expansion\nThis section analyzes the cluster expansion handling meth-\nods for long context MoE training in MoNTA implemen-\ntations. In the training of the MoE model, it typically in-",
          "score": -4.534677505493164,
          "rank": 6
        },
        {
          "doc": "yiying_Overview",
          "filename": "yiying_Overview",
          "text": "Equipping Agents: The Power of Tooling\u2022Tools: external functions, APIs, or even another (utility) agent (e.g., MCP server)\n\u2022Agents can often decide when to call tools and what tools to call\n\u2022Common tools\n\u2022Web search + crawling\n\u2022Browser\n\u2022Social media, email hooks\n\u2022Code + CLI executionAgent Memory: Knowledge, History, State\u2022LLMs only have short-term memory (i.e., context window)\n\u2022Many agents needs long-term memory and/or internal/external knowledge",
          "score": -4.673484802246094,
          "rank": 7
        },
        {
          "doc": "MoE Parallel Folding_ Heterogeneous Parallelism Mappings for Efficient   Large-S",
          "filename": "MoE Parallel Folding_ Heterogeneous Parallelism Mappings for Efficient   Large-S",
          "text": "effectively partitioning the model across pipeline stages, MCore reduces the memory footprint per\nGPU and overlaps communication with computation more efficiently. However, the coupling of\nparallelism strategies between the Attention and MoE layers renders the mappings sub-optimal for\nMoE models. (5) MCore with MoE Parallel Folding: further enhances training efficiency, achieving\nthe highest MFU values across all models: 49.3% for Mixtral-8x22B, 41.6% on Llama3-8x70B,",
          "score": -4.842418670654297,
          "rank": 8
        },
        {
          "doc": "MoNTA_ Accelerating Mixture-of-Experts Training with   Network-Traffc-Aware Para",
          "filename": "MoNTA_ Accelerating Mixture-of-Experts Training with   Network-Traffc-Aware Para",
          "text": "tations. In the training of the MoE model, it typically in-\nvolves multiple training steps with Context lengths ranging\nfrom 4K to 128K, and even up to 1M tokens. This paper\nproposes a distributed parallel training extension method for\nLong Context mixture of expert models. Based on cluster re-\nsource parameters, model parameters, and Context length, a\nstrategy for expanding expert model distributed parallelism\nis generated by combining Expert Parallelism and Context\nParallelism.",
          "score": -5.187268257141113,
          "rank": 9
        },
        {
          "doc": "yiying_ai-agents",
          "filename": "yiying_ai-agents",
          "text": "AI Agent/Workflow Frameworks\u25cfFrameworks initially proposed to standardize AI workflows, provide some out-of-box design patterns and abstractions \u25cfSome examples \u25cfLangChain: Came out the earliest, probably the most popular and hardest to use \u25cfLlamaIndex: Good RAG support \u25cfCrewAI and Camel: multi-agent framework for more complex tasks \u25cfBut a lot of unnecessary, added complexity for agents, harder to customize \u25cfMy experience of what\u2019s the easiest and sufficient for many tasks \u25cfNo framework (pure Python) \u25cfNo MCP (can just write your own functions or hooks) \u25cfNo A2A (no need for multi-agent)What is AI Agent Infra?\u25cfAgent testing and evaluation \u25cbUnit + e2e test, metrics, benchmarks, human-in-the-loop \u25cfAgent autotuning and optimization \u25cbAutomated prompt tuning, model selection, tool selection, workflow optimization \u25cfAgent hosting \u25cbServerless or long-running?  \u25cbStateful or stateless? \u25cfTooling, memory, dataDemo Time: Eigent Computer-Use Agent performing a Discord summarization task",
          "score": -5.540253639221191,
          "rank": 10
        }
      ]
    },
    {
      "query": "What are the three core components of the TinyServe system?",
      "results": [
        {
          "doc": "TinyServe_ Query-Aware Cache Selection for Efficient LLM Serving",
          "filename": "TinyServe_ Query-Aware Cache Selection for Efficient LLM Serving",
          "text": "3.1 System Overview: TinyServe\nTinyServe is a lightweight serving framework designed for serving\ntiny language models under tight memory and latency constraints.\nRather than acting as a benchmarking tool, TinyServe serves as a\nreal-time serving environment that enables sparsity-aware atten-\ntion, modular token selection, and efficient KV-cache reuse.\nThe system is organized around three core components:\n(1)Query-Aware KV Retriever: Dynamically selects relevant",
          "score": 8.581113815307617,
          "rank": 1
        },
        {
          "doc": "TinyServe_ Query-Aware Cache Selection for Efficient LLM Serving",
          "filename": "TinyServe_ Query-Aware Cache Selection for Efficient LLM Serving",
          "text": "LLM serving\u2014such as KV cache saturation and decode-time la-\ntency\u2014with modular support for token selection, cache sparsity,\nfused attention kernels, and training optimization.\nAt the core of TinyServe is a query-aware page selection mech-\nanism that approximates attention relevance using bounding-box\nmetadata, enabling selective KV access with minimal overhead.",
          "score": 4.609828472137451,
          "rank": 2
        },
        {
          "doc": "TinyServe_ Query-Aware Cache Selection for Efficient LLM Serving",
          "filename": "TinyServe_ Query-Aware Cache Selection for Efficient LLM Serving",
          "text": "duces TinyServe , a lightweight efficient serving framework using\nsmall LLMs to reproduce the key serving stack components\u2014streamingattention, dynamic batching, and quantized decoding\u2014under re-\nalistic serving scenarios. This enables fast hypothesis testing of\narchitectural changes with minimal compute cost.\nWe also note related work on Memory-Keyed Attention (MKA) [ 21],\nwhich extends attention mechanisms for long-context reasoning,\nand data-centric safety frameworks [ 13], which highlight broader",
          "score": 3.7817511558532715,
          "rank": 3
        },
        {
          "doc": "TinyServe_ Query-Aware Cache Selection for Efficient LLM Serving",
          "filename": "TinyServe_ Query-Aware Cache Selection for Efficient LLM Serving",
          "text": "pages); - Reduced HBM bandwidth pressure.\nSystem Implication. TinyServe enables dynamic query-aware\nsparsity without requiring architectural retraining. The modular\nimplementation integrates directly into TinyServe\u2019s kernel loop\nand allows hardware-sensitive scheduling: e.g., keeping hot pages\nin shared memory or limiting K to match tensor core granularity.\nThe kernel design for TinyServe can be found at algorithm 1.\n3.6 Memory Efficiency Analysis",
          "score": 2.3611176013946533,
          "rank": 4
        },
        {
          "doc": "TinyServe_ Query-Aware Cache Selection for Efficient LLM Serving",
          "filename": "TinyServe_ Query-Aware Cache Selection for Efficient LLM Serving",
          "text": "often forced to treat models as black boxes, unable to validate hy-\npotheses or perform design iteration without access to large GPU\nclusters.\nTinyServe: Large-Scale Serving at Small Scale. We introduce\nTinyServe , a lightweight serving framework that enables detailed\nanalysis of LLM training and inference behavior using tiny models\n(e.g., 125M\u2013350M parameters). TinyServe replicates core compo-\nnents of LLM serving\u2014streaming decoding, KV cache management,",
          "score": 1.5357692241668701,
          "rank": 5
        },
        {
          "doc": "TinyServe_ Query-Aware Cache Selection for Efficient LLM Serving",
          "filename": "TinyServe_ Query-Aware Cache Selection for Efficient LLM Serving",
          "text": "Serve , a lightweight and extensible serving system for deploying\ntiny LLMs (e.g., TinyLLaMA, GPT2-345M) with support for struc-\ntured KV sparsity, plugin-based token selection, and hardware-\nefficient attention kernels. Unlike prior simulation frameworks,\nTinyServe executes real-time decoding with configurable sparsity\nstrategies and fine-grained instrumentation.\nTo reduce decoding cost, we introduce a query-aware page selec-\ntionmechanism that leverages bounding-box metadata to estimate",
          "score": 1.4308445453643799,
          "rank": 6
        },
        {
          "doc": "TinyServe_ Query-Aware Cache Selection for Efficient LLM Serving",
          "filename": "TinyServe_ Query-Aware Cache Selection for Efficient LLM Serving",
          "text": "inefficiencies.\nBeyond LLMs, graph learning acceleration systems like Graph-\nSnapShot [ 11,12,15] explore caching and retrieval strategies to\noptimize large-scale graph training. These works emphasize that\ncarefully designed synthetic stressors and caching strategies are\nessential for both graph-based and language-based workloads, re-\ninforcing the importance of lightweight analysis frameworks.\n3 Methodology\n3.1 System Overview: TinyServe\nTinyServe is a lightweight serving framework designed for serving",
          "score": 0.7609111070632935,
          "rank": 7
        },
        {
          "doc": "TinyServe_ Query-Aware Cache Selection for Efficient LLM Serving",
          "filename": "TinyServe_ Query-Aware Cache Selection for Efficient LLM Serving",
          "text": "code=2k). TinyServe maintains higher hit rate and fewer\ntoken evictions.\nLLMs, TinyServe allows efficient and interpretable serving profil-\ning, supporting system-level research without relying on full-scale\ndeployments.\n5 Conclusion\nWe introduced TinyServe , a lightweight and extensible serving\nsystem for efficient inference and training acceleration with tiny\nlanguage models. TinyServe bridges system-level bottlenecks in\nLLM serving\u2014such as KV cache saturation and decode-time la-",
          "score": 0.215264692902565,
          "rank": 8
        },
        {
          "doc": "TinyServe_ Query-Aware Cache Selection for Efficient LLM Serving",
          "filename": "TinyServe_ Query-Aware Cache Selection for Efficient LLM Serving",
          "text": "port for training acceleration and fine-grained profiling. For training\nscenarios, TinyServe implements gradient-aware memory manage-\nment that selectively retains KV cache entries based on gradient\nmagnitude during backpropagation. This approach reduces mem-\nory footprint by up to 40% during fine-tuning while maintaining\ntraining stability.\nThe profiling system integrates layer-wise performance monitor-\ningwith microsecond precision, tracking attention patterns, mem-",
          "score": -0.0995793342590332,
          "rank": 9
        },
        {
          "doc": "TinyServe_ Query-Aware Cache Selection for Efficient LLM Serving",
          "filename": "TinyServe_ Query-Aware Cache Selection for Efficient LLM Serving",
          "text": "The kernel design for TinyServe can be found at algorithm 1.\n3.6 Memory Efficiency Analysis\nTo quantify memory access savings under query-aware sparsity, we\nconstruct a probabilistic cost model that accounts for (1) metadata\noverhead, (2) selected KV tokens, and (3) cross-step reuse. This anal-\nysis provides theoretical bounds on the performance improvements\nachievable through our approach.\nLet: -\ud835\udc3f: total cache length (tokens); - \ud835\udc46: page size (tokens per",
          "score": -0.2723541557788849,
          "rank": 10
        }
      ]
    },
    {
      "query": "What is NVIDIA GPU Confidential Computing (CC) and how does it secure communication?",
      "results": [
        {
          "doc": "PipeLLM_ Fast and Confidential Large Language Model Services with   Speculative ",
          "filename": "PipeLLM_ Fast and Confidential Large Language Model Services with   Speculative ",
          "text": "their memory, this encryption is separate from that used\nby NVIDIA CC. NVIDIA CC ensures the confidentiality and\nintegrity of communication between a CVM and a GPU via\nAES-GCM encryption [ 15]. A critical component of AES-\nGCM is the Initialization Vector (IV), a unique, non-repeating\nnumber (a nonce) required for each encryption session. As\nwe will show later (\u00a74.1), managing IVs presents a significant\nchallenge.\nFigure 1 illustrates the workflow of data transfers of the",
          "score": 6.115840911865234,
          "rank": 1
        },
        {
          "doc": "PipeLLM_ Fast and Confidential Large Language Model Services with   Speculative ",
          "filename": "PipeLLM_ Fast and Confidential Large Language Model Services with   Speculative ",
          "text": "GPU to protect sensitive data and models from unauthorized\naccess. Moreover, the data communication between the CVM\nand the GPU enclave is encrypted, further reinforcing the\nsecurity of I/O operations.\nAlthough GPU confidential computing effectively enhances\nsecurity for traditional small-scale AI models, it significantly\nundermines the performance of LLMs in throughput and\nlatency. Our comprehensive experiments on NVIDIA H100\nGPUs reveal that the GPU enclave can incur up to a 52.8%",
          "score": 6.101676940917969,
          "rank": 2
        },
        {
          "doc": "PipeLLM_ Fast and Confidential Large Language Model Services with   Speculative ",
          "filename": "PipeLLM_ Fast and Confidential Large Language Model Services with   Speculative ",
          "text": "denote ciphertexts moved from the GPU back to the CPU. After the\ntransfers, the current IV of CPU and GPU is 3 and 7, respectively.\nread/write GPU memory and modify the control flow. Hard-\nware GPU confidential computing has low performance over-\nhead and is backward-compatible with existing applications.\nThis paper focuses on studying hardware GPU confidential\ncomputing.\nA closer look at NVIDIA CC. Although CVMs encrypt\ntheir memory, this encryption is separate from that used",
          "score": 4.933990478515625,
          "rank": 3
        },
        {
          "doc": "PipeLLM_ Fast and Confidential Large Language Model Services with   Speculative ",
          "filename": "PipeLLM_ Fast and Confidential Large Language Model Services with   Speculative ",
          "text": "tion of how NVIDIA Confidential Computing and PipeLLM\nexecute it.\nCPU\nGPU\nGPUCPUDecrypt Encrypt\nSaved\nTime1c 3c\n1t1c\n1t\nPipeLLMNVIDIA\nCC# 1. Swap\u00a0 from GPU\n# - PCIe transfer\n# - CPU decryption\ndata = swap_from_gpu()\n# 2. GPU computation\nllm.compute()\n# 3. Load back to GPU\n# - CPU encryption\n# - PCIe transfer\nload_back_to_gpu(data)1t\n1c\n2\n3c\n3tTime\n3t3t3c\n2\nCompute2\nCompute\nFor transparency, NVIDIA Confidential Computing per-\nforms on-the-fly encryption and decryption (indicated by",
          "score": 4.7963175773620605,
          "rank": 4
        },
        {
          "doc": "PipeLLM_ Fast and Confidential Large Language Model Services with   Speculative ",
          "filename": "PipeLLM_ Fast and Confidential Large Language Model Services with   Speculative ",
          "text": "of NVIDIA Confidential Computing, while preserving its (1)\nsecurity guarantees and (2) user transparency. By user trans-\nparency, we mean that PipeLLM applies to non-modified\nLLM applications, including LLM models, deep learning\nframeworks, and any other supporting code and data. Next,\nwe elaborate on PipeLLM\u2019s threat model.\nThreat model. NVIDIA Confidential Computing aims at\nprotecting the confidentiality and integrity of applications\nrunning on GPUs; for LLM applications, these are the model",
          "score": 4.627625942230225,
          "rank": 5
        },
        {
          "doc": "PipeLLM_ Fast and Confidential Large Language Model Services with   Speculative ",
          "filename": "PipeLLM_ Fast and Confidential Large Language Model Services with   Speculative ",
          "text": "commands, but this adds substantial performance overhead\ndue to extra runtime checks for indirect memory access,\nheavily used in systems like vLLM [25].\nUnlike software-based solutions, NVIDIA Confidential\nComputing relies on hardware: NVIDIA H100 GPU is the\nfirst commercial implementation with confidential comput-\ning capability [ 15]. Working with CVMs, H100 could build a\nGPU enclave, allowing users to have exclusive control over\nthe GPU and rejecting any access from the host, such as",
          "score": 4.087099075317383,
          "rank": 6
        },
        {
          "doc": "PipeLLM_ Fast and Confidential Large Language Model Services with   Speculative ",
          "filename": "PipeLLM_ Fast and Confidential Large Language Model Services with   Speculative ",
          "text": "guarantees of NVIDIA Confidential Computing.\nAlthough PipeLLM does not compromise confidentiality\nor integrity, its mis-speculation introduces side channels in\nNOP transfers compared to NVIDIA Confidential Comput-\ning, including (1) attackers can detect if the LLM system is\ncurrently swapping by observing NOPs, and (2) attackers\ncould profile the frequency of prediction failures, potentially\nrevealing the swapping patterns of applications. The security",
          "score": 3.955214023590088,
          "rank": 7
        },
        {
          "doc": "PipeLLM_ Fast and Confidential Large Language Model Services with   Speculative ",
          "filename": "PipeLLM_ Fast and Confidential Large Language Model Services with   Speculative ",
          "text": "encrypted version on the CPU. This would allow the en-\ncrypted data to be transferred directly to the GPU during\nloading, thereby eliminating encryption overhead. While this\napproach improves performance, implementing it naively\ncompromises security guarantees. For example, reusing en-\ncrypted data enables attackers to identify data that matches\na previous transfer; more critically, it could make the system\nvulnerable to replay attacks [35].\nCurrently, NVIDIA Confidential Computing uses the AES-",
          "score": 3.8148505687713623,
          "rank": 8
        },
        {
          "doc": "PipeLLM_ Fast and Confidential Large Language Model Services with   Speculative ",
          "filename": "PipeLLM_ Fast and Confidential Large Language Model Services with   Speculative ",
          "text": "their memory to guarantee privacy. CVMs generally incur\nminimal performance overhead, averaging around 4% [ 24].\nI/O operations however may introduce significant perfor-\nmance overhead due to data copying and encryption [27].\nConfidential Computing (CC) on GPUs. Beyond CPU-\nbased CVMs, confidential computing on GPUs secures GPU\ncomputations such as LLM serving and training. Soter [40],\ndesigned for edge computing, uses CPU-side confidential\ncomputing to eliminate the trust on GPU hardware. It em-",
          "score": 3.680403232574463,
          "rank": 9
        },
        {
          "doc": "PipeLLM_ Fast and Confidential Large Language Model Services with   Speculative ",
          "filename": "PipeLLM_ Fast and Confidential Large Language Model Services with   Speculative ",
          "text": "dential computing feature. The second baseline, denoted as\n\u201cCC\u201d (Confidential Computing), showcases the performance\nof the current NVIDIA Confidential Computing framework\nspecifically applied in LLM inference.\n9 0 10 20 30 40\n32/128 256/32Throughput (tokens/s)\nInput/Output Token Length(a)FlexGen OPT-66B\n 0 5 10 15 20\n32/128 256/32\nInput/Output Token Lengthw/o CC CC PipeLLM (b)FlexGen OPT-175B\n 0 1 2 3\nOPT-30B OPT-13BThroughput (sequences/s)\nModel (c)PEFT",
          "score": 3.226597785949707,
          "rank": 10
        }
      ]
    },
    {
      "query": "Why can\u2019t you perform data-dependent operations on meta tensors?",
      "results": [
        {
          "doc": "meta",
          "filename": "meta",
          "text": "represent the actual tensors. Because meta tensors do not have real data, you cannot perform\ndata-dependent operations like torch.nonzero()  or item() . In some cases, not all device\ntypes (e.g., CPU and CUDA) have exactly the same output metadata for an operation; we\ntypically prefer representing the CUDA behavior faithfully in this situation.\nAlthough in principle meta tensor computation should always be faster than an equivalent",
          "score": 8.517169952392578,
          "rank": 1
        },
        {
          "doc": "meta",
          "filename": "meta",
          "text": "make transformations on the model before you load the actual data.\nMost operations can be performed on meta tensors, producing new meta tensors that describe\nwhat the result would have been if you performed the operation on a real tensor. You can use\nthis to perform abstract analysis without needing to spend time on compute or space to\nrepresent the actual tensors. Because meta tensors do not have real data, you cannot perform",
          "score": 6.987880706787109,
          "rank": 2
        },
        {
          "doc": "meta",
          "filename": "meta",
          "text": "a device for initialization:\nYou cannot convert a meta tensor directly to a CPU/CUDA tensor, because the meta tensor stores\nno data and we do not know what the correct data values for your new tensor are:\nUse a factory function like torch.empty_like()  to explicitly specify how you would like the\nmissing data to be filled in.\nNN modules have a convenience method torch.nn.Module.to_empty()  that allows you to move",
          "score": 3.9004530906677246,
          "rank": 3
        },
        {
          "doc": "meta",
          "filename": "meta",
          "text": "Meta device\nCreated On: Jun 17, 2025 | Last Updated On: Jun 17, 2025\nThe \u201cmeta\u201d device is an abstract device which denotes a tensor which records only metadata, but\nno actual data. Meta tensors have two primary use cases:\nModels can be loaded on the meta device, allowing you to load a representation of the model\nwithout actually loading the actual parameters into memory. This can be helpful if you need to\nmake transformations on the model before you load the actual data.",
          "score": 1.6692577600479126,
          "rank": 4
        },
        {
          "doc": "meta",
          "filename": "meta",
          "text": "Although in principle meta tensor computation should always be faster than an equivalent\nCPU/CUDA computation, many meta tensor implementations are implemented in Python\nand have not been ported to C++ for speed, so you may find that you get lower absolute\nframework latency with small CPU tensors.\nIdioms for working with meta tensors\nAn object can be loaded with torch.load()  onto meta device by specifying\nmap_location='meta' :\n>>> torch.save(torch.randn(2), 'foo.pt' )",
          "score": -0.15679049491882324,
          "rank": 5
        },
        {
          "doc": "meta",
          "filename": "meta",
          "text": "Policy.10/10/25, 3:03 PM Meta device \u2014 PyTorch 2.8 documentation\nhttps://docs.pytorch.org/docs/stable/meta.html 1/4If you have some arbitrary code which performs some tensor construction without explicitly\nspecifying a device, you can override it to instead construct on meta device by using the\ntorch.device()  context manager:\nThis is especially helpful NN module construction, where you often are not able to explicitly pass in\na device for initialization:",
          "score": -2.635075092315674,
          "rank": 6
        },
        {
          "doc": "torch",
          "filename": "torch",
          "text": "torch\nCreated On: Dec 23, 2016 | Last Updated On: Mar 10, 2025\nThe torch package contains data structures for multi-dimensional tensors and defines mathematic\noperations over these tensors. Additionally, it provides many utilities for efficient serialization of\nTensors and arbitrary types, and other useful utilities.\nIt has a CUDA counterpart, that enables you to run your tensor computations on an NVIDIA GPU\nwith compute capability >= 3.0.\nTensors\nis_tensorReturns True if obj is a PyTorch tensor.",
          "score": -3.030456066131592,
          "rank": 7
        },
        {
          "doc": "futures",
          "filename": "futures",
          "text": "has thrown an error, this value() method will also throw an error.\nReturn type:\nT\nwait ( )\nBlock until the value of this Future is ready.\nIf the value contains tensors that reside on GPUs, then an additional synchronization is\nperformed with the kernels (executing on the device) which may be asynchronously\npopulating those tensors. Such sync is non-blocking, which means that wait() will inser\nthe necessary instructions in the current streams to ensure that further operations",
          "score": -4.395951271057129,
          "rank": 8
        },
        {
          "doc": "A Survey of LLM __times_ DATA",
          "filename": "A Survey of LLM __times_ DATA",
          "text": "dimensional structures can be partitioned and processed in\nparallel, making them highly suitable for large-scale computa-\ntion. For example, tf.data.Dataset [43] can organize various\nraw data types (e.g., images, text) into a unified tensor format,\nready for direct use by models. However, tensor formats, due\nto their dense multi-dimensional storage, incur large storage\noverhead and offer poor readability, and are typically adopted\nonly in model training.\nModel Data Format. Model storage formats need to pay",
          "score": -4.5223188400268555,
          "rank": 9
        },
        {
          "doc": "MCR-DL_ Mix-and-Match Communication Runtime for Deep Learning",
          "filename": "MCR-DL_ Mix-and-Match Communication Runtime for Deep Learning",
          "text": "with minimal changes among backends and operations. These\noptimizations can be applied to incoming messages with only\na few lines of Python code before routing the operation to its\nrespective C++ backend.\nThere are two parameters for Tensor Fusion: the maximum\nfusion buffer size Band the maximum time Tto wait\nfor that fusion buffer to \ufb01ll with small tensors. MCR-DL\nintroduces a small optimization for Tensor Fusion, where if\nthe Fusion buffer does not reach Bbefore T(and therefore",
          "score": -4.643242835998535,
          "rank": 10
        }
      ]
    }
  ]
}