filename,query,gold-text,gold-doc
"['23_flashattention_2205_14135.txt', 'attention.txt']",What optimizations does FlashAttention introduce compared to standard attention kernels implemented in PyTorch?,"('FMHA targets BERT models, it only supports head dimension 64, and only runs on A100 GPUs. FMHA fuses the attention computation dropoutÂ¹softmaxÂ¹maskÂ¹QK>ÂºÂºÂºVinto one CUDA kernel. In the forward pass, it stores the attention matrix softmaxÂ¹maskÂ¹QKð‘‡ÂºÂºto HBM to be used in gradient computation. As a result, it does not oï¬€er substantial memory saving (though for shorter sequences memory footprint is often not a primary concern).', 'This module contains functions and classes that alter the behavior of torch.nn.functional.scaled_dot_product_attention', 'flex_attention This module implements the user facing API for flex_attention in PyTorch. bias Defines bias subclasses that work with scaled_dot_product_attention')",
"['cmu_llmsys-16-quantization.txt', 'cmu_llmsys-17-quantization2.txt']",What are the trade-offs between simple post-training quantization and GPTQ?,"('8CUDA APIs for Half Precisionâ€¢Using lower precision oconverting parameters from FP32 to INT8 or INT4 operform all computation in lower prevision. Reduce model accuracy: oLoss of Precision âž” accumulate quantization noise oRange mismatch âž” values are clipped and lead to information loss oQuantization error âž” rounding errors 9Direct Quantization Approachâ€¢Absmax  quant Zero -point quant 10Quantize a number', 'mixed strategy for 8 -bit quantization and 16 -bit (for outliers)2Recap of Basic Quantization Methodsâ€¢Absmax  quant Zero -point quant 4Quantize a Number to Int8 5GPTQ scale to GPT -size LLMs maintain accuracyâ€¢Revisit  layer -wise quantization of weight matrices argmin', 'layer -wise quantization + compensation for errors + precompute oaccurately compress some of the largest publicly -available models down to 3 and 4 bits, and bring end -to-end speedups')",
"['yiying_training-1.txt', 'yiying_training-2.txt']",What are the challenges of theoretical distributed training (PipeDream) and what is the scaling efficiency of distributed real-world Trainium performance (HLAT)?,"('Challenge 1: Stage Partitioningâ€¢How to partition model layers into the stages evenly? â€¢Throughput depends on the slowest stage in pipeline â€¢Solution: â€¢Proï¬le layer-wise perf and comm perf â€¢Allows a stage to be replicated (DP) â€¢Uses dynamic programming to ï¬nd optimal partition and layer replication', 'Challenge 2: Work Schedulingâ€¢How to schedule forward and backward computation on a worker? â€¢Solution: 1F1B-RR â€¢Run one forward and one backward â€¢Round-robin across replicated stages', 'Challenge 3: Weight Versioningâ€¢How to ensure the same minibatch uses the same weight version across workers for forward and backward? â€¢Otherwise computation will be far oï¬€ and training not able to converge â€¢Solution: Store multiple weight versions so that the backward and forward of the same minibatch â€¢Weights across workers can be diï¬€erent!', 'The scaling efficiency for Llama2 7b:â€¢87% on 32 nodes. MFU = 33.5%â€¢72% on 64 nodes. MFU = 27.9%Â© 2025, Amazon Web Services, Inc. or its affiliates. All rights reserved. Amazon Confidential and Trademark. Observations on training precision for 70B (preliminary)')",
"['distributed_pipelining.txt', 'distributed.txt']",What is the difference between torch.disttibuted and torch.distributed.pipelining?,"('While promising for scaling, pipelining is often difficult to implement because it needs to partition the execution of a model in addition to model weights. The partitioning of execution often requires intrusive code changes to your model. Another aspect of complexity comes from scheduling micro-batches in a distributed environment, with data flow dependency considered.', 'The torch.distributed  package provides PyTorch support and communication primitives for multiprocess parallelism across several computation nodes running on one or more machines.')",
"['01_alexnet_imagenet_2012.txt', '04_googlenet_1409_4842.txt']",Explain the importance of ImageNet in the works alexnet and googlenet.,"('We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 dif- ferent classes.', 'We propose a deep convolutional neural network architecture codenamed Incep- tion, which was responsible for setting the new state of the art for classiï¬cation and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC14).')",
18_alphazero_1712_01815.txt,What search algorithm does AlphaZero use instead of alpha-beta search?,,"Instead of an alpha-beta search with domain-speciï¬c enhancements, AlphaZero uses a general-purpose Monte-Carlo tree search (MCTS) algorithm. Each search consists of a series of simulated games of self-play that traverse a tree from root srootto leaf. Each simulation proceeds by selecting in each state sa moveawith low visit count, high move probability and high value (averaged over the leaf states of simulations that selected afroms) according to the current neural network."
05_batchnorm_1502_03167.txt,"What is internal covariate shift, and how does it affect training?",,"We deï¬ne Internal Covariate Shift as the change in the distribution of network activations due to the change in network parameters during training. To improve the training, we seek to reduce the internal covariate shift. By ï¬xing the distribution of the layer inputs xas the training progresses,we expect to improve the training speed."
23_flashattention_2205_14135.txt,What does â€œIO-awareâ€ mean in the context of FlashAttention?,,"In this paper, we argue that a missing principle is making attention algorithms IO-aware [1]â€”that is, carefully accounting for reads and writes to diï¬€erent levels of fast and slow memory (e.g., between fast GPU on-chip SRAM and relatively slow GPU high bandwidth memory, or HBM [ 45], Figure 1 left)."
bottleneck.txt,Why can cProfile and the CPU-mode autograd profiler give incorrect timings on CUDA code?,,"Due to the asynchronous nature of CUDA kernels, when running against CUDA code, the cProfile output and CPU-mode autograd profilers may not show correct timings: the reported CPU time includes only the time to launch kernels but not their GPU execution time unless synchronization occurs."
cmu_llmsys-05-dl-framework.txt,How does auto-differentiation work in these frameworks?,,"TensorFlow is an interface for expressing machine learning algorithms and an implementation for executing such algorithms â€¢PyTorch  is a programming framework for tensor computation, deep learning, and auto differentiation 6Deep Learning Programming Framework7Aspect PyTorch TensorFlow JAX NumPy Primary Use Deep learning Deep learning numerical and ML computing numerical computing Programming Paradigm Dynamic (eager execution)Static (Graph mode, or Eager)Functional transformations Procedural Auto grad dynamic comp graphstatic comp graph Functional -based with grad/jit"
cmu_llmsys-13-distributed-training.txt,"What are FlashMLA, DeepEP, and DeepGEMM, and what problems do they each solve?",,"Accelerating Transformer Layersâ€¢FlashMLA  (released 2/24/2025) FlashMLA  is an efficient MLA decoding kernel for Hopper GPUs, optimized for variable -length sequences serving. DeepEP  (released 2/25/2025) oa communication library tailored for Mixture -of-Experts ( MoE) and expert parallelism (EP). It provides high -throughput and low -latency all -to-all GPU kernels, which are also as known as MoE dispatch and combine. DeepGEMM  (released 2/26/2025) oDeepGEMM  is a library designed for clean and efficient FP8 General Matrix Multiplications (GEMMs) with fine -grained scaling 3Deepseek  opensource libraries"
yiying_llm-perf.txt,What problem does the Model Context Protocol (MCP) solve?,,MCP (Model Context Protocol) Connecting (N) LLMs to (M) external tools/resources used to be a NxM problem MCP standardizes the LLM-tool communication into a N->1->M process Build with a client-server model MCP client: the agent that needs to call tool/data MCP server: a service to expose external tools and data sources
TinyServe_ Query-Aware Cache Selection for Efficient LLM Serving.txt,What are the three core components of the TinyServe system?,,"The system is organized around three core components: (1)Query-Aware KV Retriever: Dynamically selects relevant key-value blocks at decode time based on the current query vector and page-level metadata, reducing unnecessary mem- ory access. (2)Modular Scheduling Pipeline: A dispatch loop handles incoming queries and routes them through configurable plug- ins (e.g., entropy-based early exit, token-level pruning, ap- proximate attention). This modular design allows experimen- tation with different sparsity strategies without modifying the core model. (3)Sparse Attention Executor: Efficiently computes attention over selected KV pages using fused CUDA kernels, with support for FP16/INT8 KV formats and multi-GPU dispatch."
PipeLLM_ Fast and Confidential Large Language Model Services with Speculative .txt,What is NVIDIA GPU Confidential Computing (CC) and how does it secure communication?,,"NVIDIA CC ensures the confidentiality and integrity of communication between a CVM and a GPU via AES-GCM encryption [ 15]. A critical component of AES- GCM is the Initialization Vector (IV), a unique, non-repeating number (a nonce) required for each encryption session."
meta.txt,Why canâ€™t you perform data-dependent operations on meta tensors?,,"Because meta tensors do not have real data, you cannot perform data-dependent operations like torch.nonzero()  or item() . In some cases, not all device types (e.g., CPU and CUDA) have exactly the same output metadata for an operation"
