query,split,raw_gold-text,num_gold_texts,num_retrieved,retrieved_docs,text_recall,text_precision,text_f1,num_gold_texts_matched,ctx_precision,ctx_recall,ctx_f1,chunk_overlap,min_chunk_overlap,max_chunk_overlap,chunk_overlap_count,bleu,text_supported@1,text_recall@1,text_supported@3,text_recall@3
Explain the importance of ImageNet in the works alexnet and googlenet.,multi,"('We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 dif- ferent classes.', 'We propose a deep convolutional neural network architecture codenamed Incep- tion, which was responsible for setting the new state of the art for classiÔ¨Åcation and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC14).')",2,6,"['ference. Compared to the widely used ResNet-50 (He et al.,\n2016), our EfÔ¨ÅcientNet-B4 improves the top-1 accuracy\nfrom 76.3% to 83.0% (+6.7%) with similar FLOPS. Besides\nImageNet, EfÔ¨ÅcientNets also transfer well and achieve state-of-the-art accuracy on 5 out of 8 widely used datasets, while\nreducing parameters by up to 21x than existing ConvNets.\n2. Related Work\nConvNet Accuracy: Since AlexNet (Krizhevsky et al.,\n2012) won the 2012 ImageNet competition, ConvNets have', 'ImageNet only in the choice of data augmentation, the use of\na nonlinear head at the end of the network, and the loss func-\ntion. The strength of this simple framework suggests that,\ndespite a recent surge in interest, self-supervised learning\nremains undervalued.\nAcknowledgements\nWe would like to thank Xiaohua Zhai, Rafael M√ºller and\nYani Ioannou for their feedback on the draft. We are also\ngrateful for general support from Google Research teams in\nToronto and elsewhere.\nReferences', 'composition of two transformations (applied sequentially). The\nlast column reÔ¨Çects the average over the row.\nTo understand the effects of individual data augmentations\nand the importance of augmentation composition, we in-\nvestigate the performance of our framework when applying\naugmentations individually or in pairs. Since ImageNet\nimages are of different sizes, we always apply crop and re-\nsize images (Krizhevsky et al., 2012; Szegedy et al., 2015),\nwhich makes it difÔ¨Åcult to study other augmentations in', 'as belonging to the class indicated by the image label.\nE ImageNet\nImageNet is a dataset of millions of labeled images in thousands of categories. The images\nwere collected from the web and labelled by human labellers using Amazon‚Äôs Mechanical Turk\ncrowd-sourcing tool. In 2010, a subset of roughly 1000 images in each of 1000 classes was the\nbasis of an object recognition competition, a part of the Pascal Visual Object Challenge. This', 'keeping the computational budget constant. To optimize quality, the architectural\ndecisions were based on the Hebbian principle and the intuition of multi-scale\nprocessing. One particular incarnation used in our submission for ILSVRC14 is\ncalled GoogLeNet, a 22 layers deep network, the quality of which is assessed in\nthe context of classiÔ¨Åcation and detection.\n1 Introduction\nIn the last three years, mainly due to the advances of deep learning, more concretely convolutional', 'ity. An ensemble of these residual nets achieves 3.57% error\non the ImageNet testset. This result won the 1st place on the\nILSVRC 2015 classiÔ¨Åcation task. We also present analysis\non CIFAR-10 with 100 and 1000 layers.\nThe depth of representations is of central importance\nfor many visual recognition tasks. Solely due to our ex-\ntremely deep representations, we obtain a 28% relative im-\nprovement on the COCO object detection dataset. Deep\nresidual nets are foundations of our submissions to ILSVRC', 'and there are 1000 categories instead of ten. Another difference is that the ImageNet images\noften contain multiple instances of ImageNet objects, simply due to the sheer number of object\nclasses. For this reason, even a human would have difÔ¨Åculty approaching perfect accuracy on\nthis dataset. For our experiments we resized all images to 256\x02256pixels.\nF Convolutional Neural Networks\nOur models for CIFAR-10 and ImageNet are deep, feed-forward convolutional neural networks', 'ImageNet ClassiÔ¨Åcation with Deep Convolutional\nNeural Networks\nAlex Krizhevsky\nUniversity of Toronto\nkriz@cs.utoronto.caIlya Sutskever\nUniversity of Toronto\nilya@cs.utoronto.caGeoffrey E. Hinton\nUniversity of Toronto\nhinton@cs.utoronto.ca\nAbstract\nWe trained a large, deep convolutional neural network to classify the 1.2 million\nhigh-resolution images in the ImageNet LSVRC-2010 contest into the 1000 dif-\nferent classes. On the test data, we achieved top-1 and top-5 error rates of 37.5%', 'of it. We have found that all the included the knobs and levers allow for a controlled balancing of\ncomputational resources that can result in networks that are 2\x003\x02faster than similarly performing\nnetworks with non-Inception architecture, however this requires careful manual design at this point.\n5 GoogLeNet\nWe chose GoogLeNet as our team-name in the ILSVRC14 competition. This name is an homage to\nYann LeCuns pioneering LeNet 5 network [10]. We also use GoogLeNet to refer to the particular', '1. We independently trained 7 versions of the same GoogLeNet model (including one wider\nversion), and performed ensemble prediction with them. These models were trained with\nthe same initialization (even with the same initial weights, mainly because of an oversight)\nand learning rate policies, and they only differ in sampling methodologies and the random\norder in which they see input images.\n2. During testing, we adopted a more aggressive cropping approach than that of Krizhevsky et']",1.0,0.1,0.18181818181818182,2,0.041666666666666664,1.0,0.07999999999999999,0.19308821587903746,0.07392197125256673,0.3558282208588957,10,0.04288903976953925,0.0,0.0,0.0,0.0
How does auto-differentiation work in these frameworks?,single,"TensorFlow is an interface for expressing machine learning algorithms and an implementation for executing such algorithms ‚Ä¢PyTorch  is a programming framework for tensor computation, deep learning, and auto differentiation 6Deep Learning Programming Framework7Aspect PyTorch TensorFlow JAX NumPy Primary Use Deep learning Deep learning numerical and ML computing numerical computing Programming Paradigm Dynamic (eager execution)Static (Graph mode, or Eager)Functional transformations Procedural Auto grad dynamic comp graphstatic comp graph Functional -based with grad/jit",1,4,"['torch.func  has auto-differentiation transforms (grad(f) returns a function that computes\nthe gradient of f), a vectorization/batching transform (vmap(f) returns a function that\ncomputes f over batches of inputs), and others.\nThese function transforms can compose with each other arbitrarily. For example, composing\nvmap(grad(f))  computes a quantity called per-sample-gradients that stock PyTorch cannot\nefficiently compute today.\nWhy composable function transforms?', 'Deep Learning Framework \nDesign\nLei Li\n‚Ä¢Learning parameters of an NN needs gradient calculation\n‚Ä¢Computation Graph\noto perform computation: topological traversal along the DAG\n‚Ä¢Auto Differentiation\nobuilding backward computation graph for gradient calculation\n2Recap3y=x1 +  exp(1.5 * x1 + 2.0 * x2) x1= 3, x2=0.5\nùë•1\nùë•3\nùë•5ùë§1\n=1.5\n*\n+\nùë•6exp(.)\nùë•7ùë•2\nùë•4ùë§2\n=2.0\n*\n+\nùë•7\n=1ùë•6ùë•4\nùë•5\nexp(.)\nidùë•5‚Üí6 *idùë§2*ùë•2\n*\nùë•3idùë§1*ùë•1\nùë•1‚Üí3*+Backward Computation Graph‚Ä¢How to design a deep learning framework\noDesign ideas in TensorFlow', '‚Ä¢Auto Differentiation\n4Today‚Äôs Topic\n‚Ä¢Neural network layers\noEmbedding (lookup table)\noLinear \noRelu\noAverage pooling\noSoftmax\n5A Simple Feedforward Neural Network\nLinearReluLinearSoftmax\nEmbedding\n‚ÄúIt is a good movie‚ÄùAvg‚Ä¢ùë•ùëõ,ùë¶ùëõ are data and label pairs for training\n‚Ä¢Cross entropy\n‚Ñí(ùúÉ)=1\nùëÅ‚àë\nùëõ=1ùëÅ\n‚àílogùëì (ùë•ùëõ)ùë¶ùëõ\n‚Ä¢Pytorch  CrossEntropyLoss  is implemented as\noNegative Likelihood on logits\n6Training Loss for Classificationloss = nn.CrossEntropyLoss ()\noutput  = loss( input_logits , target_labels )', 'The results above extend those of Stapor et al. (2018, section 2.4.2). An extended version of\nAlgorithm 1 including derivatives w.r.t. t0andt1can be found in Appendix C. Detailed derivations\nare provided in Appendix B. Appendix D provides Python code which computes all derivatives for\nscipy.integrate.odeint by extending the autograd automatic differentiation package. This\ncode also supports all higher-order derivatives. We have since released a PyTorch (Paszke et al.,', 'Automatic differentiation in machine learning: a survey. Journal of machine learning research , 18\n(153):1‚Äì153, 2018.\nRianne van den Berg, Leonard Hasenclever, Jakub M Tomczak, and Max Welling. Sylvester\nnormalizing Ô¨Çows for variational inference. arXiv preprint arXiv:1803.05649 , 2018.\nBob Carpenter, Matthew D Hoffman, Marcus Brubaker, Daniel Lee, Peter Li, and Michael Betan-\ncourt. The Stan math library: Reverse-mode automatic differentiation in c++. arXiv preprint\narXiv:1509.07164 , 2015.', 'Differentiation (recall previous lecture)\n17Gradient Computation\ntrain_step  = tf.train.GradientDescentOptimizer (0.5).minimize( cross_entropy )‚Ä¢How to design a deep learning framework\noDesign ideas in TensorFlow\n‚ñ™Abadi et al., ‚ÄúTensorFlow: A System for Large -Scale Machine Learning‚Äù, \nOSDI 2016\noBasic Graph node types in Tensorflow /Pytorch\noOverall design principles\n‚Ä¢Hands -on practice to implement a mini -tensorflow\n‚Ä¢Execution in Tensorflow\n18Today‚Äôs Topic\n‚Ä¢All nodes return tensors', 'coverage over PyTorch operations.\nIf you have suggestions on the API or use-cases you ºd like to be covered, please open a\nGitHub issue or reach out. We ºd love to hear about how you ºre using the library.\nWhat are composable function transforms?\nA ‚Äúfunction transform‚Äù is a higher-order function that accepts a numerical function and returns\na new function that computes a different quantity.\ntorch.func  has auto-differentiation transforms (grad(f) returns a function that computes', 'In contrast, by providing a generic vector-Jacobian product, we allow an ODE solver to be trained\nend-to-end with any other differentiable model components. While use of vector-Jacobian products\nfor solving the adjoint method has been explored in optimal control (Andersson, 2013; Andersson\net al., In Press, 2018), we highlight the potential of a general integration of black-box ODE solvers\ninto automatic differentiation (Baydin et al., 2018) for deep learning and generative modeling.\n8 Conclusion', 'Yiping Lu, Aoxiao Zhong, Quanzheng Li, and Bin Dong. Beyond Ô¨Ånite layer neural networks:\nBridging deep architectures and numerical differential equations. arXiv preprint arXiv:1710.10121 ,\n2017.\nDougal Maclaurin, David Duvenaud, and Ryan P Adams. Autograd: Reverse-mode differentiation of\nnative Python. In ICML workshop on Automatic Machine Learning , 2015.\nHongyuan Mei and Jason M Eisner. The neural Hawkes process: A neurally self-modulating', 'The main technical difÔ¨Åculty in training continuous-depth networks is performing reverse-mode\ndifferentiation (also known as backpropagation) through the ODE solver. Differentiating through\nthe operations of the forward pass is straightforward, but incurs a high memory cost and introduces\nadditional numerical error.\nWe treat the ODE solver as a black box, and compute gradients using the adjoint sensitivity\nmethod (Pontryagin et al., 1962). This approach computes gradients by solving a second, aug-']",0.0,0.0,0.0,0,0.09090909090909091,1.0,0.16666666666666669,0.23704826181960753,0.17311608961303462,0.3247011952191235,10,3.2323055855636944e-05,0.0,0.0,0.0,0.0
"What are FlashMLA, DeepEP, and DeepGEMM, and what problems do they each solve?",single,"Accelerating Transformer Layers‚Ä¢FlashMLA  (released 2/24/2025) FlashMLA  is an efficient MLA decoding kernel for Hopper GPUs, optimized for variable -length sequences serving. DeepEP  (released 2/25/2025) oa communication library tailored for Mixture -of-Experts ( MoE) and expert parallelism (EP). It provides high -throughput and low -latency all -to-all GPU kernels, which are also as known as MoE dispatch and combine. DeepGEMM  (released 2/26/2025) oDeepGEMM  is a library designed for clean and efficient FP8 General Matrix Multiplications (GEMMs) with fine -grained scaling 3Deepseek  opensource libraries",1,5,"['search for large vocabulary\nconverting sorting to \nparallel operations (max, \nfilter, re -rank)Accelerating decodingRecap: Accelerating Transformer Layers‚Ä¢FlashMLA  (released 2/24/2025)\noFlashMLA  is an efficient MLA decoding kernel for Hopper GPUs, optimized \nfor variable -length sequences serving. \n‚Ä¢DeepEP  (released 2/25/2025)\noa communication library tailored for Mixture -of-Experts ( MoE) and expert \nparallelism (EP). It provides high -throughput and low -latency all -to-all GPU ', 'parallelism (EP). It provides high -throughput and low -latency all -to-all GPU \nkernels, which are also as known as MoE dispatch and combine.\n‚Ä¢DeepGEMM  (released 2/26/2025)\noDeepGEMM  is a library designed for clean and efficient FP8 General Matrix \nMultiplications (GEMMs) with fine -grained scaling\n3Deepseek  opensource libraries\nhttps://github.com/deepseek -ai/ ‚Ä¢Overview of large -scale model training\n‚Ä¢Multi -GPU communication\n‚Ä¢Data Parallel Training via AllReduce', 'Alternative Model Structures: Linformer, Reformer\nTraining Strategy: Shallow to Deep, Layer Dropout\nEÔ¨Écient Computation: LAMB, Quantization, Hardware Optimization\n65\ncode is available at \nhttps://github.com/bytedance/lightseqStay tuned for Deepseek‚Äôs FlashMLA‚Ä¢High-performance decoding kernel optimized for Multi-head Latent Attention (MLA) on Hopper GPUs\n‚Ä¢https://github.com/deepseek-ai/FlashMLA\n66Reading for Next‚Ä¢PyTorch Distributed: Experiences on Accelerating Data Parallel Training', 'math word problems. arXiv preprint arXiv:2110.14168 ,\n2021.\nDai, D., Deng, C., Zhao, C., Xu, R. X., Gao, H., Chen,\nD., Li, J., Zeng, W., Yu, X., Wu, Y ., Xie, Z., Li, Y . K.,Huang, P., Luo, F., Ruan, C., Sui, Z., and Liang, W.\nDeepseekmoe: Towards ultimate expert specialization in\nmixture-of-experts language models, 2024.\nDeepSeek-AI. Deepseek-v2: A strong, economical, and\nefficient mixture-of-experts language model, 2024a.\nDeepSeek-AI. Deepseek-v3 technical report, 2024b. URL', 'configurations and report the one DeepSpeed-MoE performs best.\nprevailing open-source LLM inference framework, adopt-\ning a bunch of optimization including but not limited to\ncontinuous batching, paged attention, flash attention, radix-\nattention, advanced quantization, etc. SGLang declares op-\ntimizations for MoE models with high-performance, fused\ntriton kernels (OpenAI) to implement TP-based paralleliza-\ntion for both attention and expert layers.\nMetrics: Throughput is the number of tokens (tokens/s)', 'Wang, Zihui Gu, Zilin Li, and Ziwei Xie. Deepseek-v2:\nA strong, economical, and efficient mixture-of-experts\nlanguage model, 2024.\n[18] Hugging Face. Text Generation Inference. https:\n//huggingface.co/text-generation-inference ,\n2024.\n[19] Hao Fei, Han Zhang, Bin Wang, Lizi Liao, Qian Liu,\nand Erik Cambria. Empathyear: An open-source avatar\nmultimodal empathetic chatbot, 2024.\n[20] flashInfer.ai. flashinfer. https://github.com/\nflashinfer-ai/flashinfer , 2023.', '8√ó7B MoE models. DeepCompile achieves up to 1.28 √óand\n1.54√óperformance improvements over ZeRO-3 and FSDP\nbaselines, respectively, and up to a 7.01 √óthroughput increase\nin settings with limited GPU resources, using offloading.\nKeywords\ndeep learning, distributed training\n‚àóWork done while at Microsoft. Now at AMD.1 Introduction\nThe rapid growth of deep learning has led to the emergence\nof increasingly large models. Modern architectures often\ncontain billions of parameters, resulting in immense com-', 'Parallel) and DP (Data Parallelism). However,\nour analysis shows DeepSpeed-MoE‚Äôs inference\nefficiency is largely bottlenecked by EP, which is\nimplemented with costly all-to-all collectives to\nroute token activation. Our work aims to boost\nDeepSpeed-MoE by strategically reducing EP‚Äôs\ncommunication overhead with a technique named\nSpeculative MoE. Speculative MoE has two spec-\nulative parallelization schemes, speculative token\nshuffling and speculative expert grouping, which', '(FFN) modules by dynamically selecting the best backend\nimplementation of GroupGemm and DenseGemm.\nSpeculative MoE Inference - offloading and prefetch-\ning.Existing work on speculative MoE inference primarily\nfocuses on prefetching offloaded experts and strategically\nsaving GPU memories (Yi et al., 2023; Xue et al., 2024;\nZhong et al., 2024), though offloading can extend infer-\nence latency and is rarely used in latency-critical serving\nscenarios. In contrast, s-MoE focuses on the speculative', 'DBRX (Mosaic-Research), Arctic (arc) DeepSeek-MoE-\n16B (Dai et al., 2024), DeepSeek V2 (DeepSeek-AI, 2024a),\nV3 (DeepSeek-AI, 2024b), R1 (DeepSeek-AI, 2025), Grok-\n1 (X-AI), QWen-MoE (Qwen-Team, 2024), etc.\nNevertheless, at inference time, massive MoEs models‚Äô\nvoluminous expert and attention blocks still require huge\namounts of GPU/NPU1cores, memory and bandwidth to\ncompute, stash and load expert parameters for the forward\npass. Existing MoE frameworks achieve inference scala-']",1.0,0.2,0.33333333333333337,1,0.19148936170212766,1.0,0.32142857142857145,0.3293103084963724,0.18503118503118504,0.74375,10,0.11361409754211478,1.0,1.0,0.6666666666666666,1.0
What are the challenges of theoretical distributed training (PipeDream) and what is the scaling efficiency of distributed real-world Trainium performance (HLAT)?,multi,"('Challenge 1: Stage Partitioning‚Ä¢How to partition model layers into the stages evenly? ‚Ä¢Throughput depends on the slowest stage in pipeline ‚Ä¢Solution: ‚Ä¢ProÔ¨Åle layer-wise perf and comm perf ‚Ä¢Allows a stage to be replicated (DP) ‚Ä¢Uses dynamic programming to Ô¨Ånd optimal partition and layer replication', 'Challenge 2: Work Scheduling‚Ä¢How to schedule forward and backward computation on a worker? ‚Ä¢Solution: 1F1B-RR ‚Ä¢Run one forward and one backward ‚Ä¢Round-robin across replicated stages', 'Challenge 3: Weight Versioning‚Ä¢How to ensure the same minibatch uses the same weight version across workers for forward and backward? ‚Ä¢Otherwise computation will be far oÔ¨Ä and training not able to converge ‚Ä¢Solution: Store multiple weight versions so that the backward and forward of the same minibatch ‚Ä¢Weights across workers can be diÔ¨Äerent!', 'The scaling efficiency for Llama2 7b:‚Ä¢87% on 32 nodes. MFU = 33.5%‚Ä¢72% on 64 nodes. MFU = 27.9%¬© 2025, Amazon Web Services, Inc. or its affiliates. All rights reserved. Amazon Confidential and Trademark. Observations on training precision for 70B (preliminary)')",4,7,"['(parameter count) are further increased to an incredible 530 billion (Megatron-Turing-NLG Smith\net al. (2022)) and 540 billion (PaLM Chowdhery et al. (2022)), because the scaling law Henighan\net al. (2020) is still working.\nEfÔ¨Åcient Distributed Model Training. Scaling model training to tens of or hundreds of billion\nparameters is a complicated task, which requires a lot of algorithmic innovations and engineer-\ning optimization. One of the most critical challenges is that the model cannot Ô¨Åt into one single', 'increases, efficient distributed training across thousands of GPUs becomes essential. Different paral-\nlelism strategies have been proposed in recent years for distributed LLM training, including model\nparallelism, data parallelism, and pipeline parallelism [ 31;27;19]. However, a single parallelism\nstrategy has limitations regarding scalability. For example, the performance of data parallelism with\nZeRO-3 will decrease dramatically when the number of GPUs increases to several thousands [21].', 'Devanur, Greg Ganger, and Phil Gibbons. 2018. Pipedream: Fast and efficient\npipeline parallel dnn training.arXiv preprint arXiv:1806.03377(2018).\n[18] Chaoyang He, Shen Li, Mahdi Soltanolkotabi, and Salman Avestimehr. 2021.\nPipetransformer: Automated elastic pipelining for distributed training of trans-\nformers.arXiv preprint arXiv:2102.03161(2021).\n[19] Jiaao He, Jidong Zhai, Tiago Antunes, Haojie Wang, Fuwen Luo, Shangfeng\nShi, and Qin Li. 2022. FasterMoE: Modeling and Optimizing Training of Large-', 'Background: distributed training on Trainium', 'training framework becomes increasingly imperative for applica-\ntions built on top of PyTorch. This section elucidates the trajectory\nof PyTorch‚Äôs distributed training capabilities.\n2.1 Model Replication\nModel replication approaches are designed to tackle high-volume\ndatasets by scaling out and distributing computations across multi-\nple devices. DistributedDataParallel (DDP) [ 14] is the first end-to-end\ndistributed training feature in PyTorch that falls into this category.', 'distributed training [ 4]. To address this gap, we introduce DynMo ,\nan elastic load-balancing framework tailored for dynamic models. It\nis the first work to study pipeline stalls caused by training dynamic\nmodels. DynMo ensures balanced pipeline stages by dynamically\nredistributing workloads across accelerators whenever imbalance\narises, thereby improving computational efficiency and reducing\ntraining costs. The framework incorporates two different dynamic', 'effectively. Conventional distributed training methods include TP, DP, CP and PP. TP divides the\ncomputations of neural network layers across multiple devices, allowing for parallel processing\nof tensors within layers[ 31]. TP can significantly reduce the memory consumption of each model\nrank but introduces some intra-layer communication overhead. DP distributes batches of data\nacross replicas of the model on different devices, aggregating gradients during training[ 35]. Zero', 'Background: distributed training on Trainium\n4‚Ä¢AWS Trainium‚Ä¢Trn1.32xlarge contains 16 Trn accelerators, and 32 Neuron Cores‚Ä¢16GB memory per Neuron Core‚Ä¢3040 TFLOPS in FP16/BF1‚Ä¢Cost $21.50 vs. p4d.24xlarge $32.77‚Ä¢Neuron Distributed Training Library (NDTL, also called NeuronX-distributed)‚Ä¢Tensor, pipeline, data, and sequence parallelism‚Ä¢Zero-1 optimizer‚Ä¢Multiple training precision configurations‚Ä¢Automatic fault recovery‚Ä¢‚Ä¶¬© 2025, Amazon Web Services, Inc. or its affiliates. All rights reserved. Amazon Confidential and Trademark.', 'What is torch.distributed.pipelining ?\nWhile promising for scaling, pipelining is often difficult to implement because it needs to partition\nthe execution of a model in addition to model weights. The partitioning of execution often requires\nintrusive code changes to your model. Another aspect of complexity comes from scheduling\nmicro-batches in a distributed environment, with data flow dependency considered.\nThe pipelining  package provides a toolkit that does said things automatically which allows easy', 'being applied to other domains such as vision [22]. DLRM\n[3] has scaled beyond a trillion parameters with 4D parallelism\ntechniques [9]. We demonstrate that our work further improves\nthe scaling behavior of these complex parallel DL models.\nIII. B ACKGROUND\nA. DL Training\nDistributed DL can take several forms: data-parallelism,\nmodel-parallelism, and hybrid-parallelism. Data-parallism\nplaces a full model replica on each processor, and splits\nthe training data among processors. Model parallelism splits']",0.25,0.2,0.22222222222222224,1,0.05555555555555555,0.23076923076923078,0.08955223880597016,0.26208839318163185,0.17227722772277226,0.7045454545454546,10,0.0357967798952711,0.0,0.0,0.0,0.0
What are the three core components of the TinyServe system?,single,"The system is organized around three core components: (1)Query-Aware KV Retriever: Dynamically selects relevant key-value blocks at decode time based on the current query vector and page-level metadata, reducing unnecessary mem- ory access. (2)Modular Scheduling Pipeline: A dispatch loop handles incoming queries and routes them through configurable plug- ins (e.g., entropy-based early exit, token-level pruning, ap- proximate attention). This modular design allows experimen- tation with different sparsity strategies without modifying the core model. (3)Sparse Attention Executor: Efficiently computes attention over selected KV pages using fused CUDA kernels, with support for FP16/INT8 KV formats and multi-GPU dispatch.",1,1,"['3.1 System Overview: TinyServe\nTinyServe is a lightweight serving framework designed for serving\ntiny language models under tight memory and latency constraints.\nRather than acting as a benchmarking tool, TinyServe serves as a\nreal-time serving environment that enables sparsity-aware atten-\ntion, modular token selection, and efficient KV-cache reuse.\nThe system is organized around three core components:\n(1)Query-Aware KV Retriever: Dynamically selects relevant', 'LLM serving‚Äîsuch as KV cache saturation and decode-time la-\ntency‚Äîwith modular support for token selection, cache sparsity,\nfused attention kernels, and training optimization.\nAt the core of TinyServe is a query-aware page selection mech-\nanism that approximates attention relevance using bounding-box\nmetadata, enabling selective KV access with minimal overhead.', 'duces TinyServe , a lightweight efficient serving framework using\nsmall LLMs to reproduce the key serving stack components‚Äîstreamingattention, dynamic batching, and quantized decoding‚Äîunder re-\nalistic serving scenarios. This enables fast hypothesis testing of\narchitectural changes with minimal compute cost.\nWe also note related work on Memory-Keyed Attention (MKA) [ 21],\nwhich extends attention mechanisms for long-context reasoning,\nand data-centric safety frameworks [ 13], which highlight broader', 'pages); - Reduced HBM bandwidth pressure.\nSystem Implication. TinyServe enables dynamic query-aware\nsparsity without requiring architectural retraining. The modular\nimplementation integrates directly into TinyServe‚Äôs kernel loop\nand allows hardware-sensitive scheduling: e.g., keeping hot pages\nin shared memory or limiting K to match tensor core granularity.\nThe kernel design for TinyServe can be found at algorithm 1.\n3.6 Memory Efficiency Analysis', 'often forced to treat models as black boxes, unable to validate hy-\npotheses or perform design iteration without access to large GPU\nclusters.\nTinyServe: Large-Scale Serving at Small Scale. We introduce\nTinyServe , a lightweight serving framework that enables detailed\nanalysis of LLM training and inference behavior using tiny models\n(e.g., 125M‚Äì350M parameters). TinyServe replicates core compo-\nnents of LLM serving‚Äîstreaming decoding, KV cache management,', 'Serve , a lightweight and extensible serving system for deploying\ntiny LLMs (e.g., TinyLLaMA, GPT2-345M) with support for struc-\ntured KV sparsity, plugin-based token selection, and hardware-\nefficient attention kernels. Unlike prior simulation frameworks,\nTinyServe executes real-time decoding with configurable sparsity\nstrategies and fine-grained instrumentation.\nTo reduce decoding cost, we introduce a query-aware page selec-\ntionmechanism that leverages bounding-box metadata to estimate', 'inefficiencies.\nBeyond LLMs, graph learning acceleration systems like Graph-\nSnapShot [ 11,12,15] explore caching and retrieval strategies to\noptimize large-scale graph training. These works emphasize that\ncarefully designed synthetic stressors and caching strategies are\nessential for both graph-based and language-based workloads, re-\ninforcing the importance of lightweight analysis frameworks.\n3 Methodology\n3.1 System Overview: TinyServe\nTinyServe is a lightweight serving framework designed for serving', 'code=2k). TinyServe maintains higher hit rate and fewer\ntoken evictions.\nLLMs, TinyServe allows efficient and interpretable serving profil-\ning, supporting system-level research without relying on full-scale\ndeployments.\n5 Conclusion\nWe introduced TinyServe , a lightweight and extensible serving\nsystem for efficient inference and training acceleration with tiny\nlanguage models. TinyServe bridges system-level bottlenecks in\nLLM serving‚Äîsuch as KV cache saturation and decode-time la-', 'port for training acceleration and fine-grained profiling. For training\nscenarios, TinyServe implements gradient-aware memory manage-\nment that selectively retains KV cache entries based on gradient\nmagnitude during backpropagation. This approach reduces mem-\nory footprint by up to 40% during fine-tuning while maintaining\ntraining stability.\nThe profiling system integrates layer-wise performance monitor-\ningwith microsecond precision, tracking attention patterns, mem-', 'The kernel design for TinyServe can be found at algorithm 1.\n3.6 Memory Efficiency Analysis\nTo quantify memory access savings under query-aware sparsity, we\nconstruct a probabilistic cost model that accounts for (1) metadata\noverhead, (2) selected KV tokens, and (3) cross-step reuse. This anal-\nysis provides theoretical bounds on the performance improvements\nachievable through our approach.\nLet: -ùêø: total cache length (tokens); - ùëÜ: page size (tokens per']",0.0,0.3,0.0,0,0.05263157894736842,0.5,0.09523809523809525,0.2598938439047929,0.1503267973856209,0.3601694915254237,10,0.033184399159280994,1.0,0.0,0.3333333333333333,0.0
What are the trade-offs between simple post-training quantization and GPTQ?,multi,"('8CUDA APIs for Half Precision‚Ä¢Using lower precision oconverting parameters from FP32 to INT8 or INT4 operform all computation in lower prevision. Reduce model accuracy: oLoss of Precision ‚ûî accumulate quantization noise oRange mismatch ‚ûî values are clipped and lead to information loss oQuantization error ‚ûî rounding errors 9Direct Quantization Approach‚Ä¢Absmax  quant Zero -point quant 10Quantize a number', 'mixed strategy for 8 -bit quantization and 16 -bit (for outliers)2Recap of Basic Quantization Methods‚Ä¢Absmax  quant Zero -point quant 4Quantize a Number to Int8 5GPTQ scale to GPT -size LLMs maintain accuracy‚Ä¢Revisit  layer -wise quantization of weight matrices argmin', 'layer -wise quantization + compensation for errors + precompute oaccurately compress some of the largest publicly -available models down to 3 and 4 bits, and bring end -to-end speedups')",3,4,"['28\n‚Ä¢Overview of Parameter Efficient Fine -Tuning\n‚Ä¢LoRA : Low -rank Adaptation (Counter -interference adapter, \nCIAT)\n‚Ä¢QLoRA : Quantization + Low -rank training\n‚Ä¢Code Walkthrough\n30Outline\n‚Ä¢GPTQ is Post -Training Quantization (PTQ): converting the \nweights of an already trained model to a lower precision \nwithout any retraining. \n‚Ä¢Quantization -Aware Training (QAT): integrates the weight \nconversion process during the training stage. often superior \nmodel performance. ( QLoRA ) ', 'error  incurred by quantizing a single weight\n6Overall idea of GPTQ\nOptimal Brain Compression: A framework for accurate post -training quantization and pruning (2022)Optimal Brain Surgeon and General Network Pruning (1993)GPTQ: Accurate Post -Training Quantization for Generative Pre -trained Transformers. Frantar  et al. ICLR 2023. 1.Pre-compute Cholesky decomposition of the Hessian \ninverse for input data X of current (Linear) layer\n2.Iteratively handle one batch of columns of weight matrix W', '‚Ä¢The scalability is verified up to 20B models (GPT -NeoX20B)\n‚Ä¢At 1.3B scale, computation time is ~3 hours\nobut slower than GPTQ (x100 larger in ~4 hours)\n‚Ä¢integrated in Deepspeed\n22ZeroQuant\nYao et al. ZeroQuant : Efficient and Affordable Post -Training Quantization for Large -Scale Transformers. Neurips  2022.‚Ä¢Using 8 -bit quantization for \nmatrix multiplications\n‚Ä¢But, extreme outliers in \nfeatures (activation values)\noneed for wider numerical ranges \noQuantize all parameters without ', 'language models,‚Äù Advances in Neural Information Processing Systems ,\nvol. 35, pp. 17 402‚Äì17 414, 2022.\n[7] G. Xiao, J. Lin, M. Seznec, H. Wu, J. Demouth, and S. Han,\n‚ÄúSmoothquant: Accurate and efficient post-training quantization for\nlarge language models,‚Äù in International Conference on Machine\nLearning . PMLR, 2023, pp. 38 087‚Äì38 099.\n[8] E. Frantar, S. Ashkboos, T. Hoefler, and D. Alistarh, ‚ÄúGptq: Accu-\nrate post-training quantization for generative pre-trained transformers,‚Äù', '19Insight of Arbitrary Update Order for OBQ\nGPTQ: Accurate Post -Training Quantization for Generative Pre -trained Transformers. Frantar  et al. ICLR 2023. ‚Ä¢Na√Øve column update is not fast in practice \nolow compute -to-memory -access ratio\nocannot highly utilize GPUs compute.\n‚Ä¢Observation: \noRounding decisions for col i only affected \nby updates on this col\noUpdates to later columns are irrelevant at \nthis point in the process.\n‚Ä¢Efficient update\n20Lazy Batch Updates', '15Quantization   during training\npost trainingpreserve accuracy\nscale to large \nparametersBRECQ (Li et al., 2021)AdaQuant (Hubara et al., 2021)\nOBQ (Frantar et al., 2022)\nZeroQuant (Yao et al., 2022)\nLLM.int8() (Dettmers et al., 2022)Model Quantization Approaches\n16Quantization  during training\npost trainingpreserve \naccuracy\n(by quantizing each \nindividual / consecutive \nlayers)\nscale to large \nparametersBRECQ (Li et al., 2021)AdaQuant (Hubara et al., 2021)\nOBQ (Frantar et al., 2022)', 'P rec@5 85.7 % 5.6 % 75.7 %\n19Is Quantization Accurate?20Why is Quantizing LLMs Difficult?\nQuantization  during training\npost trainingpreserve accuracy\nscale to large \nparameters\n(by rounding weights to the \nnearest quantization level)BRECQ (Li et al., 2021)AdaQuant (Hubara et al., 2021)\nOBQ (Frantar et al., 2022)\nZeroQuant (Yao et al., 2022)\nLLM.int8() (Dettmers et al., 2022)accuracy loss when lower -bit \nprecision (ex. 3, 4 bits per \nparameter)‚Ä¢Layer -by-layer knowledge distillation ', 'rate post-training quantization for generative pre-trained transformers,‚Äù\narXiv preprint arXiv:2210.17323 , 2022.\n[9] Z. Liu, B. Oguz, C. Zhao, E. Chang, P. Stock, Y . Mehdad, Y . Shi, R. Kr-\nishnamoorthi, and V . Chandra, ‚ÄúLlm-qat: Data-free quantization aware\ntraining for large language models,‚Äù arXiv preprint arXiv:2305.17888 ,\n2023.\n[10] W. Wang, W. Chen, Y . Luo, Y . Long, Z. Lin, L. Zhang, B. Lin,\nD. Cai, and X. He, ‚ÄúModel compression and efficient inference for', '1.3B model - 3h\nDoes GPT -Q even work for extreme 2 -bit quantization?\n28\nHow is GPT -Q‚Äôs perf on small models compared with \naccurate -but-expensive methods?\n29\n Fastest prior method‚Ä¢https://github.com/qwopqwop200/GPTQ -for-LLaMa/  \n‚Ä¢GPTQ in \nohttps://github.com/qwopqwop200/GPTQ -for-\nLLaMa/blob/triton/gptq.py  \n31GPTQ for LLaMAGPTQ: Initialization\n32\n‚óèReshape weights from the \ninput layer\n‚óèInitialize Hessian matrixGPTQ: Hessian Matrix Update\n33‚óèUpdate Hessian matrix with \ninformation from a new ', 'return  X_quant .to(torch .int8), X_dequanthttps://colab.research.google.com/drive/1DPr4mUQ92Cc -\nxf4GgAaB6dFcFnWIvqYi?usp=sharing  \n12Direct Quantization Colab13Today‚Äôs Topic\n‚Ä¢Low precision numbers in computer\n‚Ä¢Basic Quantization Methods\nModel Quantization Approaches\n14Quantization during training\npost trainingexpensive re -training / finetuning\nModel Quantization Approaches\n15Quantization   during training\npost trainingpreserve accuracy\nscale to large ']",0.0,0.0,0.0,0,0.0,0.0,0.0,0.22622898047962403,0.19750519750519752,0.2865979381443299,10,4.0495543026899486e-05,0.0,0.0,0.0,0.0
What does ‚ÄúIO-aware‚Äù mean in the context of FlashAttention?,single,"In this paper, we argue that a missing principle is making attention algorithms IO-aware [1]‚Äîthat is, carefully accounting for reads and writes to diÔ¨Äerent levels of fast and slow memory (e.g., between fast GPU on-chip SRAM and relatively slow GPU high bandwidth memory, or HBM [ 45], Figure 1 left).",1,4,"['aware‚Äîaccounting for reads and writes between levels of GPU memory. We propose FlashAttention ,\nan IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes\nbetween GPU high bandwidth memory (HBM) and GPU on-chip SRAM. We analyze the IO complexity\nofFlashAttention , showing that it requires fewer HBM accesses than standard attention, and is\noptimal for a range of SRAM sizes. We also extend FlashAttention to block-sparse attention, yielding', 'pression for large language models,‚Äù arXiv preprint arXiv:2308.07633 ,\n2023.\n[13] T. Dao, D. Fu, S. Ermon, A. Rudra, and C. R ¬¥e, ‚ÄúFlashattention: Fast\nand memory-efficient exact attention with io-awareness,‚Äù Advances in\nNeural Information Processing Systems , vol. 35, pp. 16 344‚Äì16 359,\n2022.15\n[14] W. Kwon, Z. Li, S. Zhuang, Y . Sheng, L. Zheng, C. H. Yu, J. Gonzalez,\nH. Zhang, and I. Stoica, ‚ÄúEfficient memory management for large\nlanguage model serving with pagedattention,‚Äù in Proceedings of the', 'Transformers are slow and memory-hungry on long sequences, since the time and memory complexity\nof self-attention are quadratic in sequence length. Approximate attention methods have attempted\nto address this problem by trading oÔ¨Ä model quality to reduce the compute complexity, but often do\nnot achieve wall-clock speedup. We argue that a missing principle is making attention algorithms IO-\naware‚Äîaccounting for reads and writes between levels of GPU memory. We propose FlashAttention ,', '[10] Tri Dao, Daniel Y . Fu, Stefano Ermon, Atri Rudra, and Christopher R√©. Flashattention:\nFast and memory-efficient exact attention with io-awareness. In Sanmi Koyejo, S. Mo-\nhamed, A. Agarwal, Danielle Belgrave, K. Cho, and A. Oh, editors, Advances in Neu-\nral Information Processing Systems 35: Annual Conference on Neural Information Pro-\ncessing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - Decem-\nber 9, 2022 , 2022. URL http://papers.nips.cc/paper_files/paper/2022/hash/', '48550/arXiv.2307.08691 .\nTri Dao, Daniel Y . Fu, Stefano Ermon, Atri Rudra, and Christopher R√©. Flashattention:\nFast and memory-efficient exact attention with io-awareness. In Sanmi Koyejo, S. Mo-\nhamed, A. Agarwal, Danielle Belgrave, K. Cho, and A. Oh (eds.), Advances in Neural\nInformation Processing Systems 35: Annual Conference on Neural Information Process-\ning Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9,\n2022 , 2022. URL http://papers.nips.cc/paper_files/paper/2022/hash/', 'computation back to HBM. Right:Speedup over the PyTorch implementation of attention on GPT-2.\nFlashAttention does not read and write the large ùëÅ\x02ùëÅattention matrix to HBM, resulting in an 7.6 \x02\nspeedup on the attention computation.\nGPUs, compute speed has out-paced memory speed [ 61,62,63], and most operations in Transformers are\nbottlenecked by memory accesses [ 43]. IO-aware algorithms have been critical for similar memory-bound', 'FlashAttention : Fast and Memory-EÔ¨Écient Exact Attention\nwith IO-Awareness\nTri Daoy, Daniel Y. Fuy, Stefano Ermony, Atri Rudraz, and Christopher R√©y\nyDepartment of Computer Science, Stanford University\nzDepartment of Computer Science and Engineering, University at BuÔ¨Äalo, SUNY\n{trid,danfu}@cs.stanford.edu ,ermon@stanford.edu ,atri@buffalo.edu ,\nchrismre@cs.stanford.edu\nJune 24, 2022\nAbstract\nTransformers are slow and memory-hungry on long sequences, since the time and memory complexity', 'the attention computation into smaller block-wise computation to reduce the IO between SRAM and\nthe high bandwidth memory (HBM). The hardware implementation of FlashAttention family (Dao\net al., 2022; Dao, 2023) make them the most widely-adopted attention acceleration framework. It\nremains unclear whether we can implement various sparse self-attention in such hardware-aware way,\nso that the training speed can be further boosted over FlashAttention.\n2.2 I SSUES WITH PLUG-IN-AND -PLAY KV E VICTION METHODS', 'wall-clock time. We analyze its IO complexity, showing that our method requires much fewer HBM accesses\ncompared to standard attention. We further show that FlashAttention can serve as a useful primitive by\nextending it to handle block-sparse attention.\nWe focus here on the forward pass for ease of exposition; Appendix B contains details for the backward.\n3.1 An EÔ¨Écient Attention Algorithm With Tiling and Recomputation', 'We analyze the IO complexity [ 1] ofFlashAttention , proving that it requires ùëÇ¬πùëÅ2ùëë2ùëÄ\x001¬∫HBM\naccesses where ùëëis the head dimension and ùëÄis the size of SRAM, as compared to Œ©¬πùëÅùëë¬∏ùëÅ2¬∫of standard\nattention. For typical values of ùëëandùëÄ,FlashAttention requires many times fewer HBM accesses\ncompared to standard attention (up to 9 \x02fewer, as shown in Fig. 2). Moreover, we provide a lower bound,\nshowing that no exact attention algorithm can asymptotically improve on the number of HBM accesses over\nall SRAM sizes.']",1.0,0.2,0.33333333333333337,1,0.045454545454545456,1.0,0.08695652173913045,0.198378368077404,0.15242494226327943,0.3155737704918033,10,0.0249362217775502,0.0,1.0,0.0,1.0
"What is internal covariate shift, and how does it affect training?",single,"We deÔ¨Åne Internal Covariate Shift as the change in the distribution of network activations due to the change in network parameters during training. To improve the training, we seek to reduce the internal covariate shift. By Ô¨Åxing the distribution of the layer inputs xas the training progresses,we expect to improve the training speed.",1,4,"['ratesandcarefulparameterinitialization,andmakesitno -\ntoriously hard to train models with saturating nonlineari-\nties. We refer to this phenomenon as internal covariate\nshift, and address the problem by normalizing layer in-\nputs. Ourmethoddrawsitsstrengthfrommakingnormal-\nizationapartofthemodelarchitectureandperformingthe\nnormalization for each training mini-batch . Batch Nor-\nmalizationallowsustousemuchhigherlearningratesand\nbe less careful about initialization. It also acts as a regu-', 'that the distribution of nonlinearity inputs remains more\nstable as the network trains, then the optimizer would be\nless likely to get stuck in the saturated regime, and the\ntrainingwouldaccelerate.\nWe refer to the change in the distributions of internal\nnodes of a deep network, in the course of training, as In-\nternal Covariate Shift . Eliminating it offers a promise of\nfaster training. We propose a new mechanism, which we\ncallBatch Normalization , that takes a step towards re-', 'that we can match its performance using only 7% of the\ntraining steps, and can further exceed its accuracy by a\nsubstantial margin. Using an ensemble of such networks\ntrained with Batch Normalization, we achieve the top-5\nerror rate that improves upon the best known results on\nImageNetclassiÔ¨Åcation.2 Towards Reducing Internal\nCovariateShift\nWe deÔ¨Åne Internal Covariate Shift as the change in the\ndistribution of network activations due to the change in\nnetworkparametersduringtraining. Toimprovethetrain-', 'callBatch Normalization , that takes a step towards re-\nducing internal covariate shift, and in doing so dramati-\ncally accelerates the training of deep neural nets. It ac-\ncomplishes this via a normalization step that Ô¨Åxes the\nmeansandvariancesoflayerinputs. BatchNormalization\nalso has a beneÔ¨Åcial effect on the gradient Ô¨Çow through\nthe network, by reducing the dependence of gradients\non the scale of the parameters or of their initial values.\nThis allows us to use much higher learning rates with-', 'networkparametersduringtraining. Toimprovethetrain-\ning, we seek to reduce the internal covariate shift. By\nÔ¨Åxingthe distributionof the layer inputs xas the training\nprogresses,weexpecttoimprovethetrainingspeed. Ithas\nbeen long known (LeCunetal., 1998b; Wiesler &Ney,\n2011) that the network training convergesfaster if its in-\nputsarewhitened‚Äìi.e.,linearlytransformedtohavezero\nmeansandunitvariances,anddecorrelated. Aseachlayer\nobservestheinputsproducedbythelayersbelow,itwould', 'computation , 9(8):1735‚Äì1780, 1997.\n[16] S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep\nnetwork training by reducing internal covariate shift. In ICML , 2015.\n[17] H. Jegou, M. Douze, and C. Schmid. Product quantization for nearest\nneighbor search. TPAMI , 33, 2011.\n[18] H. Jegou, F. Perronnin, M. Douze, J. Sanchez, P. Perez, and\nC. Schmid. Aggregating local image descriptors into compact codes.\nTPAMI , 2012.\n[19] Y . Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick,', 'network training by reducing internal covariate shift. arXiv\npreprint arXiv:1502.03167 , 2015.\nJi, X., Henriques, J. F., and Vedaldi, A. Invariant information\nclustering for unsupervised image classiÔ¨Åcation and segmenta-\ntion. In Proceedings of the IEEE International Conference on\nComputer Vision , pp. 9865‚Äì9874, 2019.\nKingma, D. P. and Welling, M. Auto-encoding variational bayes.\narXiv preprint arXiv:1312.6114 , 2013.\nKolesnikov, A., Zhai, X., and Beyer, L. Revisiting self-supervised', 'Ô¨Åtting,inabatch-normalizednetworkwefoundthatitcan\nbeeitherremovedorreducedinstrength.\n4 Experiments\n4.1 Activationsovertime\nTo verify the effects of internal covariate shift on train-\ning, and the ability of Batch Normalization to combat it,\nweconsideredtheproblemofpredictingthedigitclasson\ntheMNISTdataset(LeCunetal.,1998a). Weusedavery\nsimple network, with a 28x28binary image as input, and\n510K20K30K40K50K0.70.80.91\n  \nWithout BN\nWith BN\n‚àí202\n‚àí202\n(a) (b)WithoutBN (c)With BN', 'and Aaron van den Oord. Data-efÔ¨Åcient image recognition with contrastive predictive coding. In\nICML , 2020.\n10Published as a conference paper at ICLR 2021\nSergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by\nreducing internal covariate shift. 2015.\nDiederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR , 2015.\nAlexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan Puigcerver, Jessica Yung, Sylvain Gelly,', 'detailsofensembleandmulticropinferencearesimilarto\n(Szegedyet al., 2014).\nWe demonstrate in Fig. 4 that batch normalization al-\nlowsusto set new state-of-the-artby a healthymarginon\ntheImageNetclassiÔ¨Åcationchallengebenchmarks.\n5 Conclusion\nWe have presented a novel mechanism for dramatically\naccelerating the training of deep networks. It is based on\nthe premise that covariate shift, which is known to com-\nplicate the trainingof machine learning systems, also ap-']",1.0,0.2,0.33333333333333337,1,0.0975609756097561,0.6666666666666666,0.1702127659574468,0.25247457425648573,0.17167381974248927,0.4393305439330544,10,0.05277785265659266,0.0,0.0,0.3333333333333333,1.0
What is NVIDIA GPU Confidential Computing (CC) and how does it secure communication?,single,"NVIDIA CC ensures the confidentiality and integrity of communication between a CVM and a GPU via AES-GCM encryption [ 15]. A critical component of AES- GCM is the Initialization Vector (IV), a unique, non-repeating number (a nonce) required for each encryption session.",1,1,"['their memory, this encryption is separate from that used\nby NVIDIA CC. NVIDIA CC ensures the confidentiality and\nintegrity of communication between a CVM and a GPU via\nAES-GCM encryption [ 15]. A critical component of AES-\nGCM is the Initialization Vector (IV), a unique, non-repeating\nnumber (a nonce) required for each encryption session. As\nwe will show later (¬ß4.1), managing IVs presents a significant\nchallenge.\nFigure 1 illustrates the workflow of data transfers of the', 'GPU to protect sensitive data and models from unauthorized\naccess. Moreover, the data communication between the CVM\nand the GPU enclave is encrypted, further reinforcing the\nsecurity of I/O operations.\nAlthough GPU confidential computing effectively enhances\nsecurity for traditional small-scale AI models, it significantly\nundermines the performance of LLMs in throughput and\nlatency. Our comprehensive experiments on NVIDIA H100\nGPUs reveal that the GPU enclave can incur up to a 52.8%', 'denote ciphertexts moved from the GPU back to the CPU. After the\ntransfers, the current IV of CPU and GPU is 3 and 7, respectively.\nread/write GPU memory and modify the control flow. Hard-\nware GPU confidential computing has low performance over-\nhead and is backward-compatible with existing applications.\nThis paper focuses on studying hardware GPU confidential\ncomputing.\nA closer look at NVIDIA CC. Although CVMs encrypt\ntheir memory, this encryption is separate from that used', 'tion of how NVIDIA Confidential Computing and PipeLLM\nexecute it.\nCPU\nGPU\nGPUCPUDecrypt Encrypt\nSaved\nTime1c 3c\n1t1c\n1t\nPipeLLMNVIDIA\nCC# 1. Swap\xa0 from GPU\n# - PCIe transfer\n# - CPU decryption\ndata = swap_from_gpu()\n# 2. GPU computation\nllm.compute()\n# 3. Load back to GPU\n# - CPU encryption\n# - PCIe transfer\nload_back_to_gpu(data)1t\n1c\n2\n3c\n3tTime\n3t3t3c\n2\nCompute2\nCompute\nFor transparency, NVIDIA Confidential Computing per-\nforms on-the-fly encryption and decryption (indicated by', 'of NVIDIA Confidential Computing, while preserving its (1)\nsecurity guarantees and (2) user transparency. By user trans-\nparency, we mean that PipeLLM applies to non-modified\nLLM applications, including LLM models, deep learning\nframeworks, and any other supporting code and data. Next,\nwe elaborate on PipeLLM‚Äôs threat model.\nThreat model. NVIDIA Confidential Computing aims at\nprotecting the confidentiality and integrity of applications\nrunning on GPUs; for LLM applications, these are the model', 'commands, but this adds substantial performance overhead\ndue to extra runtime checks for indirect memory access,\nheavily used in systems like vLLM [25].\nUnlike software-based solutions, NVIDIA Confidential\nComputing relies on hardware: NVIDIA H100 GPU is the\nfirst commercial implementation with confidential comput-\ning capability [ 15]. Working with CVMs, H100 could build a\nGPU enclave, allowing users to have exclusive control over\nthe GPU and rejecting any access from the host, such as', 'guarantees of NVIDIA Confidential Computing.\nAlthough PipeLLM does not compromise confidentiality\nor integrity, its mis-speculation introduces side channels in\nNOP transfers compared to NVIDIA Confidential Comput-\ning, including (1) attackers can detect if the LLM system is\ncurrently swapping by observing NOPs, and (2) attackers\ncould profile the frequency of prediction failures, potentially\nrevealing the swapping patterns of applications. The security', 'encrypted version on the CPU. This would allow the en-\ncrypted data to be transferred directly to the GPU during\nloading, thereby eliminating encryption overhead. While this\napproach improves performance, implementing it naively\ncompromises security guarantees. For example, reusing en-\ncrypted data enables attackers to identify data that matches\na previous transfer; more critically, it could make the system\nvulnerable to replay attacks [35].\nCurrently, NVIDIA Confidential Computing uses the AES-', 'their memory to guarantee privacy. CVMs generally incur\nminimal performance overhead, averaging around 4% [ 24].\nI/O operations however may introduce significant perfor-\nmance overhead due to data copying and encryption [27].\nConfidential Computing (CC) on GPUs. Beyond CPU-\nbased CVMs, confidential computing on GPUs secures GPU\ncomputations such as LLM serving and training. Soter [40],\ndesigned for edge computing, uses CPU-side confidential\ncomputing to eliminate the trust on GPU hardware. It em-', 'dential computing feature. The second baseline, denoted as\n‚ÄúCC‚Äù (Confidential Computing), showcases the performance\nof the current NVIDIA Confidential Computing framework\nspecifically applied in LLM inference.\n9 0 10 20 30 40\n32/128 256/32Throughput (tokens/s)\nInput/Output Token Length(a)FlexGen OPT-66B\n 0 5 10 15 20\n32/128 256/32\nInput/Output Token Lengthw/o CC CC PipeLLM (b)FlexGen OPT-175B\n 0 1 2 3\nOPT-30B OPT-13BThroughput (sequences/s)\nModel (c)PEFT']",1.0,0.1,0.18181818181818182,1,0.0425531914893617,1.0,0.08163265306122448,0.2126242238890293,0.07,0.5651260504201681,10,0.056608201732842894,1.0,1.0,0.3333333333333333,1.0
What is the difference between torch.disttibuted and torch.distributed.pipelining?,multi,"('While promising for scaling, pipelining is often difficult to implement because it needs to partition the execution of a model in addition to model weights. The partitioning of execution often requires intrusive code changes to your model. Another aspect of complexity comes from scheduling micro-batches in a distributed environment, with data flow dependency considered.', 'The torch.distributed  package provides PyTorch support and communication primitives for multiprocess parallelism across several computation nodes running on one or more machines.')",2,2,"['What is torch.distributed.pipelining ?\nWhile promising for scaling, pipelining is often difficult to implement because it needs to partition\nthe execution of a model in addition to model weights. The partitioning of execution often requires\nintrusive code changes to your model. Another aspect of complexity comes from scheduling\nmicro-batches in a distributed environment, with data flow dependency considered.\nThe pipelining  package provides a toolkit that does said things automatically which allows easy', 'splitting the module. (default: None)\nReturn type:\nA pipeline representation of class Pipe.\nclass torch.distributed.pipelining. Pipe (split_gm , num_stages ,\nhas_loss_and_backward , loss_spec )\ntorch.distributed.pipelining. pipe_split ( )\npipe_split is a special operator that is used to mark the boundary between stages in a module.\nIt is used to split the module into stages. It is a no-op if your annotated module is run eagerly.\nExample\nThe above example will be split into two stages.\nMicrobatch Utilities', 'model to be partitioned such that multiple micro-batches can execute different parts of the mode\ncode concurrently. Pipeline parallelism can be an effective technique for:\nlarge-scale training\nbandwidth-limited clusters\nlarge model inference\nThe above scenarios share a commonality that the computation per device cannot hide the\ncommunication of conventional parallelism, for example, the weight all-gather of FSDP.\nWhat is torch.distributed.pipelining ?', 'First, the pipeline  API turns our model into a directed acyclic graph (DAG) by tracing the model. \ntraces the model using torch.export  ‚Äì a PyTorch 2 full-graph capturing tool.\nThen, it groups together the operations and parameters needed by a stage into a reconstructed\nsubmodule: submod_0, submod_1, ‚Ä¶from torch.distributed.pipelining  import build_stage\nfrom torch.nn.parallel  import DistributedDataParallel\ndp_mod = DistributedDataParallel (stage_mod )\ninfo = pipe.info()', 'Pipeline Parallelism\nCreated On: Jun 16, 2025 | Last Updated On: Jun 16, 2025\ntorch.distributed.pipelining  is currently in alpha state and under development. API\nchanges may be possible. It was migrated from the PiPPy project.\nWhy Pipeline Parallel?\nPipeline Parallelism is one of the primitive parallelism for deep learning. It allows the execution of \nmodel to be partitioned such that multiple micro-batches can execute different parts of the mode', 'Policy.10/10/25, 3:04 PM Pipeline Parallelism ‚Äî PyTorch 2.8 documentation\nhttps://docs.pytorch.org/docs/stable/distributed.pipelining.html 9/17[source\n[sourceLogging\nYou can turn on additional logging using the TORCH_LOGS  environment variable from torch._logging\nTORCH_LOGS=+pp  will display logging.DEBUG  messages and all levels above it.\nTORCH_LOGS=pp  will display logging.INFO  messages and above.\nTORCH_LOGS=-pp  will display logging.WARNING  messages and above.\nAPI Reference\nModel Split APIs', 'navigating, you agree to allow our usage of cookies. As the current maintainers of this site,\nFacebook ºs Cookies Policy applies. Learn more, including about available controls: Cookies\nPolicy.10/10/25, 3:04 PM Pipeline Parallelism ‚Äî PyTorch 2.8 documentation\nhttps://docs.pytorch.org/docs/stable/distributed.pipelining.html 12/17[source\n[sourceclass torch.distributed.pipelining.stage. PipelineStage (submodule ,\nstage_index , num_stages , device, input_args =None, output_args =None,', 'Distributed communication package -\ntorch.distributed\nCreated On: Jul 12, 2017 | Last Updated On: Jul 14, 2025\nPlease refer to PyTorch Distributed Overview for a brief introduction to all features related\nto distributed training.\nBackends\ntorch.distributed  supports three built-in backends, each with different capabilities. The table\nbelow shows which functions are available for use with CPU / CUDA tensors. MPI supports CUDA\nonly if the implementation used to build PyTorch supports it.\nBackend gloo mpi nccl', 'Policy.10/10/25, 3:04 PM Pipeline Parallelism ‚Äî PyTorch 2.8 documentation\nhttps://docs.pytorch.org/docs/stable/distributed.pipelining.html 7/17Alternatively, if you would like to build the stage runtime later after some modification to the\nstage_mod, you can use a functional version of the build_stage  API. For example:\nThe pipeline  frontend uses a tracer (torch.export) to capture your model into a single\ngraph. If your model is not full-graph ºable, you can use our manual frontend below.', 'navigating, you agree to allow our usage of cookies. As the current maintainers of this site,\nFacebook ºs Cookies Policy applies. Learn more, including about available controls: Cookies\nPolicy.10/10/25, 3:04 PM Pipeline Parallelism ‚Äî PyTorch 2.8 documentation\nhttps://docs.pytorch.org/docs/stable/distributed.pipelining.html 3/17the Transformer class defined in Torchtitan shows a pattern of building an easily partitionable\nmodel.']",0.5,0.1,0.16666666666666669,1,0.13333333333333333,1.0,0.23529411764705882,0.2287861725830361,0.11776859504132231,0.7337278106508875,10,0.09054061013044125,1.0,0.5,0.3333333333333333,0.5
What optimizations does FlashAttention introduce compared to standard attention kernels implemented in PyTorch?,multi,"('FMHA targets BERT models, it only supports head dimension 64, and only runs on A100 GPUs. FMHA fuses the attention computation dropout¬πsoftmax¬πmask¬πQK>¬∫¬∫¬∫Vinto one CUDA kernel. In the forward pass, it stores the attention matrix softmax¬πmask¬πQKùëá¬∫¬∫to HBM to be used in gradient computation. As a result, it does not oÔ¨Äer substantial memory saving (though for shorter sequences memory footprint is often not a primary concern).', 'This module contains functions and classes that alter the behavior of torch.nn.functional.scaled_dot_product_attention', 'flex_attention This module implements the user facing API for flex_attention in PyTorch. bias Defines bias subclasses that work with scaled_dot_product_attention')",3,2,"['tention and block-sparse FlashAttention compared to the baselines in exact, approximate, and sparse\nattention (exact numbers in Appendix E). Runtime grows quadratically with sequence length, but FlashAt-\ntention runs signiÔ¨Åcantly faster than exact attention baselines, up to 3 \x02faster than the PyTorch\nimplementation. The runtimes of many approximate/sparse attention mechanisms grow linearly with se-\nquence length, but FlashAttention still runs faster than approximate and sparse attention for short', 'HBM. We implement FlashAttention in CUDA to achieve Ô¨Åne-grained control over memory access and\nfuse all the attention operations into one GPU kernel. Even with the increased FLOPs due to recomputation,\nour algorithm both runs faster (up to 7.6x on GPT-2 [ 67], Figure 1 right) and uses less memory ‚Äîlinear\nin sequence length‚Äîthan standard attention, thanks to the massively reduced amount of HBM access.\nWe analyze the IO complexity [ 1] ofFlashAttention , proving that it requires ùëÇ¬πùëÅ2ùëë2ùëÄ\x001¬∫HBM', 'computation back to HBM. Right:Speedup over the PyTorch implementation of attention on GPT-2.\nFlashAttention does not read and write the large ùëÅ\x02ùëÅattention matrix to HBM, resulting in an 7.6 \x02\nspeedup on the attention computation.\nGPUs, compute speed has out-paced memory speed [ 61,62,63], and most operations in Transformers are\nbottlenecked by memory accesses [ 43]. IO-aware algorithms have been critical for similar memory-bound', 'time of all models. Each task has a diÔ¨Äerent sequence length varying between 1024 and 4096. We follow the\nimplementation and experimental setting in Tay et al. [80]and Xiong et al. [90].3Table 3 shows that FlashAt-\ntention achieves up 2.4\x02speed-up compared to standard attention. Block-sparse FlashAttention is\nfaster than all of the approximate attention methods that we have tested.\nTable 3: The performance of standard attention, FlashAttention , block-sparse FlashAttention , and approximate', 'wall-clock time. We analyze its IO complexity, showing that our method requires much fewer HBM accesses\ncompared to standard attention. We further show that FlashAttention can serve as a useful primitive by\nextending it to handle block-sparse attention.\nWe focus here on the forward pass for ease of exposition; Appendix B contains details for the backward.\n3.1 An EÔ¨Écient Attention Algorithm With Tiling and Recomputation', 'We analyze the IO complexity [ 1] ofFlashAttention , proving that it requires ùëÇ¬πùëÅ2ùëë2ùëÄ\x001¬∫HBM\naccesses where ùëëis the head dimension and ùëÄis the size of SRAM, as compared to Œ©¬πùëÅùëë¬∏ùëÅ2¬∫of standard\nattention. For typical values of ùëëandùëÄ,FlashAttention requires many times fewer HBM accesses\ncompared to standard attention (up to 9 \x02fewer, as shown in Fig. 2). Moreover, we provide a lower bound,\nshowing that no exact attention algorithm can asymptotically improve on the number of HBM accesses over\nall SRAM sizes.', 'make them efficiency in practice. They inspired a novel hybrid sparse attention architecture that\nmeets several desiderata that we find crucial for achieving both practical efficiency gains and strong\naccuracy on downstream tasks, called as Head-Heterogenous Strided Transformer ( HHST ). We will\nopen-source our kernel library and make it a plug-in-and-play alternative for FlashAttention-2 module\nin popular training frameworks like Megatron and Pytorch. We also integrated S2-A TTENTION into', 'In Fig. 2 (left), we see that even though FlashAttention has higher FLOP count compared to standard\nattention (due to recomputation in the backward pass), it has much fewer HBM accesses, resulting in much\nfaster runtime. In Fig. 2 (middle), we vary the block size ùêµùëêofFlashAttention , which results in diÔ¨Äerent\namounts of HBM accesses, and measure the runtime of the forward pass. As block size increases, the number', 'its theoretical reduction in FLOPs has rarely translated into wall-clock speed-up\nover its dense attention counterparts, mainly due to the lack of hardware-level\noptimizations like FlashAttention (Dao, 2023). Meanwhile, it remains unclear\nwhether sparse attention can maintain the model‚Äôs quality at the scale of today‚Äôs\nlarge language models (LLMs), and how this can be achieved. This paper presents\nSparsely-Sharded Attention ( S2-A TTENTION ), an optimized Triton kernel library', 'an approximate attention algorithm that is faster than any existing approximate attention method.\nFlashAttention trains Transformers faster than existing baselines: 15% end-to-end wall-clock speedup\non BERT-large (seq. length 512) compared to the MLPerf 1.1 training speed record, 3 \x02speedup on\nGPT-2 (seq. length 1K), and 2.4 \x02speedup on long-range arena (seq. length 1K-4K). FlashAttention\nand block-sparse FlashAttention enable longer context in Transformers, yielding higher quality models']",0.0,0.0,0.0,0,0.021739130434782608,0.14285714285714285,0.03773584905660378,0.24492047796697275,0.22244094488188976,0.2787878787878788,10,4.638258647124538e-05,0.0,0.0,0.0,0.0
What problem does the Model Context Protocol (MCP) solve?,single,MCP (Model Context Protocol) Connecting (N) LLMs to (M) external tools/resources used to be a NxM problem MCP standardizes the LLM-tool communication into a N->1->M process Build with a client-server model MCP client: the agent that needs to call tool/data MCP server: a service to expose external tools and data sources,1,5,"['https://lilianweng.github.io/posts/2023-06-23-agent/\nMCP (Model Context Protocol)‚óèConnecting (N) LLMs to (M) external tools/resources used to be a NxM problem ‚óèMCP standardizes the LLM-tool communication into a N->1->M process ‚óèBuild with a client-server model ‚óèMCP client: the agent that needs to call tool/data ‚óèMCP server: a service to expose external tools and data sources', 'LLMs + training for tool use: ToolformerMCP (Model Context Protocol)‚óèConnecting (N) LLMs to (M) external tools/resources used to be a NxM problem ‚óèMCP standardizes the LLM-tool communication into a N->1->M process ‚óèBuild with a client-server model ‚óèMCP client: the agent that needs to call tool/data ‚óèMCP server: a service to expose external tools and data sources', 'communication overhead. And the largest model Llama3-8x70B could not be trained using only\nTP+EP due to memory constraints. (4) MCore framework leverages pipeline parallelism (PP) in\naddition to TP, EP, and DP, achieves a better balance between communication and computation. This\nresults in higher MFU values, reaching 46.3% on Mixtral-8x22B and 35.3% on Qwen-2-57B. By\neffectively partitioning the model across pipeline stages, MCore reduces the memory footprint per', 'superior performance. (2) ETP in the MoE layer introduces substantially higher communication\noverhead compared to EP, with this effect being particularly pronounced in fine-grained MoE models.\n(3) Fine-grained MoE models exhibit notably lower computation-to-communication ratios. When\nETPxEP exceeds 8, necessitating inter-node communication, communication overhead dominates,\nFigure 4: Context-scaling experiments by increasing context length and number of GPUs up to 128K\nand 1024.\n9(a) Mixtral 8x22B model', 'AI Agent/Workflow Frameworks‚óèFrameworks initially proposed to standardize AI workflows, provide some out-of-box design patterns and abstractions ‚óèSome examples ‚óèLangChain: Came out the earliest, probably the most popular and hardest to use ‚óèLlamaIndex: Good RAG support ‚óèCrewAI and Camel: multi-agent framework for more complex tasks ‚óèBut a lot of unnecessary, added complexity for agents, harder to customize ‚óèMy experience of what‚Äôs the easiest and sufficient for many tasks ‚óèNo framework (pure Python) ‚óèNo MCP (can just write your own functions or hooks) ‚óèNo A2A (no need for multi-agent)What is AI Agent Infra?‚óèAgent testing and evaluation ‚óãUnit + e2e test, metrics, benchmarks, human-in-the-loop ‚óèAgent autotuning and optimization ‚óãAutomated prompt tuning, model selection, tool selection, workflow optimization ‚óèAgent hosting ‚óãServerless or long-running?  ‚óãStateful or stateless? ‚óèTooling, memory, dataDemo Time: Eigent Computer-Use Agent performing a Discord summarization taskOutline‚óèTransformer primer ‚óèIntroduction oriented for LLM infra (perf problems), not the theory ‚óèLLM performanceSelf Attention', '(b)Backward Computation.\nFigure 11: Communication Conflict\nmunication, conflicts caused by asynchronous communica-\ntion between EP and DP/PP can be reduced by setting priori-\nties for different communication primitives: EP¬øPP¬øCP¬øDP,\nensuring the efficiency of EP AllToAll communication.\nCluster Expansion\nThis section analyzes the cluster expansion handling meth-\nods for long context MoE training in MoNTA implemen-\ntations. In the training of the MoE model, it typically in-', 'Equipping Agents: The Power of Tooling‚Ä¢Tools: external functions, APIs, or even another (utility) agent (e.g., MCP server)\n‚Ä¢Agents can often decide when to call tools and what tools to call\n‚Ä¢Common tools\n‚Ä¢Web search + crawling\n‚Ä¢Browser\n‚Ä¢Social media, email hooks\n‚Ä¢Code + CLI executionAgent Memory: Knowledge, History, State‚Ä¢LLMs only have short-term memory (i.e., context window)\n‚Ä¢Many agents needs long-term memory and/or internal/external knowledge', 'effectively partitioning the model across pipeline stages, MCore reduces the memory footprint per\nGPU and overlaps communication with computation more efficiently. However, the coupling of\nparallelism strategies between the Attention and MoE layers renders the mappings sub-optimal for\nMoE models. (5) MCore with MoE Parallel Folding: further enhances training efficiency, achieving\nthe highest MFU values across all models: 49.3% for Mixtral-8x22B, 41.6% on Llama3-8x70B,', 'tations. In the training of the MoE model, it typically in-\nvolves multiple training steps with Context lengths ranging\nfrom 4K to 128K, and even up to 1M tokens. This paper\nproposes a distributed parallel training extension method for\nLong Context mixture of expert models. Based on cluster re-\nsource parameters, model parameters, and Context length, a\nstrategy for expanding expert model distributed parallelism\nis generated by combining Expert Parallelism and Context\nParallelism.', 'AI Agent/Workflow Frameworks‚óèFrameworks initially proposed to standardize AI workflows, provide some out-of-box design patterns and abstractions ‚óèSome examples ‚óèLangChain: Came out the earliest, probably the most popular and hardest to use ‚óèLlamaIndex: Good RAG support ‚óèCrewAI and Camel: multi-agent framework for more complex tasks ‚óèBut a lot of unnecessary, added complexity for agents, harder to customize ‚óèMy experience of what‚Äôs the easiest and sufficient for many tasks ‚óèNo framework (pure Python) ‚óèNo MCP (can just write your own functions or hooks) ‚óèNo A2A (no need for multi-agent)What is AI Agent Infra?‚óèAgent testing and evaluation ‚óãUnit + e2e test, metrics, benchmarks, human-in-the-loop ‚óèAgent autotuning and optimization ‚óãAutomated prompt tuning, model selection, tool selection, workflow optimization ‚óèAgent hosting ‚óãServerless or long-running?  ‚óãStateful or stateless? ‚óèTooling, memory, dataDemo Time: Eigent Computer-Use Agent performing a Discord summarization task']",1.0,0.2,0.33333333333333337,1,0.1,1.0,0.18181818181818182,0.2896665234277881,0.049586776859504134,0.8763736263736264,10,0.04615897693334086,1.0,1.0,0.6666666666666666,1.0
What search algorithm does AlphaZero use instead of alpha-beta search?,single,"Instead of an alpha-beta search with domain-speciÔ¨Åc enhancements, AlphaZero uses a general-purpose Monte-Carlo tree search (MCTS) algorithm. Each search consists of a series of simulated games of self-play that traverse a tree from root srootto leaf. Each simulation proceeds by selecting in each state sa moveawith low visit count, high move probability and high value (averaged over the leaf states of simulations that selected afroms) according to the current neural network.",1,1,"['v\x19E[zjs].AlphaZero learns these move probabilities and value estimates entirely from self-\nplay; these are then used to guide its search.\nInstead of an alpha-beta search with domain-speciÔ¨Åc enhancements, AlphaZero uses a general-\npurpose Monte-Carlo tree search (MCTS) algorithm. Each search consists of a series of simu-\nlated games of self-play that traverse a tree from root srootto leaf. Each simulation proceeds by\nselecting in each state sa moveawith low visit count, high move probability and high value', 'out when evaluating a large subtree. In contrast, alpha-beta search computes an explicit mini-\nmax, which propagates the biggest approximation errors to the root of the subtree. Using MCTS\nmay allow AlphaZero to effectively combine its neural network representations with a powerful,\ndomain-independent search.\nDomain Knowledge\n1. The input features describing the position, and the output features describing the move,\nare structured as a set of planes; i.e. the neural network architecture is matched to the', 'An approach based on training dual policy and value networks using AlphaZero -like policy\niteration was successfully applied to improve on the state-of-the-art in Hex ( 3).\n11MCTS and Alpha-Beta Search\nFor at least four decades the strongest computer chess programs have used alpha-beta search\n(18, 23 ).AlphaZero uses a markedly different approach that averages over the position evalu-\nations within a subtree, rather than computing the minimax evaluation of that subtree. How-', '(see Table 1).\nWe also analysed the relative performance of AlphaZero ‚Äôs MCTS search compared to the\nstate-of-the-art alpha-beta search engines used by StockÔ¨Åsh andElmo .AlphaZero searches just\n80 thousand positions per second in chess and 40 thousand in shogi, compared to 70 million\nforStockÔ¨Åsh and 35 million for Elmo .AlphaZero compensates for the lower number of evalu-\nations by using its deep neural network to focus much more selectively on the most promising', 'ations within a subtree, rather than computing the minimax evaluation of that subtree. How-\never, chess programs using traditional MCTS were much weaker than alpha-beta search pro-\ngrams, ( 4, 24 ); while alpha-beta programs based on neural networks have previously been un-\nable to compete with faster, handcrafted evaluation functions.\nAlphaZero evaluates positions using non-linear function approximation based on a deep', 'AlphaZero evaluates positions using non-linear function approximation based on a deep\nneural network, rather than the linear function approximation used in typical chess programs.\nThis provides a much more powerful representation, but may also introduce spurious approxi-\nmation errors. MCTS averages over these approximation errors, which therefore tend to cancel\nout when evaluating a large subtree. In contrast, alpha-beta search computes an explicit mini-', 'and a tabula rasa reinforcement learning algorithm.\nInstead of a handcrafted evaluation function and move ordering heuristics, AlphaZero utilises\na deep neural network (p;v) =f\x12(s)with parameters \x12. This neural network takes the board po-\nsitionsas an input and outputs a vector of move probabilities pwith components pa=Pr(ajs)\n2for each action a, and a scalar value vestimating the expected outcome zfrom position s,\nv\x19E[zjs].AlphaZero learns these move probabilities and value estimates entirely from self-', 'the-art programs are based on powerful engines that search many millions of positions, leverag-\ning handcrafted domain expertise and sophisticated domain adaptations. AlphaZero is a generic\nreinforcement learning algorithm ‚Äì originally devised for the game of Go ‚Äì that achieved su-\nperior results within a few hours, searching a thousand times fewer positions, given no domain\n3The prevalence of draws in high-level chess tends to compress the Elo scale, compared to shogi or Go.', 'ons ( 5). These programs use a similar algorithm to computer chess programs, again based on a\nhighly optimised alpha-beta search engine with many domain-speciÔ¨Åc adaptations.\nGo is well suited to the neural network architecture used in AlphaGo because the rules of\nthe game are translationally invariant (matching the weight sharing structure of convolutional\nnetworks), are deÔ¨Åned in terms of liberties corresponding to the adjacencies between points', 'scaled more effectively with thinking time than either StockÔ¨Åsh orElmo , calling into question\nthe widely held belief ( 4, 11 ) that alpha-beta search is inherently superior in these domains.3\nFinally, we analysed the chess knowledge discovered by AlphaZero . Table 2 analyses the\nmost common human openings (those played more than 100,000 times in an online database\nof human chess games ( 1)). Each of these openings is independently discovered and played']",1.0,0.1,0.18181818181818182,1,0.09090909090909091,1.0,0.16666666666666669,0.275945791189619,0.09803921568627451,0.7235294117647059,10,0.07228122536695612,1.0,1.0,0.3333333333333333,1.0
Why can cProfile and the CPU-mode autograd profiler give incorrect timings on CUDA code?,single,"Due to the asynchronous nature of CUDA kernels, when running against CUDA code, the cProfile output and CPU-mode autograd profilers may not show correct timings: the reported CPU time includes only the time to launch kernels but not their GPU execution time unless synchronization occurs.",1,5,"['torch.utils.bottleneck  -h for more usage instructions.\nBecause your script will be profiled, please ensure that it exits in a finite amount of time.\nDue to the asynchronous nature of CUDA kernels, when running against CUDA code, the\ncProfile output and CPU-mode autograd profilers may not show correct timings: the\nreported CPU time reports the amount of time used to launch the kernels but does not\ninclude the time the kernel spent executing on a GPU unless the operation does a', 'include the time the kernel spent executing on a GPU unless the operation does a\nsynchronize. Ops that do synchronize appear to be extremely expensive under regular\nCPU-mode profilers. In these case where timings are incorrect, the CUDA-mode autograd\nprofiler may be helpful.python  -m  torch .utils .bottleneck  /path /to /source /script .py  [args]\nWarning ‚ö†\nWarning ‚ö†\nTo analyze traffic and optimize your experience, we serve cookies on this site. By clicking or', 'Similarly, Intel¬Æ VTune‚Ñ¢ Profiler  helps to analyze performance on Intel platforms\nfurther with torch.autograd.profiler.emit_itt() .\nIf you are profiling CUDA code, the first profiler that bottleneck  runs (cProfile) will\ninclude the CUDA startup time (CUDA buffer allocation cost) in its time reporting. This\nshould not matter if your bottlenecks result in code much slower than the CUDA startup\ntime.\nFor more complicated uses of the profilers (like in a multi-GPU case), please see', 'should first check if your script is CPU-bound (‚ÄúCPU total time is much greater than CUDA\ntotal time‚Äù). If it is CPU-bound, looking at the results of the CPU-mode autograd profiler\nwill help. If on the other hand your script spends most of its time executing on the GPU,\nthen it makes sense to start looking for responsible CUDA operators in the output of the\nCUDA-mode autograd profiler.\nOf course the reality is much more complicated and your script might not be in one of', 'Consequently, if any checkpointed functions involve randomness, this may result in\nincorrect gradients. (Note that if CUDA devices are among the devices detected, it will be\nprioritized; otherwise, the first device encountered will be selected.) If there are no CPU-\ntensors, the default device type state (default value is cuda, and it could be set to other\ndevice by DefaultDeviceType) will be saved and restored. However, the logic has no way', 'To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or\nnavigating, you agree to allow our usage of cookies. As the current maintainers of this site,\nFacebook ºs Cookies Policy applies. Learn more, including about available controls: Cookies\nPolicy.10/10/25, 3:03 PM torch.utils.bottleneck ‚Äî PyTorch 2.8 documentation\nhttps://docs.pytorch.org/docs/stable/bottleneck.html 1/3To decide which (CPU-only-mode or CUDA-mode) autograd profiler output to look at, you', 'fast CPU thread aggressively allocates GPU memory blocks and\ncauses defragmentations. If it is difficult to identify with certainty\nfrom latency measurements or profiled traces, CUDA malloc retry\ncan serve as a helpful indicator, which can be obtained from the\nnum_alloc_retries key in the torch.cuda.memory_stats() dictionary.\nThe experiments conducted with T5 models have demonstrated\nthat the rate limiter technique can greatly benefit training efficiency,611M 2.28B 11.3B', 'down. The next feature in line will be autograd that will properly create the autograd graph and then\nredispatch down. Finally, we reach the backend kernel for CUDA which will launch the right CUDA\nkernel and return the final result. On the way out, autograd will attach the graph to the output and,\nfinally, autocast will have a chance to do any update it needs on exit.\nOne configuration of the dispatcher is the order in which all these feature and backend keys are', 'havior to overlap with gradient computation is challenging\nbecause PyTorch automatically generates the gradient com-\nputation graph [69]. To precisely control communication\nstart/end time, our initial attempt to manually implement cus-\ntomized backward pass leads to poor throughput performance\ndue to triggering less efficient kernels than torch.autograd().\nTo tackle this issue, we developed a no-operation module.\nThis module receives the communication handle during the', 'and less on-device memory copy overhead. Taking GPT-3\n13B training as an example, with sequence length of 512 and\nmicro-batch size of 4, we notice significant training iteration\ntime reduction (around 10-15%) if we switch from cudaGraph\noff mode to on mode, which shows the benefits of reducing\nidle time between adjacent compute kernels. On the other\nhand, if we increase the micro-batch size to 16, enabling cud-\naGraph leads to 5-10% longer iteration time than disabling']",1.0,0.1,0.18181818181818182,1,0.08108108108108109,1.0,0.15,0.22648854275428842,0.11627906976744186,0.5333333333333333,10,0.04126754922177097,1.0,1.0,0.3333333333333333,1.0
Why can‚Äôt you perform data-dependent operations on meta tensors?,single,"Because meta tensors do not have real data, you cannot perform data-dependent operations like torch.nonzero()  or item() . In some cases, not all device types (e.g., CPU and CUDA) have exactly the same output metadata for an operation",1,5,"['represent the actual tensors. Because meta tensors do not have real data, you cannot perform\ndata-dependent operations like torch.nonzero()  or item() . In some cases, not all device\ntypes (e.g., CPU and CUDA) have exactly the same output metadata for an operation; we\ntypically prefer representing the CUDA behavior faithfully in this situation.\nAlthough in principle meta tensor computation should always be faster than an equivalent', 'make transformations on the model before you load the actual data.\nMost operations can be performed on meta tensors, producing new meta tensors that describe\nwhat the result would have been if you performed the operation on a real tensor. You can use\nthis to perform abstract analysis without needing to spend time on compute or space to\nrepresent the actual tensors. Because meta tensors do not have real data, you cannot perform', 'a device for initialization:\nYou cannot convert a meta tensor directly to a CPU/CUDA tensor, because the meta tensor stores\nno data and we do not know what the correct data values for your new tensor are:\nUse a factory function like torch.empty_like()  to explicitly specify how you would like the\nmissing data to be filled in.\nNN modules have a convenience method torch.nn.Module.to_empty()  that allows you to move', 'Meta device\nCreated On: Jun 17, 2025 | Last Updated On: Jun 17, 2025\nThe ‚Äúmeta‚Äù device is an abstract device which denotes a tensor which records only metadata, but\nno actual data. Meta tensors have two primary use cases:\nModels can be loaded on the meta device, allowing you to load a representation of the model\nwithout actually loading the actual parameters into memory. This can be helpful if you need to\nmake transformations on the model before you load the actual data.', ""Although in principle meta tensor computation should always be faster than an equivalent\nCPU/CUDA computation, many meta tensor implementations are implemented in Python\nand have not been ported to C++ for speed, so you may find that you get lower absolute\nframework latency with small CPU tensors.\nIdioms for working with meta tensors\nAn object can be loaded with torch.load()  onto meta device by specifying\nmap_location='meta' :\n>>> torch.save(torch.randn(2), 'foo.pt' )"", 'Policy.10/10/25, 3:03 PM Meta device ‚Äî PyTorch 2.8 documentation\nhttps://docs.pytorch.org/docs/stable/meta.html 1/4If you have some arbitrary code which performs some tensor construction without explicitly\nspecifying a device, you can override it to instead construct on meta device by using the\ntorch.device()  context manager:\nThis is especially helpful NN module construction, where you often are not able to explicitly pass in\na device for initialization:', 'torch\nCreated On: Dec 23, 2016 | Last Updated On: Mar 10, 2025\nThe torch package contains data structures for multi-dimensional tensors and defines mathematic\noperations over these tensors. Additionally, it provides many utilities for efficient serialization of\nTensors and arbitrary types, and other useful utilities.\nIt has a CUDA counterpart, that enables you to run your tensor computations on an NVIDIA GPU\nwith compute capability >= 3.0.\nTensors\nis_tensorReturns True if obj is a PyTorch tensor.', 'has thrown an error, this value() method will also throw an error.\nReturn type:\nT\nwait ( )\nBlock until the value of this Future is ready.\nIf the value contains tensors that reside on GPUs, then an additional synchronization is\nperformed with the kernels (executing on the device) which may be asynchronously\npopulating those tensors. Such sync is non-blocking, which means that wait() will inser\nthe necessary instructions in the current streams to ensure that further operations', 'dimensional structures can be partitioned and processed in\nparallel, making them highly suitable for large-scale computa-\ntion. For example, tf.data.Dataset [43] can organize various\nraw data types (e.g., images, text) into a unified tensor format,\nready for direct use by models. However, tensor formats, due\nto their dense multi-dimensional storage, incur large storage\noverhead and offer poor readability, and are typically adopted\nonly in model training.\nModel Data Format. Model storage formats need to pay', 'with minimal changes among backends and operations. These\noptimizations can be applied to incoming messages with only\na few lines of Python code before routing the operation to its\nrespective C++ backend.\nThere are two parameters for Tensor Fusion: the maximum\nfusion buffer size Band the maximum time Tto wait\nfor that fusion buffer to Ô¨Åll with small tensors. MCR-DL\nintroduces a small optimization for Tensor Fusion, where if\nthe Fusion buffer does not reach Bbefore T(and therefore']",1.0,0.2,0.33333333333333337,1,0.09090909090909091,1.0,0.16666666666666669,0.1988586550363367,0.12317327766179541,0.5368663594470046,10,0.049042746052530124,1.0,1.0,0.3333333333333333,1.0
