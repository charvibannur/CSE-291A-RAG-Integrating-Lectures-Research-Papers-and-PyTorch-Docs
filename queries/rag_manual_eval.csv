filename,query,gold-doc
18_alphazero_1712_01815.txt,What search algorithm does AlphaZero use instead of alpha-beta search?,"Instead of an alpha-beta search with domain-speciﬁc enhancements, AlphaZero uses a general-purpose Monte-Carlo tree search (MCTS) algorithm. Each search consists of a series of simulated games of self-play that traverse a tree from root srootto leaf. Each simulation proceeds by selecting in each state sa moveawith low visit count, high move probability and high value (averaged over the leaf states of simulations that selected afroms) according to the current neural network."
05_batchnorm_1502_03167.txt,"What is internal covariate shift, and how does it affect training?","We deﬁne Internal Covariate Shift as the change in the distribution of network activations due to the change in network parameters during training. To improve the training, we seek to reduce the internal covariate shift. By ﬁxing the distribution of the layer inputs xas the training progresses,we expect to improve the training speed."
23_flashattention_2205_14135.txt,What does “IO-aware” mean in the context of FlashAttention?,"In this paper, we argue that a missing principle is making attention algorithms IO-aware [1]—that is, carefully accounting for reads and writes to diﬀerent levels of fast and slow memory (e.g., between fast GPU on-chip SRAM and relatively slow GPU high bandwidth memory, or HBM [ 45], Figure 1 left)."
bottleneck.txt,Why can cProfile and the CPU-mode autograd profiler give incorrect timings on CUDA code?,"Due to the asynchronous nature of CUDA kernels, when running against CUDA code, the cProfile output and CPU-mode autograd profilers may not show correct timings: the reported CPU time includes only the time to launch kernels but not their GPU execution time unless synchronization occurs."
cmu_llmsys-05-dl-framework.txt,How does auto-differentiation work in these frameworks?,"TensorFlow is an interface for expressing machine learning algorithms and an implementation for executing such algorithms •PyTorch  is a programming framework for tensor computation, deep learning, and auto differentiation 6Deep Learning Programming Framework7Aspect PyTorch TensorFlow JAX NumPy Primary Use Deep learning Deep learning numerical and ML computing numerical computing Programming Paradigm Dynamic (eager execution)Static (Graph mode, or Eager)Functional transformations Procedural Auto grad dynamic comp graphstatic comp graph Functional -based with grad/jit"
cmu_llmsys-13-distributed-training.txt,"What are FlashMLA, DeepEP, and DeepGEMM, and what problems do they each solve?","Accelerating Transformer Layers•FlashMLA  (released 2/24/2025) FlashMLA  is an efficient MLA decoding kernel for Hopper GPUs, optimized for variable -length sequences serving. DeepEP  (released 2/25/2025) oa communication library tailored for Mixture -of-Experts ( MoE) and expert parallelism (EP). It provides high -throughput and low -latency all -to-all GPU kernels, which are also as known as MoE dispatch and combine. DeepGEMM  (released 2/26/2025) oDeepGEMM  is a library designed for clean and efficient FP8 General Matrix Multiplications (GEMMs) with fine -grained scaling 3Deepseek  opensource libraries"
yiying_llm-perf.txt,What problem does the Model Context Protocol (MCP) solve?,MCP (Model Context Protocol) Connecting (N) LLMs to (M) external tools/resources used to be a NxM problem MCP standardizes the LLM-tool communication into a N->1->M process Build with a client-server model MCP client: the agent that needs to call tool/data MCP server: a service to expose external tools and data sources
TinyServe_ Query-Aware Cache Selection for Efficient LLM Serving.txt,What are the three core components of the TinyServe system?,"The system is organized around three core components: (1)Query-Aware KV Retriever: Dynamically selects relevant key-value blocks at decode time based on the current query vector and page-level metadata, reducing unnecessary mem- ory access. (2)Modular Scheduling Pipeline: A dispatch loop handles incoming queries and routes them through configurable plug- ins (e.g., entropy-based early exit, token-level pruning, ap- proximate attention). This modular design allows experimen- tation with different sparsity strategies without modifying the core model. (3)Sparse Attention Executor: Efficiently computes attention over selected KV pages using fused CUDA kernels, with support for FP16/INT8 KV formats and multi-GPU dispatch."
PipeLLM_ Fast and Confidential Large Language Model Services with Speculative .txt,What is NVIDIA GPU Confidential Computing (CC) and how does it secure communication?,"NVIDIA CC ensures the confidentiality and integrity of communication between a CVM and a GPU via AES-GCM encryption [ 15]. A critical component of AES- GCM is the Initialization Vector (IV), a unique, non-repeating number (a nonce) required for each encryption session."
meta.txt,Why can’t you perform data-dependent operations on meta tensors?,"Because meta tensors do not have real data, you cannot perform data-dependent operations like torch.nonzero()  or item() . In some cases, not all device types (e.g., CPU and CUDA) have exactly the same output metadata for an operation"
