filename,query,gold-text
"['01_alexnet_imagenet_2012.txt']","What dataset did AlexNet train on and why was it important?","('We trained a large convolutional neural network on the ImageNet LSVRC-2010 dataset (1.2M high-resolution images, 1000 classes); ImageNet provided the scale necessary to demonstrate deep CNNs\' effectiveness for large-scale visual recognition.',)"
"['02_resnet_1512_03385.txt']","What key architectural change does ResNet introduce to enable training much deeper networks?","('ResNet introduces residual connections (identity shortcuts) that let gradients bypass layers, enabling effective training of very deep networks by mitigating vanishing gradients.',)"
"['03_vgg_1409_1556.txt']","What is the main design principle of VGG networks?","('VGG uses a very uniform architecture of small 3x3 convolutional filters stacked deeply, showing that increased depth with simple building blocks improves representation.',)"
"['04_googlenet_1409_4842.txt']","How did GoogLeNet (Inception) change convolutional network design?","('GoogLeNet introduced Inception modules that compute multiple filter sizes in parallel and concatenate outputs, improving efficiency and representational power with fewer parameters.',)"
"['05_batchnorm_1502_03167.txt']","What problem does Batch Normalization address and how?","('BatchNorm reduces internal covariate shift by normalizing layer inputs across a mini-batch and applying learned scale and shift, which stabilizes and speeds training.',)"
"['06_dropout_1207_0580.txt']","Why is dropout used during training and what effect does it have?","('Dropout randomly zeros activations during training to prevent co-adaptation and overfitting, acting like model ensembling and improving generalization.',)"
"['07_transformer_1706_03762.txt']","What are the two main components of the Transformer architecture?","('The Transformer uses multi-head self-attention layers and position-wise feed-forward networks (with residual connections and layer normalization) in both encoder and decoder stacks.',)"
"['08_bert_1810_04805.txt']","What pretraining tasks does BERT use?","('BERT is pretrained using Masked Language Modeling (MLM) and Next Sentence Prediction (NSP) to learn bidirectional contextual representations.',)"
"['09_vit_2010_11929.txt']","How does Vision Transformer (ViT) process images differently from CNNs?","('ViT splits an image into patches, embeds them as tokens, and processes them with a standard Transformer encoder, relying on self-attention rather than convolutional inductive bias.',)"
"['10_efficientnet_1905_11946.txt']","What idea underlies EfficientNet's scaling method?","('EfficientNet uses compound scaling to jointly scale depth, width, and input resolution with a simple set of coefficients to produce efficient models across sizes.',)"
"['11_nas_rl_1611_01578.txt']","What does NAS with reinforcement learning aim to automate?","('NAS-RL automates neural architecture search by using an RL controller to propose architectures that are trained and evaluated, enabling discovery of high-performing architectures without manual design.',)"
"['12_ddpm_2006_11239.txt']","What is the core idea behind Denoising Diffusion Probabilistic Models (DDPM)?","('DDPMs model data generation as learning to reverse a gradual noising (diffusion) process, training a model to denoise and sample by reversing the diffusion.',)"
"['13_score_sde_2011_13456.txt']","How do score-based models / SDEs relate to diffusion models?","('Score-based models estimate data score functions and can be cast as stochastic differential equations (SDEs) whose reverse process produces samples by integrating estimated scores over time.',)"
"['14_simclr_2002_05709.txt']","What is the main training objective in SimCLR?","('SimCLR uses a contrastive learning objective that pulls together representations from augmented views of the same image and pushes apart views from different images, using a projection head and large batch or memory for negatives.',)"
"['15_gcn_1609_02907.txt']","What problem do Graph Convolutional Networks (GCNs) address?","('GCNs generalize convolution to graph-structured data by aggregating and transforming neighbor features, enabling representation learning on graphs for node classification and related tasks.',)"
"['16_neural_ode_1806_07366.txt']","How do Neural ODEs parameterize continuous-depth networks?","('Neural ODEs define the derivative of hidden state with a neural network and compute outputs by integrating this ODE (e.g., with an ODE solver), treating depth as a continuous variable.',)"
"['17_unet_1505_04597.txt']","What is the U-Net architecture primarily used for and what is its structure?","('U-Net is used for image segmentation and features an encoder-decoder with skip connections that transfer high-resolution features from encoder to decoder to recover spatial detail.',)"
"['18_alphazero_1712_01815.txt']","What search algorithm does AlphaZero use instead of alpha-beta search?","('AlphaZero uses Monte-Carlo Tree Search (MCTS) guided by a neural network policy and value function, replacing handcrafted alpha-beta heuristics.',)"
"['19_deepspeed_zero_1910_02054.txt']","What problem does ZeRO (in DeepSpeed) solve?","('ZeRO shards optimizer states, gradients, and parameters across data-parallel ranks to reduce memory footprint and enable training of larger models efficiently.',)"
"['20_megatron_lm_1909_08053.txt']","What parallelism strategy does Megatron-LM use to scale transformer training?","('Megatron-LM implements tensor (model) parallelism by splitting large matrix multiplications across GPU ranks and combines it with data parallelism to scale model training.',)"
"['21_gpt3_2005_14165.txt']","What capability did GPT-3 demonstrate compared to earlier models?","('GPT-3 showed strong few-shot and zero-shot generalization by scaling model size (175B params), enabling many tasks without task-specific fine-tuning.',)"
"['22_gshard_2006_16668.txt']","What is the core idea of GShard and MoE layers?","('GShard uses Mixture-of-Experts to route tokens to different expert sub-networks, increasing model capacity with conditional computation while keeping per-token cost low.',)"
"['23_flashattention_2205_14135.txt']","What does 'IO-aware' mean in the context of FlashAttention?","('IO-aware means designing attention algorithms while carefully accounting for reads/writes across memory hierarchies (on-chip SRAM vs HBM) to reduce costly data movement and improve performance.',)"
"['24_fsdp_2304_11277.txt']","What does FSDP (Fully Sharded Data Parallel) do?","('FSDP shards model parameters, gradients, and optimizer state across workers with careful synchronization to reduce memory consumption and support larger models.',)"
"['25_lora_2106_09685.txt']","What is LoRA and how does it enable efficient fine-tuning?","('LoRA injects low-rank adapters into weight updates, fine-tuning only small low-rank matrices while keeping original weights frozen, significantly reducing tuning cost and storage.',)"
"['A Survey of LLM __times_ DATA.txt']","What does a survey of LLM timelines typically summarize?","('Such a survey summarizes major LLM milestones, architectures, scaling trends, and practical trade-offs (compute, data, inference costs) across years.',)"
"['AIBrix_ Towards Scalable_ Cost-Effective Large Language Model Inference   Infras.txt']","What is the main goal of AIBrix-like inference infrastructure proposals?","('They aim to provide scalable, cost-effective LLM inference through hybrid caching, request routing, and efficient model serving to reduce latency and cost at scale.',)"
"['Apt-Serve_ Adaptive Request Scheduling on Hybrid Cache for Scalable LLM   Infere.txt']","What does adaptive request scheduling in hybrid cache systems optimize?","('Adaptive scheduling selects which cache tier (hot model shards, KV cache, or backing store) to use per request to balance latency and throughput while minimizing expensive memory accesses.',)"
"['attention.txt']","What is the scaled dot-product attention formula?","('Scaled dot-product attention computes softmax((QK^T)/sqrt(d_k)) V, where Q,K,V are query/key/value matrices and d_k is key dimension for scaling.',)"
"['backends.txt']","Why does backend choice matter for deep learning workloads?","('Backends determine kernel implementations, memory layouts, and hardware-specific optimizations; selecting or tuning backends impacts performance, numerical behavior, and available features.',)"
"['Balanced and Elastic End-to-end Training of Dynamic LLMs.txt']","What does 'elastic training' refer to in dynamic LLM training?","('Elastic training adapts compute and model partitioning during training to changing resource availability, enabling balanced throughput while scaling across variable resources.',)"
"['bottleneck.txt']","What is a system 'bottleneck' and how is it diagnosed?","('A bottleneck is the system component (CPU, GPU, memory, I/O, network) limiting throughput. It is diagnosed using profiling metrics, utilization traces, and targeted microbenchmarks.',)"
"['checkpoint.txt']","Why are checkpoints important in large-scale training?","('Checkpoints save model weights, optimizer state, and training metadata to enable recovery from failures, resume training, and enable analysis and reproducibility.',)"
"['cmu_llmsys-01-intro.txt']","What topics are covered in an LLM systems course introduction?","('Introductions cover LLM history, system-level challenges (scaling, latency, cost), and high-level architectures for training and serving large models.',)"
"['cmu_llmsys-02-gpu-programming.txt']","What GPU programming considerations are important for ML kernels?","('Considerations include memory hierarchy (registers, shared memory, HBM), occupancy, kernel fusion, and minimizing global memory traffic for performance.',)"
"['cmu_llmsys-03-gpu-programming2.txt']","How do kernel launches and synchronization affect GPU performance?","('Frequent small kernel launches and excessive synchronization reduce utilization; fusing kernels and overlapping compute with communication improves throughput.',)"
"['cmu_llmsys-04-autodiff.txt']","How does automatic differentiation compute gradients?","('Autodiff constructs a computation graph and applies the chain rule (reverse-mode/backprop) to compute gradients efficiently by traversing the graph backward.',)"
"['cmu_llmsys-05-dl-framework.txt']","What distinguishes dynamic vs static computation graphs?","('Dynamic graphs (e.g., PyTorch eager) build computation at runtime enabling native control flow; static graphs (e.g., older TF graph mode) are compiled ahead for optimizations.',)"
"['cmu_llmsys-06-transformer.txt']","What implementation detail is key for efficient transformer attention?","('Key details include memory-efficient attention kernels, fused softmax/dropout, and blocking to exploit on-chip memory and minimize HBM transfers.',)"
"['cmu_llmsys-07-llms.txt']","What system-level challenges do LLMs present?","('Challenges include model parallelism, memory capacity, serving latency, cost-efficient inference, and data pipeline scaling for training.',)"
"['cmu_llmsys-08-tokenization.txt']","Why is subword tokenization (e.g., BPE) used for LLMs?","('Subword tokenization balances vocabulary size and expressive power, handling rare/unknown words while keeping sequence lengths reasonable.',)"
"['cmu_llmsys-09-decoding.txt']","What are typical decoding strategies for autoregressive models?","('Greedy decoding, beam search, top-k/top-p sampling, and temperature scaling are common strategies balancing quality and diversity.',)"
"['cmu_llmsys-10-gpu-acceleration.txt']","How do GPUs accelerate deep learning?","('GPUs accelerate dense parallel linear algebra via many-core SIMD-like execution, high memory bandwidth, and specialized libraries (cuBLAS, cuDNN).',)"
"['cmu_llmsys-11-transformer-acc.txt']","Name one kernel-level optimization for transformer acceleration.","('Fusing QKV projections and attention softmax into a single CUDA kernel to reduce memory traffic and kernel overhead.',)"
"['cmu_llmsys-13-distributed-training.txt']","What are the main distributed training paradigms?","('Data parallelism, model (tensor) parallelism, pipeline/sharded approaches, and hybrid combinations are the main paradigms for scaling training.',)"
"['cmu_llmsys-14-ddp.txt']","What does PyTorch DDP (DistributedDataParallel) provide?","('DDP wraps models to replicate parameters across processes and performs efficient gradient synchronization across ranks for synchronous data-parallel training.',)"
"['cmu_llmsys-15-model-parallel.txt']","Why use model parallelism?","('Model parallelism splits very large model parameters across devices when a single device lacks memory to hold the full model.',)"
"['cmu_llmsys-15-serving.txt']","What is a key concern when serving LLMs at scale?","('Balancing latency and throughput via batching, caching, and efficient kernel selection while managing cost and memory usage.',)"
"['cmu_llmsys-16-quantization.txt']","What is post-training quantization and its primary downside?","('Post-training quantization maps FP32 weights to lower precision (INT8/4) without retraining; downside is potential accuracy degradation, especially without per-channel or outlier handling.',)"
"['cmu_llmsys-17-quantization2.txt']","How does GPTQ differ from naive quantization?","('GPTQ performs layer-wise quantization with error compensation and optimized scaling to quantize large language models to very low bit widths (3â€“4 bits) with small accuracy loss.',)"
"['cmu_llmsys-18-peft.txt']","What is PEFT and why is it useful?","('Parameter-Efficient Fine-Tuning (PEFT) updates a small subset of parameters (adapters, LoRA, etc.) to adapt large models with low compute and storage cost.',)"
"['cmu_llmsys-19-MoE.txt']","What challenge does MoE address and what's a routing concern?","('MoE increases capacity via experts, but routing must balance expert load and communication cost; load imbalance and routing overhead are key concerns.',)"
"['cmu_llmsys-20-FlashAttention_tridao.txt']","What benefit does FlashAttention provide for variable-length sequences?","('FlashAttention reduces memory movement and enables faster attention computation by being IO-aware and using fused kernels, benefiting variable-length workloads when implemented efficiently.',)"
"['cmu_llmsys-21-zero.txt']","What is ZeRO's key insight for memory reduction?","('ZeRO shards optimizer and gradient state across ranks, distributing memory footprint to allow larger model training per GPU.',)"