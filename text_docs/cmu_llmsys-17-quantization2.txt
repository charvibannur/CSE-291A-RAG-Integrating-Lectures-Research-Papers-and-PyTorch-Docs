11868/11968 LLM Systems
LLM Quantization -GPTQ
Lei Li
Ack: Kath Choi, Aashiq  Muhamed, Zhen Wu, Hongyi  Jin, Wayne Wangâ€¢low-bit number representation in computer
oBF16: 16 -bit half precision floating point numbers, better for ML tasks
oint8
â€¢Direct quantization of numbers
oabsmax : linearly scale according to max abs value
ozero-point: finding zero -point and scale
â€¢Layer -wise quantization approaches for NN weights
oAdaQuant
oKD: ZeroQuant
oLLM.int8(): mixed strategy for 8 -bit quantization and 16 -bit (for outliers)2Recap of Basic Quantization Methodsâ€¢Absmax  quant â€¢Zero -point quant
4Quantize a Number to Int8
5GPTQ
â€¢scale to GPT -size LLMs
â€¢maintain accuracyâ€¢Revisit  layer -wise quantization of weight matrices
argmin
à·¡ğ‘Šğ‘Šğ‘‹ âˆ’à·¡ğ‘Šğ‘‹22
X: input data to current layer (d x Len)      W: projection matrix
â€¢Key ideas: 
1.Quantizes one column -block of weights at a time
2.Updates all the not -yet-quantized weights, to compensate for the 
error  incurred by quantizing a single weight
6Overall idea of GPTQ
Optimal Brain Compression: A framework for accurate post -training quantization and pruning (2022)Optimal Brain Surgeon and General Network Pruning (1993)GPTQ: Accurate Post -Training Quantization for Generative Pre -trained Transformers. Frantar  et al. ICLR 2023. 1.Pre-compute Cholesky decomposition of the Hessian 
inverse for input data X of current (Linear) layer
2.Iteratively handle one batch of columns of weight matrix W
1.quantize the weights using a rounding method 
2.calculates the rounding error
3.updates the weights in the column block accordingly.
4.After processing the batch, it updates all remaining weights 
based on the blockâ€™s errors.
7GPTQ algorithm8Pre-compute Hessian Inverse and Decompose
ğº=Cholesky (2ğ‘‹âˆ™ğ‘‹ğ‘‡+ğœ†ğ¼âˆ’1))ğ‘‡ 
Cholesky 
decomposition, given 
a symmetric positive -
definite matrix  A
ğ´=ğ¿âˆ™ğ¿ğ‘‡X is batch_size Ã—128k( len) 
Ã—7168(dim) in Deepseek -
V3 â” 7168 Ã—(bsÃ—128k)9GPTQ: Block -wise Quantize and Update
weight matrix W, block size B=4 
quantized
blockcurrent blockremaining 
block (float)current column
1.quantize the column weights
(e.g. using int8 or int4) 
ğ‘:,ğ‘—=quant (ğ‘Š:,ğ‘—)10GPTQ: Block -wise Quantize and Update
weight matrix W, block size B=4 
quantized
blockcurrent blockremaining 
block (float)current column
1.quantize the column weights 
(e.g. using int8 or int4 
rounding) 
ğ‘:,ğ‘—=quant (ğ‘Š:,ğ‘—)
2.calculates the rounding error 
ğ¸:,ğ‘—âˆ’ğ‘–=(ğ‘Š:,ğ‘—âˆ’ğ‘„:,ğ‘—)/ğºğ‘—,ğ‘—i j
ğº=Cholesky (2ğ‘‹âˆ™ğ‘‹ğ‘‡+ğœ†ğ¼âˆ’1))ğ‘‡ 11GPTQ: Block -wise Quantize and Update
weight matrix W, block size B=4 
quantized
blockcurrent blockremaining 
block (float)current column
1.quantize the columnâ€™s weights 
(e.g. using int8 or int4) 
ğ‘:,ğ‘—=quant (ğ‘Š:,ğ‘—)
2.calculates the rounding error 
ğ¸:,ğ‘—âˆ’ğ‘–=(ğ‘Š:,ğ‘—âˆ’ğ‘„:,ğ‘—)/ğºğ‘—,ğ‘—
3.updates the remaining 
weights in the column block  i j
ğ‘Š:,ğ‘—:(ğ‘–+ğµ)
=ğ‘Š:,ğ‘—:(ğ‘–+ğµ)âˆ’ğ¸:,ğ‘—âˆ’ğ‘–âˆ™ğºğ‘—,ğ‘—:(ğ‘–+ğµ)12GPTQ: Lazy -update for rest weights
weight matrix W, block size B=4 
quantized
blockcurrent blockremaining 
block (float)current column
1.quantize the columnâ€™s weights 
(e.g. using int8 or int4) 
ğ‘:,ğ‘—=quant (ğ‘Š:,ğ‘—)
2.calculates the rounding error 
ğ¸:,ğ‘—âˆ’ğ‘–=(ğ‘Š:,ğ‘—âˆ’ğ‘„:,ğ‘—)/ğºğ‘—,ğ‘—
3.updates the rmaining  weights 
in the column block  i j
ğ‘Š:,ğ‘—:(ğ‘–+ğµ)
=ğ‘Š:,ğ‘—:(ğ‘–+ğµ)âˆ’ğ¸:,ğ‘—âˆ’ğ‘–âˆ™ğºğ‘—,ğ‘—:(ğ‘–+ğµ)
After compute the current block
4.update all remaining weights
ğ‘Š:,ğ‘–+ğµ:
=ğ‘Š:,ğ‘–+ğµ:âˆ’ğ¸âˆ™ğºğ‘–:ğ‘–+ğµ,ğ‘–+ğµ:GPTQ Algorithm
13
=G in previous 
slidesâ€¢Quantize one weight in W can be solved using the Optimal 
Brain Surgeon method (OBS)
oâ” We can update one column of weights
â€¢Iteratively updating the inverse Hessian can be updated 
efficiently (rank -1 update using Optimal Brain Quantization, 
OBQ method)
oâ” Using Cholesky to pre -compute
â€¢Updating weights after calculating rounding errors can be 
done in batch and lazy -fashion14Why GPTQ works?â€¢Taylor approximation to find optimal single weight in W to remove, and optimal 
update of remaining weights to compensate.
â€¢Weight to prune ğœ”ğ‘ which incurs the minimal increase in loss and the 
corresponding update of the remaining weights ğ›¿ğ‘ is,
In transformers, ğœ”ğ‘ could potentially be LayerNorm /FFN/MHA weights
â€¢H is a dÃ—d Hessian matrix where ğ‘‘=ğ‘‘ğ‘Ÿğ‘œğ‘¤ âˆ™ğ‘‘ğ‘ğ‘œğ‘™, which is expensive to store 
and compute with. 
â€¢H needs to be updated and inverted at O(d) pruning steps with a Î˜(d3 ) 
complexity. Total runtime O(d4 ) is too inefficient.
15Optimal Brain Surgeon
ğœ”ğ‘= ğ‘ğ‘Ÿğ‘” min
ğœ”ğ‘ğœ”ğ‘ğœ”ğ‘2
ğ»âˆ’1ğ‘ğ‘,ğ›¿ğ‘=âˆ’ğœ”ğ‘
ğ»âˆ’1ğ‘ğ‘âˆ™ğ»:,ğ‘ ,âˆ’1â€¢OBQ picks the greedy optimal weight  ğ‘Šğ‘  to quantize next, along with 
the update ğ›¿ğ¹  to all remaining unquantized weights F.
â€¢quantizes weights iteratively 
until all weights are quantized.
â€¢Hessian ğ»=2ğ‘‹ğ‘‹ğ‘‡
16Optimal Brain Quantization
ğ‘¤ğ‘= ğ‘ğ‘Ÿğ‘” min
ğ‘¤ğ‘ğ‘ğ‘¢ğ‘ğ‘›ğ‘¡ ğ‘¤ğ‘âˆ’ğ‘¤ğ‘2
ğ»ğ¹âˆ’1ğ‘ğ‘,ğ›¿ğ¹=âˆ’ğ‘¤ğ‘âˆ’ğ‘ğ‘¢ğ‘ğ‘›ğ‘¡ (ğ‘¤ğ‘)
ğ»ğ¹âˆ’1ğ‘ğ‘âˆ™(ğ»ğ¹âˆ’1):,ğ‘âˆ™ 
full-precisionupdate1.Row wise decomposition: OBQ applies OBS per row of the weight 
matrix
No Hessian interaction between different rows and so we can work with 
the individual  ğ‘‘ğ‘ğ‘œğ‘™ Ã— ğ‘‘ğ‘ğ‘œğ‘™ H corresponding to each of the ğ‘‘ğ‘Ÿğ‘œğ‘¤  rows. 
2.Efficient inverse
Reduces overall costs of this process to ğ‘‚(ğ‘‘ğ‘Ÿğ‘œğ‘¤ âˆ™ğ‘‘ğ‘ğ‘œğ‘™3)  time and ğ‘‚(ğ‘‘ğ‘ğ‘œğ‘™2) 
memory.
17Optimal Brain Quantization
à·
ğ‘–=1ğ‘‘ğ‘Ÿğ‘œğ‘¤
ğ‘Šğ‘–,:ğ‘‹ âˆ’à·¢ğ‘Šğ‘–,:ğ‘‹22
â€¢Quantize all rows of weights in same order .
oâ” column -wise update
â€¢ğ¹ and  ğ»ğ¹âˆ’1are always the same for all rows 
as ğ»ğ¹ depends only on the layer inputs ğ‘‹ğ¹, 
which are the same for all rows.
â€¢Perform update on ğ»ğ¹âˆ’1 only ğ‘‘ğ‘ğ‘œğ‘™ times, once 
per column, rather than ğ‘‘ğ‘Ÿğ‘œğ‘¤ âˆ™ğ‘‘ğ‘ğ‘œğ‘™ times, once 
per weight. 
â€¢This reduces the overall runtime 
from ğ‘‚(ğ‘‘ğ‘Ÿğ‘œğ‘¤ âˆ™ğ‘‘ğ‘ğ‘œğ‘™3) to 
ğ‘‚(ğ‘šğ‘ğ‘¥ (ğ‘‘ğ‘Ÿğ‘œğ‘¤ âˆ™ğ‘‘ğ‘ğ‘œğ‘™2,ğ‘‘ğ‘ğ‘œğ‘™3)).
18Column Update using OBQ is efficient
â€¢OBQ quantizes weights in a specific order defined by the 
corresponding errors.
â€¢Improvement over quantizing the weights in arbitrary order is 
generally small
oquantized weights with large individual error is balanced out by those weights 
towards the end of the process
ofew other unquantized weights can be adjusted for compensation
19Insight of Arbitrary Update Order for OBQ
GPTQ: Accurate Post -Training Quantization for Generative Pre -trained Transformers. Frantar  et al. ICLR 2023. â€¢NaÃ¯ve column update is not fast in practice 
olow compute -to-memory -access ratio
ocannot highly utilize GPUs compute.
â€¢Observation: 
oRounding decisions for col i only affected 
by updates on this col
oUpdates to later columns are irrelevant at 
this point in the process.
â€¢Efficient update
20Lazy Batch Updates
ğ‘Š:,ğ‘–+ğµ:=ğ‘Š:,ğ‘–+ğµ:âˆ’ğ¸âˆ™ğºğ‘–:ğ‘–+ğµ,ğ‘–+ğµ:â€¢Numerical inaccuracies, can become a 
major problem  at the scale of LLMs,
â€¢ğ»ğ¹âˆ’1  can become indefinite
â€¢Observation:
oOnly information required from ğ»ğ¹ğ‘âˆ’1  when quantizing 
weight q from unquantized ğ¹ğ‘, are the elements in row 
q starting with the diagonal. 
â€¢GPTQ leverages Cholesky kernels to 
precompute all information from ğ»âˆ’1without any 
significant increase in memory consumption.
21Cholesky Pre -computation
â€¢How is GPT -Qâ€™s perf on small models compared with accurate -but-
expensive methods?
â€¢How does GPT -Qâ€™s quantization time scale with model size?
â€¢How is GPT -Qâ€™s perf on large models compared with Round -to-
nearest methods?
â€¢How does GPT -Q speed up model inference in practical 
applications?
â€¢Does GPT -Q even work for extreme 2 -bit quantization?
22Effectiveness of GPTQ?â€¢Calibration data randomly sampled from C -4 dataset to 
ensure GPTQ is not task -aware.
â€¢Standard uniform per -row asymmetric quantization on the 
min-max grid
â€¢Quantize on each transformer block (6 layers), with input X 
from last quantized block output.
23Quantization Experiment Setupâ€¢Single -batch inference is memory -bound because of GEMVs. Although 
dequantization consumes extra compute, the custom kernel reduces 
memory access and thus reduces e2e time. 
24How does GPT -Q speed up model inference in practical 
applications?
OPT-175B mode (Measured with pipeline parallelism)How is GPT -Qâ€™s perf on large models compared with 
Round -to-nearest methods?
25
How is GPT -Qâ€™s perf on large models compared with 
Round -to-nearest methods?
26
How does GPT -Qâ€™s quantization time scale with model 
size?
27* Measured on single A100ZeroQuant -LKD GPT-Q
1.3B model - 3h
Does GPT -Q even work for extreme 2 -bit quantization?
28
How is GPT -Qâ€™s perf on small models compared with 
accurate -but-expensive methods?
29
 Fastest prior methodâ€¢https://github.com/qwopqwop200/GPTQ -for-LLaMa/  
â€¢GPTQ in 
ohttps://github.com/qwopqwop200/GPTQ -for-
LLaMa/blob/triton/gptq.py  
31GPTQ for LLaMAGPTQ: Initialization
32
â—Reshape weights from the 
input layer
â—Initialize Hessian matrixGPTQ: Hessian Matrix Update
33â—Update Hessian matrix with 
information from a new 
batch of the input and 
output pairs GPTQ: Lazy Batch -Update
34
â—Processes weight matrix W in blocks.
â—Updates quantization parameters conditionally 
based on group size and static grouping 
settings.GPTQ: Lazy Batch -Update
35
â—Applies quantization function quantize to 
weights and computes the loss due to 
quantization.
â—Adjusts remaining block weights based on 
quantization error to minimize the overall error.GPTQ: Cholesky Reformulation
36
â—Applies damping to the Hessian 
matrix diagonals
â—Performs Cholesky decomposition 
and inversion
â—Transforms the Hessian into its 
inverse.37import  random
from  auto_gptq  import  AutoGPTQForCausalLM , BaseQuantizeConfig
from  datasets import  load_dataset
import  torch
from  transformers import  AutoTokenizer
# Define base model and output directory
model_id  = "gpt2â€ #modify to your model
out_dir  = model_id  + "-GPTQâ€
# Load quantize config, model and tokenizer
quantize_config  = BaseQuantizeConfig (bits= 4, group_size =128, damp_percent =0.01 , desc_act =False )
model = AutoGPTQForCausalLM.from_pretrained (model_id , quantize_config )
tokenizer = AutoTokenizer.from_pretrained (model_id )
# Load data and tokenize examples
n_samples  = 1024
data = load_dataset ("allenai /c4" , data_files ="en/c4-train.00001 -of-01024.json.gz" , split= f"train [:{n_samples *5}]")
tokenized_data  = tokenizer( "\n\n".join (data[ 'text' ]), return_tensors ='pt')
# Format tokenized examples
examples_ids  = []
for _ in range (n_samples ):
i = random.randint (0, tokenized_data.input_ids.shape [1] - tokenizer.model_max_length  - 1)
j = i + tokenizer.model_max_length
input_ids  = tokenized_data.input_ids [:, i:j]
attention_mask  = torch.ones_like (input_ids )
examples_ids.append ({'input_ids ': input_ids , 'attention_mask ': attention_mask })
# Quantize with GPTQ
model.quantize (examples_ids , batch_size =1, use_triton =True )
# Save model and tokenizer
model.save_quantized (out_dir , use_safetensors =True )
tokenizer.save_pretrained (out_dir )AutoGPTQ  Toolâ€¢GPTQ
oapproximate second -order of weights
olayer -wise quantization + compensation for errors + precompute
oaccurately compress some of the largest publicly -available 
models down to 3 and 4 bits, and bring end -to-end speedups
â€¢Limitations
oTheoretical computation is the same
oFocus on weight quantization, and does not consider activation 
quantization
38Summary and Limitations