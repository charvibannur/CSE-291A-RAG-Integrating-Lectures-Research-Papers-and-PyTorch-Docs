Transformer
Lei Li
â€¢Design of a Deep Learning Framework
oTensorflow , a computation graph defined as dataflow
otwo stages: defining the computation graph and then executing 
the computation (with optimization)
â–ªPlaceholder nodes for taking input
â–ªVariable nodes for storing parameters
â–ªOperation node, with input node and output for holding computed result 
2Recapâ€¢Implementing Transformer, the backbone of LLMs
oEncoder -decoder architecture
oEmbedding
oMultihead  Attention and FFN, Decoder Self -Attention
oLayernorm
â€¢Training techniques and Performance of Transformer
â€¢Code walkthrough 
3Todayâ€™s Topic
Type of Language Models
4
Decoder
EncoderEncoder -only
Masked LM
Non-autoregressiveEncoder -decoder Decoder -only
Autoregressive
Encoder
 Decoder
e.g. BERT
RoBERTa
ESM (for protein)e.g. T5 e.g. GPT
LLaMA
ProGen  (for protein)Encoder -Decoder Paradigm 
ğ‘ğœƒ(ğ‘¦|ğ‘¥)=âˆ
ğ‘–ğ‘(ğ‘¦ğ‘–|ğ‘¥,ğ‘¦1:ğ‘–âˆ’1)
conditional prob. modeled by 
neural networks (Transformer)
Decoder
Encoder
Source: æˆ‘å–œæ¬¢å”±æ­Œå’Œè·³èˆ ã€‚target:   
I like singing and dancing.
5â€¢Conditional text generation: directly learning a function 
mapping from source sequence to target sequence
ğ‘ğœƒ(ğ‘¦|ğ‘¥)=âˆ
ğ‘¡ğ‘(ğ‘¦ğ‘¡|ğ‘¥,ğ‘¦1:ğ‘¡âˆ’1;ğœƒ)
â€¢Previous encoder/decoder: LSTM or GRUSequence to Sequence Learning
Sutskever et al. Sequence to Sequence Learning with Neural Networks. 2014
RNN
.
RNN
dancing
RNN
like
RNN
I
RNN
<eos>
RNN
è·³èˆ
RNN
å–œæ¬¢
RNN
æˆ‘
RNN
<bos>
è·³èˆ å–œæ¬¢ æˆ‘ <eos>
Sourceï¼šTargetï¼š Encoder
6â€¢Full context and parallel: use Attention in both encoder and 
decoder
â€¢no recurrent ==> concurrent encodingMotivation for a new Architecture
Decoder
Encoder
Source: æˆ‘å–œæ¬¢å”±æ­Œå’Œè·³èˆ ã€‚target:   
I like singing and dancing.
7Transformer
<BOS>I likeI like singingEncoder Decoder
MHA + FFNMHA 
+FFNMHA + 
FFNMHA + 
FFN
MHA + 
FFNMHA + 
FFNMHA + 
FFN
MHA + 
FFNMHA + 
FFNMHA + 
FFNSoftmax Softmax SoftmaxI like singing and 
dancing.
Token
Embedding
Table
æˆ‘ä¸€å’Œâ€¦
MHA + FFN
Token
Embedding
Table
Ilikeyouâ€¦
æˆ‘å–œæ¬¢å”±æ­Œå’Œè·³èˆã€‚
Vaswani et al. Attention is All You Need. 2017+pos emb+pos emb
8â€¢Token Embedding: (tokenization next lec.) 
oShared (tied) input and output embedding from 
lookup table
â€¢Positional Embedding: 
oto distinguish words in different position, Map 
position labels to embedding, dimension is same 
as Tok Emb, for t -th pos, i-th dim
ğ‘ƒğ¸t,2ğ‘–=sin(ğ‘¡
10002ğ‘–/ğ‘‘)
ğ‘ƒğ¸t,2ğ‘–+1=cos(ğ‘¡
10002ğ‘–/ğ‘‘)Embedding
9â€¢Instead of one vector for each token
â€¢break into multiple heads
â€¢each head perform attention 
Head ğ‘–=Attention (ğ‘„ğ‘Šğ‘–ğ‘„,ğ¾ğ‘Šğ‘–ğ¾,ğ‘‰ğ‘Šğ‘–ğ‘‰)
MultiHead (ğ‘„,ğ¾,ğ‘‰)
=Concat (Head 1,Head 2,â€¦,Head â„)ğ‘Šğ‘œMulti -head Attention
Q
Scaled Dot -Product
Attention
Linear
 Linear
 Linear
Concat
Linear
h
K V
10sent len x sent len
lenx dim
Alammar, The Illustrated TransformerMulti -head Attention
Q: why divided by sqrt(d)?Query
Key
ValueX are input embeddings from previous layer (num of tok * dim)
11Multihead  Attention and FFN
ğ‘‰ğ‘¥ ğ¾ğ‘¥ ğ‘„ğ‘¥
Dot Product
Softmax
Linear
Ã—
LayerNorm
ğ‘¦ğ‘–=ğ‘¤ğ‘–ğ‘¥ğ‘–âˆ’ğœ‡(ğ‘¥)
ğœ(ğ‘¥)+ğ‘ğ‘–
+
 LinearReLU
Linear
LayerNorm+ MHA FFN
(ğ‘„ğ‘¥)ğ‘‡Ã—ğ¾ğ‘¥/ğ‘‘ğ‘˜Attention (ğ‘„,ğ¾,ğ‘‰,ğ‘¥)=Softmax ((ğ‘„ğ‘¥)ğ‘‡ğ¾ğ‘¥
ğ‘‘)â‹…(ğ‘‰ğ‘¥)ğ‘‡FFN (ğ‘¥)=ğ‘šğ‘ğ‘¥ (0,ğ‘¥â‹…ğ‘Š1+ğ‘1)â‹…ğ‘Š2+ğ‘2
Residual
Conn
12â€¢Maskout right side before softmax ( -inf)Decoder Self -Attention
MatMul
Scale
Mask (opt.)
SoftMax
MatMul
Q K VScaled Dot -Product Attention
13â€¢Residual Connection
â€¢Make it zero mean and unit 
variance within layer
â€¢Post-norm 
â€¢Pre-normResidual Connection and Layer 
Normalization
14â€¢on Canvas
15Quiz 3â€¢Implementing Transformer, the backbone of LLMs
oEncoder -decoder architecture
oEmbedding
oMultihead  Attention and FFN, Decoder Self -Attention
oLayernorm
â€¢Training techniques and Performance of Transformer
â€¢Code walkthrough 
16Today â€™s Topic
â€¢C layers of encoder (=6)
â€¢D layers of decoder (=6)
â€¢Token Embedding: 512 
(base), 1024 (large)
â€¢FFN dim=2048Transformer in Original Paper
Multi -Head
Attention
Add & Norm
Input
Embedding
InputsFeed Forward
Add & Norm
Output
Embedding
Feed Forward
Add & Norm
Multi -Head
Attention
Add & Norm
Add & Norm
Masked
Multi -Head
Attention
Linear
Softmax
Output
Proba
bilities
Output
s
(shifted 
right)MatMul
Scale
Mask (opt.)
SoftMax
MatMul
Q K VScaled Dot -Product 
Attention
Q
Scaled Dot -Product
Attention
Linear
 Linear
 Linear
Concat
Linear
h
K VMulti-Head 
Attention
17ğ‘ƒ(ğ‘Œ|ğ‘‹)=âˆğ‘ƒ(ğ‘¦ğ‘¡|ğ‘¦<ğ‘¡,ğ‘¥)
â€¢Training loss: Cross -Entropy
ğ‘™=âˆ’âˆ‘
ğ‘›âˆ‘
ğ‘¡logğ‘“ğœƒ(ğ‘¥ğ‘›,ğ‘¦ğ‘›,1,â€¦,ğ‘¦ğ‘›,ğ‘¡âˆ’1)
â€¢Teacher -forcing during training.
â€¢pretend to know groundtruth  for 
prefixTraining Transformer
Decoder
Encoder
Source: æˆ‘å–œæ¬¢å”±æ­Œå’Œè·³èˆ ã€‚target:   
I like singing and dancing.
18â€¢Dropout
oApplied to before residual
oand to embedding, pos emb.
op=0.1 ~ 0.3
â€¢Label smoothing
o0.1 probability assigned to non -truth
â€¢Vocabulary: 
oEn-De: 37K using BPE
oEn-Fr: 32k word -piece (similar to BPE)Training Transformer for MT
19â€¢Assume ğ‘¦âˆˆğ‘…ğ‘› is the one -hot encoding of label
â€¢Approximating 0/1 values with softmax  is hard
â€¢The smoothed version
oCommonly use Label Smoothing
ğ‘¦ğ‘–={1if belongs  to class  ğ‘–
0 otherwise
ğ‘¦ğ‘–={1âˆ’ğœ– ifbelongs toclass  ğ‘–
ğœ–/(ğ‘›âˆ’1) otherwise
ğœ–=0.1
20â€¢Batch
ogroup by approximate sentence length
ostill need shufflingHardware  
oone machine with 8 GPUs (in 2017 paper)
obase model: 100k steps ( 12 hours)
olarge model: 300k steps ( 3.5 days)
â€¢Adam Optimizer
oincrease learning rate during warmup, then decrease
oğœ‚=1
ğ‘‘ğ‘šğ‘–ğ‘› (1
ğ‘¡,ğ‘¡
ğ‘¡03)Training
21ğ‘šğ‘¡+1=ğ›½1ğ‘šğ‘¡âˆ’(1âˆ’ğ›½1)ğ›»â„“(ğ‘¥ğ‘¡)
ğ‘£ğ‘¡+1=ğ›½2ğ‘£ğ‘¡+(1âˆ’ğ›½2)(ğ›»â„“(ğ‘¥ğ‘¡))2
à·ğ‘šğ‘¡+1=ğ‘šğ‘¡+1
1âˆ’ğ›½1ğ‘¡+1
à·œğ‘£ğ‘¡+1=ğ‘£ğ‘¡+1
1âˆ’ğ›½2ğ‘¡+1
ğ‘¥ğ‘¡+1=ğ‘¥ğ‘¡âˆ’ğœ‚
à·œğ‘£ğ‘¡+1+ğœ–à·ğ‘šğ‘¡+1ADAMâ€¢A single model obtained by averaging the last 5 
checkpoints, which were written at 10 -minute interval 
(base)
â€¢decoding length: within source length + 50
omore on decoding in next lectureModel Average
23â€¢Sequence -to-sequence encoder -decoder framework for 
conditional generation, including Machine Translation
â€¢Key components in Transformer ( why each?)
oPositional Embedding (to distinguish tokens at different pos)
oMultihead  attention
oResidual connection
olayer norm
24Summaryhttps:// nlp.seas.harvard.edu /annotated -transformer/
25Code Go -throughâ€¢Neural Machine Translation of Rare Words with Subword  
Units. Sennrich  et al. 2016 .
â€¢SentencePiece : A simple and language independent 
subword  tokenizer and detokenizer  for Neural Text 
Processing. Kudo and Richardson. 2018
26Reading for Next Class