[source
[source
[sourcetorch.__future__
Created On: Feb 05, 2024 | Last Updated On: Jun 12, 2025
torch.__future__. set_overwrite_module_params_on_conversion (value )
Sets whether to assign new tensors to the parameters instead of changing the
existing parameters in-place when converting an nn.Module .
When enabled, the following methods will assign new parameters to the module:
1. module.{device}()  (e.g. nn.Module.cuda() ) for moving a module between devices
2. module.{dtype}()  (e.g. nn.Module.float() ) for converting a module to a different
dtype
3. nn.Module.to()
4. nn.Module.to_empty()
Parameters:
value (bool) – Whether to assign new tensors or not.
torch.__future__. get_overwrite_module_params_on_conversion ( )
Returns whether to assign new tensors to the parameters instead of changing the existing
parameters in-place when converting an torch.nn.Module . Defaults to False.
See set_overwrite_module_params_on_conversion()  for more information.
Return type:
bool
torch.__future__. set_swap_module_params_on_conversion (value )
Sets whether to use swap_tensors()  instead of setting .data to change the existing
parameters in-place when converting an nn.Module  and instead of
param.copy_(state_dict[key])  when loading a state dict into an nn.Module .To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or
navigating, you agree to allow our usage of cookies. As the current maintainers of this site,
Facebookʼs Cookies Policy applies. Learn more, including about available controls: Cookies
Policy.10/10/25, 3:03 PM torch.__future__ — PyTorch 2.8 documentation
https://docs.pytorch.org/docs/stable/future_mod.html 1/4[sourceThis function takes precedence over
get_overwrite_module_params_on_conversion()
When enabled, the following methods will swap the existing parameters in-place:
1. module.{device}()  (e.g. nn.Module.cuda() ) for moving a module between devices
2. module.{dtype}()  (e.g. nn.Module.float() ) for converting a module to a different
dtype
3. nn.Module.to()
4. nn.Module.to_empty()
5. nn.Module.load_state_dict()
The semantics for load_state_dict()  when this is set are as follows:
1. For each parameter/buffer, its corresponding state_dict['key']  is transformed via
module_load()  (i.e. res = param.module_load(state_dict['key']) )
2. If necessary, res will be wrapped in an Parameter
3. The parameter/buffer in the module will be swapped via swap_tensors()  with res
Parameters:
value (bool) – Whether to use swap_tensors()  or not.
torch.__future__. get_swap_module_params_on_conversion ( )
Returns whether to use swap_tensors()  instead of setting .data to change the existing
parameters in-place when converting an nn.Module . Defaults to False.
See set_swap_module_params_on_conversion()  for more information.
Return type:
bool
Rate this Page★★★★★Note 
To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or
navigating, you agree to allow our usage of cookies. As the current maintainers of this site,
Facebookʼs Cookies Policy applies. Learn more, including about available controls: Cookies
Policy.10/10/25, 3:03 PM torch.__future__ — PyTorch 2.8 documentation
https://docs.pytorch.org/docs/stable/future_mod.html 2/4Previous
torch.configNext
Torch Environment Variables
© Copyright PyTorch Contributors.
Built with the PyData Sphinx Theme 0.15.4.Send Feedback
Docs
Access comprehensive
developer documentation
for PyTorchTutorials
Get in-depth tutorials for
beginners and advanced
developersResources
Find development
resources and get your
questions answered
View Docs View Tutorials View Resources
Stay in touch for updates, event info, and the latest news
First Name* Last Name* Email*
Select Country* SUBMIT
By submitting this form, I consent to receive marketing emails from the LF and its projects
regarding their events, training, research, developments, and related announcements. I understand
that I can unsubscribe at any time using the links in the footers of the emails I receive. Privacy
Policy.
To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or
navigating, you agree to allow our usage of cookies. As the current maintainers of this site,
Facebookʼs Cookies Policy applies. Learn more, including about available controls: Cookies
Policy.10/10/25, 3:03 PM torch.__future__ — PyTorch 2.8 documentation
https://docs.pytorch.org/docs/stable/future_mod.html 3/4© PyTorch. Copyright © The Linux Foundation®. All rights reserved. The Linux Foundation has
registered trademarks and uses trademarks. For more information, including terms of use, privacy
policy, and trademark usage, please see our Policies page. Trademark Usage. Privacy Policy.
To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or
navigating, you agree to allow our usage of cookies. As the current maintainers of this site,
Facebookʼs Cookies Policy applies. Learn more, including about available controls: Cookies
Policy.10/10/25, 3:03 PM torch.__future__ — PyTorch 2.8 documentation
https://docs.pytorch.org/docs/stable/future_mod.html 4/4