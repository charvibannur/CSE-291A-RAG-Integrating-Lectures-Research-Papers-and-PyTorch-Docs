11868 LLM Systems
Auto Differentiation
Lei Li
â€¢GPU is composed of 
ostreaming processing units (SMs)
â–ªeach with four partitions of 32 cores
â–ªshared L1 cache 
omemory
oL2 cache: share with all SMs
â€¢Threads organized in
ogrid of thread blocks
oeach block is divided into warps 
running on one SM.
2Recap
Grid GPU
Warp
1Thread Block
Warp
1Warp
2
Warp
3Warp
4SM
partition1â€¢Basic CUDA operations
omemory allocation
odata movement
ocreating threads and running on SMs
â–ªspecifying number of threads and number of blocks in a grid
oreferring to data in GPU memory within a thread
â–ªusing building index variables to refer to the data
â€¢Implementing parallel matrix operations on GPUs
opartition the data and computation and assign to threads
3Recapâ€¢Learning algorithm for Neural Network
â€¢Computation Graph
â€¢Auto Differentiation
4Todayâ€™s Topic
â€¢Neural network layers
oEmbedding (lookup table)
oLinear 
oRelu
oAverage pooling
oSoftmax
5A Simple Feedforward Neural Network
LinearReluLinearSoftmax
Embedding
â€œIt is a good movieâ€Avgâ€¢ğ‘¥ğ‘›,ğ‘¦ğ‘› are data and label pairs for training
â€¢Cross entropy
â„’(ğœƒ)=1
ğ‘âˆ‘
ğ‘›=1ğ‘
âˆ’logğ‘“ (ğ‘¥ğ‘›)ğ‘¦ğ‘›
â€¢Pytorch  CrossEntropyLoss  is implemented as
oNegative Likelihood on logits
6Training Loss for Classificationloss = nn.CrossEntropyLoss ()
output  = loss( input_logits , target_labels )
grads = output.backward ()
7Todayâ€™s focus (using PyTorch  Example)
how is backward implemented?
how does it work on any network?â€¢Given a training set of input -output pairs ğ·
={(ğ‘¥ğ‘›,ğ‘¦ğ‘›)}ğ‘›=1ğ‘
oğ‘¥ğ‘›and ğ‘¦ğ‘›may both be vectors
â€¢To find the model parameters such that the model 
produces the most accurate output for each 
training input
oOr a close approximation of it
â€¢Learning the parameter of a neural network is an 
instance!
oThe network architecture is given
8The Learning Problem
ğ‘¦
ğ‘¿â€¢Consider a generic function minimization problem, where x is 
unknown variable
min ğ‘“(ğ‘¥)
ğ‘¥ where  ğ‘“:â„ğ‘‘â†’â„
â€¢Iterative update algorithm 
ğ‘¥ğ‘¡+1â†ğ‘¥ğ‘¡+Î”
â€¢so that ğ‘“(ğ‘¥ğ‘¡+1)â‰ªğ‘“(ğ‘¥ğ‘¡)
â€¢How to find Î”
9Generic Iterative Algorithmğ‘“(ğ‘¥ğ‘¡+Î”ğ‘¥)â‰ˆğ‘“(ğ‘¥ğ‘¡)+Î”ğ‘¥ğ‘‡ğ›»ğ‘“|ğ‘¥ğ‘¡
â€¢To make Î”ğ‘¥ğ‘‡ğ›»ğ‘“|ğ‘¥ğ‘¡ smallest
oâ‡’Î”ğ‘¥in the opposite  direction  of ğ›»ğ‘“|ğ‘¥ğ‘¡i.e.Î”ğ‘¥=âˆ’ğ›»ğ‘“|ğ‘¥ğ‘¡
â€¢Update rule: ğ‘¥ğ‘¡+1=ğ‘¥ğ‘¡âˆ’ğœ‚ğ›»ğ‘“ |ğ‘¥ğ‘¡
â€¢ğœ‚is a hyper -parameter to control the learning rate
10Gradient Descentset learning rate eta.
1.set initial parameter ğœƒâ†ğœƒ0
2.for epoch = 1 to maxEpoch or until converg :
3.  for each batch in the data:
4.    total_g  = 0
5.    for each data (x, y) in data batch :
6. compute error err(f(x; ğœƒ) -y)
7. compute gradient ğ‘”=ğœ•err (ğœƒ)
ğœ•ğœƒ
8. total_g += g
9. update ğœƒ= ğœƒ-eta * total_g / N
11(Stochastic) Gradient Descent Algorithmâ€¢Goal: ğœ•ğ‘™
ğœ•ğ‘¤ğ‘–
â€¢Forward computation
â€¢Backpropogation
12How to compute the gradient for every 
parameter in an â€œarbitrary network â€?
LinearReluLinearSoftmax
Embedding
â€œIt is a good movieâ€Avgâ€¢Learning algorithm for Neural Network
â€¢Computation Graph
â€¢Auto Differentiation
13Todayâ€™s Topic
â€¢Calculations on a neural network can be defined using 
computation graph
â€¢Each node denotes a variable or an operation
â€¢Directed edges to connect nodes, indicating the input 
values for operations. 
14Computation Graph
ğ‘¥1 â„2 â„3
ğ‘¤1*relu(.)â„4
ğ‘¤2*ğ‘œ5
ğ‘¦1CEloss15ğ‘¥1
ğ‘¥3
ğ‘¥51.5
*
+
ğ‘¥6exp(.)
ğ‘¥7ğ‘¥2
ğ‘¥42.0
*
+f = x1 +  exp( 1.5 * x1 + 2.0 * x2) 
Computation:
1.Topological sorting of all 
nodes
2.Calculate the value for 
each node given its inputx1= 3, x2=0.5â€¢Put all nodes into un -
processed queue. 
â€¢Repeatedly, find a node 
without incoming edges from 
un-processed nodes
oevaluate its value based on 
operation
oremove the node from the 
queue and add it to processed 
queue
16Topological Sort
ğ‘¥1
ğ‘¥3
ğ‘¥51.5
*
+
ğ‘¥6exp(.)
ğ‘¥7ğ‘¥2
ğ‘¥42.0
*
+â€¢Most autodiff  systems, including Pytorch /Autograd , explicitly 
construct the computation graph. 
â€¢TensorFlow provide mini -languages for building 
computation graphs directly. 
â€¢Disadvantage: need to learn a totally new API. 
â€¢Autograd  (JAX) instead builds them by tracing the forward 
pass computation (similar to numpy ).
17Building Computation Graphâ€¢Node class, with attributes
ovalue: the actual value computed on a particular set of inputs 
ofun: the primitive operation defining the node 
oargs and kwargs : the arguments the op was called with 
oparents: the parent Nodes
â€¢More details in next lecture
18Implementation
https://github.com/mattjj/autodidact  â€¢Autogradâ€™s  NumPy module provides primitive ops which 
look and feel like NumPy functions, but secretly build the 
computation graph.
19Wrapper around Numpy
â€¢Learning algorithm for Neural Network
â€¢Computation Graph
â€¢Auto Differentiation
oconstructing the computation graph for calculating gradients
20Today â€™s Topic
â€¢To learn a neural network, we need gradient of loss function 
w.r.t.  parameters. 
â€¢Parameters are also variables, and represented as nodes in 
the computation graph.
â€¢Chain rule => backpropogation
ğ‘‘ğ‘¦(ğ‘§)
ğ‘‘ğ‘¥=ğ‘‘ğ‘¦(ğ‘§)
ğ‘‘ğ‘§âˆ™ğ‘‘ğ‘§
ğ‘‘ğ‘¥
21Gradient Calculation22y=x1 +  exp(1.5 * x1 + 2.0 * x2) Computing the derivatives ğœ•ğ‘¦
ğœ•ğ‘¥ğ‘–
Define à´¥ğ‘¥ğ‘–=ğœ•ğ‘¦
ğœ•ğ‘¥ğ‘–x1= 3, x2=0.5
ğ‘¥1
ğ‘¥3
ğ‘¥5ğ‘¤1
=1.5
*
+
ğ‘¥6exp(.)
ğ‘¥7ğ‘¥2
ğ‘¥4ğ‘¤2
=2.0
*
+23y=x1 +  exp( 1.5 * x1 + 2.0 * x2) Computing the derivatives ğœ•ğ‘¦
ğœ•ğ‘¥ğ‘–
Define à´¥ğ‘¥ğ‘–=ğœ•ğ‘¦
ğœ•ğ‘¥ğ‘–
ğ‘¥7=1
ğ‘¥6=1
ğ‘¥5=ğœ•ğ‘¦
ğœ•ğ‘¥6âˆ™ğœ•ğ‘¥6
ğœ•ğ‘¥5=ğ‘¥6âˆ™exp(ğ‘¥5)
ğ‘¥4=ğœ•ğ‘¦
ğœ•ğ‘¥5âˆ™ğœ•ğ‘¥5
ğœ•ğ‘¥4=ğ‘¥5
ğ‘¥3=ğœ•ğ‘¦
ğœ•ğ‘¥5âˆ™ğœ•ğ‘¥5
ğœ•ğ‘¥3=ğ‘¥5
ğ‘¤2=ğœ•ğ‘¦
ğœ•ğ‘¥4âˆ™ğœ•ğ‘¥4
ğœ•ğ‘¤2=ğ‘¥4âˆ™ğ‘¥2x1= 3, x2=0.5
ğ‘¥1
ğ‘¥3
ğ‘¥5ğ‘¤1
=1.5
*
+
ğ‘¥6exp(.)
ğ‘¥7ğ‘¥2
ğ‘¥4ğ‘¤2
=2.0
*
+24ğ‘¦ ğ‘§ğ‘¥ Ò§ğ‘¥=à´¤ğ‘¦âˆ™ğœ•ğ‘¦
ğœ•ğ‘¥+Ò§ğ‘§âˆ™ğœ•ğ‘§
ğœ•ğ‘¥
â€¦Node with multiple outgoing edgesJacobian
ğ½=ğœ•ğ‘¦
ğœ•ğ‘¥=ğœ•ğ‘¦1
ğœ•ğ‘¥1ğœ•ğ‘¦1
ğœ•ğ‘¥2
ğœ•ğ‘¦2
ğœ•ğ‘¥1ğœ•ğ‘¦2
ğœ•ğ‘¥2
25Partial derivatives for Vectors
row: keep y index, iter x index
col: keep x index, iter y indexğ½=ğœ•ğ‘¦
ğœ•ğ‘¥=ğœ•ğ‘¦1
ğœ•ğ‘¥1ğœ•ğ‘¦1
ğœ•ğ‘¥2
ğœ•ğ‘¦2
ğœ•ğ‘¥1ğœ•ğ‘¦2
ğœ•ğ‘¥2
â€¢computing the partial derivative for each node (vector)
Ò§ğ‘¥=ğ½ğ‘‡à´¤ğ‘¦
26Vector Jacobian Productğ‘¦=ğ‘Šğ‘¥
Ò§ğ‘¥=ğ‘Šğ‘‡à´¤ğ‘¦
27Exampleâ€¢For each primitive operation, we must specify VJPs for each 
of its arguments. 
â€¢defvjp  (defined in core.py ) is a convenience routine for 
registering VJPs. 
 defvjp (anp.exp ,    lambda g, ans, x: ans * g)
28Implementing Vector -Jacobian Product (VJP)â€¢Instead of explicitly computing the derivatives (gradients) for 
each data sample following the backward direction
â€¢Construct a computation graph for gradient calculation for 
every node 
â€¢Applicable to any input data (and output=loss)
29Auto Differentiation30y=x1 +  exp(1.5 * x1 + 2.0 * x2) Computing the derivatives ğœ•ğ‘¦
ğœ•ğ‘¥ğ‘–
Define à´¥ğ‘¥ğ‘–=ğœ•ğ‘¦
ğœ•ğ‘¥ğ‘–x1= 3, x2=0.5
ğ‘¥1
ğ‘¥3
ğ‘¥5ğ‘¤1
=1.5
*
+
ğ‘¥6exp(.)
ğ‘¥7ğ‘¥2
ğ‘¥4ğ‘¤2
=2.0
*
+31y=x1 +  exp(1.5 * x1 + 2.0 * x2) x1= 3, x2=0.5
ğ‘¥1
ğ‘¥3
ğ‘¥5ğ‘¤1
=1.5
*
+
ğ‘¥6exp(.)
ğ‘¥7ğ‘¥2
ğ‘¥4ğ‘¤2
=2.0
*
+
ğ‘¥7
=1ğ‘¥6ğ‘¥4
ğ‘¥5
exp(.)
idğ‘¥5â†’6 *idğ‘¤2*ğ‘¥2
*
ğ‘¥3idğ‘¤1*ğ‘¥1
ğ‘¥1â†’3*+Quiz
32ğ‘¥1
ğ‘¥2
ğ‘¥4ğ‘¤
*
ğ‘¥6exp(.)
ğ‘¥7+ğ‘¥5
=1ğ‘¥3-
1/x33Implementing Backward Pass 
(important for HW 1)
34Build the AutoDiff  Graph
35Use AutoGrad
â€¢use finite differences to check our gradient calculations
ğœ•ğ‘“(ğ‘¥1,ğ‘¥2)
ğœ•ğ‘¥1=ğ‘“ğ‘¥1+â„,ğ‘¥2âˆ’ğ‘“(ğ‘¥1âˆ’â„,ğ‘¥2)
2â„
â€¢Care the precision!
oUse double precision (fp64)
oPick a small â„=0.000001
oCompute the forward difference through the graph twice
36How to check the correctness of gradientâ€¢Learning parameters of an NN needs gradient calculation
â€¢Computation Graph
oto perform computation: topological traversal along the DAG
â€¢Auto Differentiation
obuilding backward computation graph for gradient calculation
â€¢https:// github.com /mattjj /autodidact/
37Summaryâ€¢Auto Diff survey, https://arxiv.org/abs/1502.05767  
â€¢The Elements of Differentiable Programming (Book), 
https://arxiv.org /abs/2403.14606  
38Additional Readingâ€¢TensorFlow: A System for Large -Scale Machine Learning, 
OSDI 2016.
39Reading for Next Class