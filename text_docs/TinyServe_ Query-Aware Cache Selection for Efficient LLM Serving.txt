TinyServe: Query-Aware Cache Selection for Efficient LLM
Serving
Dong Liu
Yale University
New Haven, CT, USA
dong.liu.dl2367@yale.eduYanxuan Yu
Columbia University
New York, NY, USA
yy3523@columbia.edu
Abstract
Serving large language models (LLMs) efficiently remains challeng-
ing due to the high memory and latency overhead of key-value
(KV) cache access during autoregressive decoding. We present Tiny-
Serve , a lightweight and extensible serving system for deploying
tiny LLMs (e.g., TinyLLaMA, GPT2-345M) with support for struc-
tured KV sparsity, plugin-based token selection, and hardware-
efficient attention kernels. Unlike prior simulation frameworks,
TinyServe executes real-time decoding with configurable sparsity
strategies and fine-grained instrumentation.
To reduce decoding cost, we introduce a query-aware page selec-
tionmechanism that leverages bounding-box metadata to estimate
attention relevance between the query and KV cache blocks. This
enables selective KV loading with minimal overhead and no model
modifications. Our fused CUDA kernel integrates page scoring,
sparse memory access, and masked attention in a single pass.
Experiments show that TinyServe achieves up to 3.4Ã—speedup
and over 2Ã—memory savings with negligible accuracy drop. Addi-
tional analysis of cache reuse, page hit rate, and multi-GPU scaling
confirms its practicality as an efficient system-level design for LLM
training and inference research on resource-constrained hardware.
CCS Concepts
â€¢Computer systems organization â†’Parallel architectures .
Keywords
efficient inference, efficient training, LLM serving, KV cache, spar-
sity, runtime systems, CUDA kernels
ACM Reference Format:
Dong Liu and Yanxuan Yu. 2025. TinyServe: Query-Aware Cache Selection
for Efficient LLM Serving. In Proceedings of the 33rd ACM International
Conference on Multimedia (MM â€™25), October 27â€“31, 2025, Dublin, Ireland.
ACM, New York, NY, USA, 10 pages. https://doi.org/10.1145/3746027.3758181
1 Introduction
Large Language Models (LLMs) have become central to modern
AI applications, powering systems in dialogue, retrieval, summa-
rization, and code generation. While recent efforts have greatly
improved model quality, the cost of training and inference has
emerged as the dominant bottleneck in deployment under long-
context or high-throughput conditionsâ€”has. Decoding each token
This work is licensed under a Creative Commons Attribution 4.0 International License.
MM â€™25, Dublin, Ireland
Â©2025 Copyright held by the owner/author(s).
ACM ISBN 979-8-4007-2035-2/2025/10
https://doi.org/10.1145/3746027.3758181requires repeated attention over a growing key-value (KV) cache,
stressing memory, latency, and compute efficiency. As a result, re-
cent systems such as vLLM [ 7], TGI [ 6], and FasterTransformer [ 26]
have introduced sophisticated strategies like paged attention, spec-
ulative decoding, and cache reordering to reduce overhead.
Despite these engineering advances, understanding the inter-
nal dynamics of LLM training and inference remains difficult. Core
trade-offsâ€”such as sparsity vs. accuracy, batching latency vs. through-
put, or memory usage vs. token reuseâ€”often behave unpredictably,
and full-scale evaluations on 7B+ models are prohibitively expen-
sive and difficult to interpret. Moreover, system researchers are
often forced to treat models as black boxes, unable to validate hy-
potheses or perform design iteration without access to large GPU
clusters.
TinyServe: Large-Scale Serving at Small Scale. We introduce
TinyServe , a lightweight serving framework that enables detailed
analysis of LLM training and inference behavior using tiny models
(e.g., 125Mâ€“350M parameters). TinyServe replicates core compo-
nents of LLM servingâ€”streaming decoding, KV cache management,
token routing, and quantizationâ€”in a fully controllable environ-
ment. Crucially, it supports fine-grained instrumentation and plug-
in modules such as entropy-based early exit, query-aware KV se-
lection, and approximate attention.
Our central insight is that many critical serving behaviorsâ€”such
as attention bottlenecks, context boundary effects, and cache spar-
sity dynamicsâ€”emerge in small models under realistic serving work-
loads. By emulating production serving scenarios with tiny LLMs,
we can approximate the performance trends and failure modes of
large-scale deployments at a fraction of the cost.
Query-Aware Sparsity and Efficient KV Access. To demonstrate
the utility of TinyServe, we propose a query-aware token selection
mechanism that leverages low-cost metadata to dynamically se-
lect the most relevant parts of the KV cache for each query. This
design emulates practical attention sparsity patterns and yields
substantial memory and latency savings while preserving accuracy.
We evaluate this mechanism across PG19, LongBench, and pro-
duction serving tasks, and find that it achieves up to 3.4 Ã—speedup
with minimal performance degradationâ€”even under aggressive
KV budgets. Additionally, TinyServe supports training acceleration
through efficient gradient checkpointing and memory-optimized
backpropagation for fine-tuning scenarios.
Our contributions are:
â€¢We propose TinyServe , a serving framework that enables
fast, interpretable training inference using tiny yet efficient
architecture.arXiv:2509.12211v1  [cs.DC]  28 Aug 2025MM â€™25, October 27â€“31, 2025, Dublin, Ireland Dong Liu and Yanxuan Yu
â€¢We introduce a query-aware KV selection mechanism that
captures sparsity patterns conditioned on current queries,
reducing memory movement while preserving accuracy.
â€¢We conduct extensive experiments on both standard and
diagnostic datasets, demonstrating that TinyServe faithfully
replicates key latency-accuracy tradeoffs observed in large
models.
By bridging system-level research and efficient experimentation,
TinyServe paves the way for accessible, reproducible, and theory-
informed studies of LLM serving behavior.
2 Related Work
2.1 Small-Scale Models for LLM serving
While most LLM research focuses on large-scale models with bil-
lions of parameters, several recent works advocate for using small-
scale models as scientific tools to probe and understand model be-
havior. TinyStories [ 3] and TinyLLaMA [ 33] demonstrate that small
language models (125Mâ€“350M) can capture many linguistic prop-
erties seen in larger models when trained appropriately. Induction
head analyses [ 27] and circuit-level interpretability [ 25] further
reveal that elementary synthetic tasks can uncover generalizable
mechanisms such as copying, compositionality, and positional bias.
Our work continues this line by repurposing tiny LLMs for training
and inference-level analysis , showing that even at small scale,
token-wise latency, cache reuse behavior, and accuracy degradation
can be faithfully reproduced.
In addition, recent works such as MT2ST [ 17], DRTR [ 14], and
Distance Recomputator [ 10] highlight how small- to medium-scale
models can provide deep insights into representation learning and
graph neural networks. Similarly, TinyServe [ 18] demonstrates
the value of small-scale models as efficient proxies for large-scale
inference profiling.
2.2 LLM Inference Profiling and Acceleration
Many system-level frameworks have been proposed to optimize
the inference efficiency of large models. vLLM [ 7] introduces Page-
dAttention to improve memory and batching efficiency during
multi-turn decoding. FasterTransformer, TGI, and TensorRT-LLM
further implement custom CUDA kernels, fused ops, and quantiza-
tion strategies to reduce latency. However, profiling these systems is
computationally expensive and often obscures fine-grained insights
due to complexity and variability in deployment.
Recent system contributions include FastCache [ 23], which pro-
poses learnable linear approximations for diffusion transformers,
and PiKV [ 19,20], which targets KV cache management for mixture-
of-experts models. Quantization has also been extensively studied,
with LLMEasyQuant [ 16] providing a scalable framework for dis-
tributed inference. Designing surveys [ 9,22] further consolidate
best practices and open challenges in efficient LLM training and
inference.
Instead of profiling production-scale models, our work intro-
duces TinyServe , a lightweight efficient serving framework using
small LLMs to reproduce the key serving stack componentsâ€”streamingattention, dynamic batching, and quantized decodingâ€”under re-
alistic serving scenarios. This enables fast hypothesis testing of
architectural changes with minimal compute cost.
We also note related work on Memory-Keyed Attention (MKA) [ 21],
which extends attention mechanisms for long-context reasoning,
and data-centric safety frameworks [ 13], which highlight broader
efficiency and safety concerns in LLM deployment.
2.3 Serving-Oriented Benchmarking
Inspired by algorithmic reasoning and interpretability benchmarks,
recent works explore the use of serving-oriented tasks to elicit
specific behaviors from LLMs. For instance, tasks such as copying,
counting, and rare token recall have been used to diagnose atten-
tion failures [ 24] and memory degradation [ 31]. Our work adapts
this idea specifically to the serving domain , creating targeted
serving scenarios that stress attention reuse, cache fragmentation,
and token entropy behavior in the decoding loop. These serving
benchmarks enable precise evaluation of system interventions (e.g.,
pruning or approximation) and help isolate root causes of serving
inefficiencies.
Beyond LLMs, graph learning acceleration systems like Graph-
SnapShot [ 11,12,15] explore caching and retrieval strategies to
optimize large-scale graph training. These works emphasize that
carefully designed synthetic stressors and caching strategies are
essential for both graph-based and language-based workloads, re-
inforcing the importance of lightweight analysis frameworks.
3 Methodology
3.1 System Overview: TinyServe
TinyServe is a lightweight serving framework designed for serving
tiny language models under tight memory and latency constraints.
Rather than acting as a benchmarking tool, TinyServe serves as a
real-time serving environment that enables sparsity-aware atten-
tion, modular token selection, and efficient KV-cache reuse.
The system is organized around three core components:
(1)Query-Aware KV Retriever: Dynamically selects relevant
key-value blocks at decode time based on the current query
vector and page-level metadata, reducing unnecessary mem-
ory access.
(2)Modular Scheduling Pipeline: A dispatch loop handles
incoming queries and routes them through configurable plug-
ins (e.g., entropy-based early exit, token-level pruning, ap-
proximate attention). This modular design allows experimen-
tation with different sparsity strategies without modifying
the core model.
(3)Sparse Attention Executor: Efficiently computes attention
over selected KV pages using fused CUDA kernels, with
support for FP16/INT8 KV formats and multi-GPU dispatch.
In TinyServe, each decode step activates the TinyServe pipeline:
the query vector is used to score KV pages, top-ranked pages are
fetched, sparse attention is performed, and plug-in modules may
trigger pruning or early stopping. This design balances flexibility
and efficiency, and supports both static deployment and research
prototyping for sparsity strategies in small-scale LLMs.TinyServe: Query-Aware Cache Selection for Efficient LLM Serving MM â€™25, October 27â€“31, 2025, Dublin, Ireland
3.2 Training Acceleration and Profiling Support
Beyond inference optimization, TinyServe provides specialized sup-
port for training acceleration and fine-grained profiling. For training
scenarios, TinyServe implements gradient-aware memory manage-
ment that selectively retains KV cache entries based on gradient
magnitude during backpropagation. This approach reduces mem-
ory footprint by up to 40% during fine-tuning while maintaining
training stability.
The profiling system integrates layer-wise performance monitor-
ingwith microsecond precision, tracking attention patterns, mem-
ory access patterns, and computational bottlenecks across different
model layers. This enables detailed analysis of training dynam-
ics and helps identify optimization opportunities in both forward
and backward passes. The profiling data is collected through light-
weight instrumentation hooks that add minimal overhead (<2%) to
the training process.
For distributed training scenarios, TinyServe supports asynchro-
nous gradient synchronization with configurable communication
patterns, allowing researchers to experiment with different paral-
lelization strategies without modifying the core training loop. This
is particularly valuable for exploring efficient training strategies on
resource-constrained hardware.
3.3 Inference Time Is Dominated by Decode
Stage
LLM inference consists of two stages: prefill and decode. In the
prefill stage, all prompt tokens are embedded and transformed into
Key (ð¾), Query (ð‘„), and Value ( ð‘‰) vectors. These are stored in the
KV cache and used to compute the first output token.
During decoding, a new token is generated per step. For each
token, a fresh ð‘„is produced and compared with all stored ð¾vectors
to compute attention scores, which are then used to weigh the
corresponding ð‘‰vectors. Since decoding occurs per output token
and reads the entire KV cache each time, it accounts for the majority
of latencyâ€”especially when sequence lengths reach 16K or 32K
tokens.
3.4 Optimizing Query-Aware Sparsity for
Efficient Tiny LLMs
While prior work has demonstrated that only a fraction of KV
tokens are critical for accurate predictions [ 5,35], we observe that
the set of critical tokens varies significantly across queries. As
illustrated in figure 1, certain tokens may have minimal impact
across most decoding steps, yet become momentarily crucial when
aligned with a specific query.
To efficiently support inference in tiny LLMsâ€”where compute
and memory budgets are limitedâ€”we optimize the self-attention
mechanism through query-aware sparsity : selecting only the most
relevant KV tokens conditioned on the current query vector. This
dynamic sparsity mechanism eliminates the overhead of storing
and attending to irrelevant tokens, while maintaining accuracy by
preserving context relevant to the current decoding step.
In TinyServe, we implement query-aware routing at page granu-
larity. For each page, lightweight metadataâ€”channel-wise min and
max values of stored Key vectorsâ€”is maintained. During inference,
a score is estimated between the current Query vector and each
Figure 1: Motivation for Query-Aware Token Selection. Each
query vector attends to different subsets of KV pages. Uni-
form cache retention leads to unnecessary memory reads,
while query-aware routing enables dynamic sparsity by fo-
cusing on high-relevance regions.
pageâ€™s metadata, enabling efficient selection of top- ð¾pages with
minimal memory movement. This mechanism offers a practical
tradeoff: retaining full KV coverage in structure, but only comput-
ing over the most impactful parts.
3.5 Query-Aware Page Selection
In a standard Transformer decoder layer, the attention computation
at decode step ð‘¡involves a fresh query ð‘žð‘¡âˆˆRð‘‘attending over all
past keysð¾<ð‘¡={ð‘˜1,ð‘˜2,...,ð‘˜ ð‘¡âˆ’1}:
Attn(ð‘žð‘¡,ð¾,ð‘‰)=ð‘¡âˆ’1âˆ‘ï¸
ð‘–=1softmax(ð‘žâŠ¤
ð‘¡ð‘˜ð‘–)Â·ð‘£ð‘–
This process is latency-critical during inference due to two bot-
tlenecks:
â€¢Memory movement : loading allð‘˜ð‘–,ð‘£ð‘–from high-bandwidth
memory (HBM);
â€¢Unstructured access : attention requires full key scan with
no cache prefetch pattern.
To address this, TinyServe introduces a structured memory
layout via token grouping into fixed-size pages . Letð¾=Ãð‘ƒ
ð‘—=1Kð‘—
be partitioned into ð‘ƒ=âŒˆð‘¡/ð‘†âŒ‰pages of size ð‘†. Each pageKð‘—stores a
small metadata summary ðœ™(Kð‘—)that enables relevance estimation.
Problem Formulation. We define a relevance function ð‘Ÿ:Rð‘‘Ã—
R2ð‘‘â†’Rsuch that:
ð‘Ÿ(ð‘žð‘¡,ðœ™(Kð‘—))â‰ˆ max
ð‘˜âˆˆKð‘—ð‘žâŠ¤
ð‘¡ð‘˜
We then select a subset Sð‘¡âŠ†{1,...,ð‘ƒ}of page indices such
that:
Sð‘¡=TopKð‘—ð‘Ÿ(ð‘žð‘¡,ðœ™(Kð‘—))with|Sð‘¡|=ð¾
Attention is then only computed over the union of selected pages:
SparseAttn(ð‘žð‘¡)=âˆ‘ï¸
ð‘—âˆˆSð‘¡âˆ‘ï¸
ð‘˜ð‘–âˆˆKð‘—softmax(ð‘žâŠ¤
ð‘¡ð‘˜ð‘–)Â·ð‘£ð‘–MM â€™25, October 27â€“31, 2025, Dublin, Ireland Dong Liu and Yanxuan Yu
Relevance Function. We instantiate ð‘Ÿas a directional bounding-
box estimator , which uses per-dimension bounds:
ðœ™(Kð‘—)=(ð‘šð‘—,ð‘€ð‘—)âˆˆR2ð‘‘, (1)
ð‘Ÿ(ð‘žð‘¡,ðœ™(Kð‘—))=ð‘‘âˆ‘ï¸
ð‘–=1(
ð‘žð‘¡,ð‘–Â·ð‘€ð‘—,ð‘–,ifð‘žð‘¡,ð‘–â‰¥0
ð‘žð‘¡,ð‘–Â·ð‘šð‘—,ð‘–,ifð‘žð‘¡,ð‘–<0(2)
Hardware Execution Model. Let each pageKð‘—reside in HBM,
and assume the following: - Page fetch cost from HBM: ðœhbÂ·ð‘†cycles;
- Cache-resident metadata ðœ™(Kð‘—)is stored in SRAM or L2, costing
negligibleðœmeta; - Page selection cost is O(ð‘ƒÂ·ð‘‘), but can be fused
into a single kernel on GPU.
Letð¾pages be selected. The effective latency cost becomes:
Latencyð‘¡=ðœmetaÂ·ð‘ƒ|   {z   }
lightweight scan+ðœhbÂ·ð¾Â·ð‘†|     {z     }
KV load+ðœattn(ð¾Â·ð‘†)
This structure-aware design ensures: - Query-dependent cache
activation; - Memory-aware scheduling (e.g., prefetching selected
pages); - Reduced HBM bandwidth pressure.
System Implication. TinyServe enables dynamic query-aware
sparsity without requiring architectural retraining. The modular
implementation integrates directly into TinyServeâ€™s kernel loop
and allows hardware-sensitive scheduling: e.g., keeping hot pages
in shared memory or limiting K to match tensor core granularity.
The kernel design for TinyServe can be found at algorithm 1.
3.6 Memory Efficiency Analysis
To quantify memory access savings under query-aware sparsity, we
construct a probabilistic cost model that accounts for (1) metadata
overhead, (2) selected KV tokens, and (3) cross-step reuse. This anal-
ysis provides theoretical bounds on the performance improvements
achievable through our approach.
Let: -ð¿: total cache length (tokens); - ð‘†: page size (tokens per
page); -ð¾: number of selected pages; - ð‘€: memory per token (bytes);
-ðœŒ: reuse probability of selected pages across adjacent decode steps.
The memory movement per decode step is:
Load =2ð‘€Â·ð¿
ð‘†+ðœŒÂ·ð¾Â·ð‘†
where: -ð¿
ð‘†pages store min/max metadata (two vectors of length
ð‘‘), -ðœŒaccounts for amortized reuseâ€”i.e., only ðœŒð¾pages are newly
loaded per step.
To compare with full-cache attention, we normalize:
Memory Fraction =1
ð‘†+ðœŒÂ·ð¾Â·ð‘†
ð¿
Theoretical Bounds. For optimal page size ð‘†âˆ—=âˆšï¸
ð¿/ð¾, the mem-
ory fraction becomes:
Memory Fractionâˆ—=2âˆšï¸
ð¾/ð¿
ð‘†âˆ—+ðœŒÂ·ð¾Â·ð‘†âˆ—
ð¿=2âˆšï¸‚
ð¾
ð¿Â·ðœŒ
This provides a theoretical lower bound on memory movement.
For typical values ( ð¾=0.3ð‘ƒ,ð¿=32ð¾,ð‘†=16), we achieveâˆ¼8Ã—
reduction in memory movement compared to full-cache attention.Algorithm 1: Fused Query-Aware Sparse Attention Kernel
Require: Query vector ð‘žð‘¡âˆˆRð‘‘, Page metadata {ðœ™ð‘—=
(ð‘šð‘—,ð‘€ð‘—)}ð‘ƒ
ð‘—=1, KV-cache{ð‘˜ð‘–,ð‘£ð‘–}ð¿
ð‘–=1
Ensure: Output vector ð‘œð‘¡âˆˆRð‘‘
1:// Step 1: Relevance scoring over page metadata (in
L2/shared)
2:for all pageð‘—=1toð‘ƒin parallel do
3:ð‘ ð‘—â†0
4:forð‘–=1toð‘‘do
5:ð‘žð‘–â†ð‘žð‘¡[ð‘–]
6:ð‘ ð‘—+=ð‘žð‘–Â·
ð‘žð‘–â‰¥0 ?ð‘€ð‘—,ð‘–:ð‘šð‘—,ð‘–
7:end for
8:end for
9:// Step 2: Top- ð¾page selection (shared heap or
radix select)
10:Sð‘¡â†TopK(ð‘ 1,...,ð‘  ð‘ƒ)
11:// Step 3: Sparse KV gather (HBM access)
12:Initializeð¾selected,ð‘‰selectedâ†âˆ…
13:for allð‘—âˆˆSð‘¡in parallel do
14: Fetch pageKð‘—={ð‘˜ð‘—,1,...,ð‘˜ ð‘—,ð‘†}from HBM
15: Append keys to ð¾selected , values toð‘‰selected
16:end for
17:// Step 4: Attention computation over selected KV
pairs
18:forð‘–=1to|ð¾selected|do
19:ð‘Žð‘–â†ð‘žâŠ¤
ð‘¡ð‘˜ð‘–
20:end for
21:ð›¼â†softmax(ð‘Ž)
22:ð‘œð‘¡â†Ã
ð‘–ð›¼ð‘–Â·ð‘£ð‘–
23:RETURNð‘œð‘¡
Query-Aware Accuracy Analysis. The bounding-box estimator
introduces approximation error ðœ–in relevance scoring:
ðœ–=max
ð‘˜âˆˆKð‘—ð‘žâŠ¤
ð‘¡ð‘˜âˆ’ð‘Ÿ(ð‘žð‘¡,ðœ™(Kð‘—))
Under reasonable assumptions about query and key distributions,
we can bound this error:
E[ðœ–]â‰¤ð‘‘Â·ðœŽ2
ð‘†Â·âˆšï¸
log(ð‘†)
whereðœŽ2is the variance of key vectors within a page. This bound
ensures that our approximation maintains high accuracy while
enabling significant memory savings.
3.7 Summary
TinyServe emulates realistic LLM serving at small scale, while en-
abling fine-grained stress tests and plug-in mechanisms like query-
aware routing. It significantly reduces memory usage without sac-
rificing interpretability, making it ideal for systems research and
design validation. The additional training acceleration and profiling
capabilities further enhance TinyServeâ€™s utility as a comprehensive
platform for both serving and training research.TinyServe: Query-Aware Cache Selection for Efficient LLM Serving MM â€™25, October 27â€“31, 2025, Dublin, Ireland
4 Experiments
4.1 Experimental Setup
We evaluate TinyServe across multiple model scales: TinyLLaMA-
125M [ 33], GPT2-345M [ 29], OPT-350M [ 34], GPT2-774M, and
LLaMA-1.3B. Our comprehensive benchmarks include:
Language Modeling: PG19 [ 30], WikiText-103, C4-News Long-
Range Tasks: LongBench [ 1] (NarrativeQA, Qasper, GovReport,
TriviaQA, HotpotQA), Passkey retrieval [ 31]Reasoning Tasks:
MMLU [ 4], LAMBADA [ 28], Multi-turn QA, Code generation Serv-
ing Workloads: Production multi-user scenarios with 512-2048
concurrent requests
For larger models (GPT2-774M, LLaMA-1.3B), we implement
proper sequence length handling: inputs exceeding 2048 tokens are
processed using sliding window with 1024-token overlap, while
shorter sequences are padded appropriately. This ensures fair com-
parison across all baselines without truncation artifacts.
We evaluate against comprehensive baselines including produc-
tion systems and research methods:
â€¢Production Systems: vLLM [ 7] (PagedAttention), TGI [ 6]
(FlashAttention), TensorRT-LLM (optimized kernels)
â€¢Research Methods: StreamingLLM [ 32] (window size=2048),
SnapKV [8] (cluster size=64), PyramidKV [2] (top-k=512)
â€¢Pruning Baselines: FullCache (no pruning), SoftPrune (thresh-
old=0.1), EntropyStop (threshold=0.5)
vLLM Integration: To demonstrate practical deployment, we
have modified vLLMâ€™s PagedAttention kernel to integrate our query-
aware page selection mechanism. The TinyServe results in our ex-
periments represent the performance of this modified vLLM imple-
mentation compared to the original vLLM baseline. This integration
maintains full backward compatibility while achieving significant
performance improvements through our query-aware approach.
All methods are evaluated under identical budget constraints
(2048-8192 tokens) on 8 Ã—A100 80GB GPUs using FP16. Results are
averaged over 5 runs with standard deviation reported. Random
seeds are fixed across all experiments for reproducibility.
4.2 Comprehensive Model-Scale Evaluation
4.3 Comprehensive Ablation Study
4.4 Serving Stack Evaluation Under Multi-User
Workload
To validate TinyServeâ€™s effectiveness in realistic serving scenar-
ios, we conduct comprehensive multi-user workload experiments
comparing our modified vLLM implementation against the original
vLLM baseline:
4.4.1 Concurrent Request Simulation. We simulate realistic serving
workloads with 512-2048 concurrent requests using Poisson arrival
patterns (mean inter-arrival time: 50ms). Each request generates a
sequence of 100-500 tokens, and we measure end-to-end latency
including request queuing, session management, and response gen-
eration.
4.4.2 Session Management Analysis. We evaluate TinyServeâ€™s ses-
sion management capabilities by tracking cross-request cache reuse
and session migration overhead:
10
5
0
5
10Session ID
10.0
7.5
5.0
2.5
0.02.55.07.5
Cache Reuse0.75
0.50
0.25
0.000.250.500.751.00
Migration Cost3D Session Flow Dynamics
Session Nodes
Cache HitSession ReuseMigration
Load Balance
Response Time
Throughput
Memory EfficiencyScalability0.20.40.60.81.0
Performance Comparison Radar
TinyServe
BaselineAdvanced Session Analysis VisualizationFigure 2: Visualization demonstrating of TinyServeâ€™s session
management capabilities. Left: 3D session flow dynamics
showing spiral-like session evolution with color-coded time
progression and session nodes. Right: Performance compar-
ison radar chart across 8 key metrics (Cache Hit, Session
Reuse, Migration, Load Balance, Response Time, Throughput,
Memory Efficiency, Scalability), demonstrating TinyServeâ€™s
comprehensive advantages over baseline systems.
0
100
200
300
400
500Session Size
0.600.650.700.750.800.85
Cache Reuse Rate0.000.050.100.150.200.250.300.35
Migration Cost (ms)Session Management Performance
0.100
0.075
0.050
0.025
0.0000.0250.0500.0750.100
Cache Hit RateSession Reuse Migration Efficiency
Load Balance
Response Time Throughput0.20.40.60.81.0
TinyServe Performance Metrics
0 20 40 60 80 100
Time Steps0.650.700.750.800.850.900.951.001.05Cache Reuse Rate
Cache Reuse Over Time
0.700.750.800.850.900.951.00
GPU1 GPU2 GPU3 GPU4 GPU5 GPU6 GPU7 GPU8GPU1
GPU2
GPU3
GPU4
GPU5
GPU6
GPU7
GPU80.80 0.19 0.23 0.28 0.29
0.80 0.20 0.18 0.21 0.17
0.15 0.70 0.19
0.11 0.70 0.26 0.17 0.21
0.21 0.11 0.25 0.14 0.19 0.16
0.18 0.18 0.19 0.28 0.27 0.27
0.18 0.12 0.26 0.29 0.27 0.16
0.25 0.15 0.16 0.13Session Migration Patterns
0.00.10.20.30.40.50.60.70.8
GPU1 GPU2 GPU3 GPU4 GPU5 GPU6 GPU7 GPU80.00.20.40.60.81.0GPU Utilization0.850.92
0.780.880.95
0.820.890.91GPU Load Distribution
P99 Latency Throughput GPU Util Memory
Performance Metrics0.00.20.40.60.81.0Normalized ScorePerformance Comparison
TinyServe
vLLM
TGI
TensorRT-LLMTinyServe Session Management Analysis
Figure 3: Session management performance showing cache
reuse rate and migration overhead across different session
sizes. TinyServe achieves higher cache reuse while minimiz-
ing migration costs.
4.5 Overall Comparison
We report accuracy, latency (ms/token), throughput (tokens/s), and
KV cache hit rate across all LongBench tasks under fixed 2048 token
budget. Results are visualized as radar plots in Figure 4 with error
bars showing 95% confidence intervals. TinyServe consistently
demonstrates superior trade-offs between latency and accuracy,
while maintaining higher KV hit rate due to its query-aware selec-
tion mechanism.
4.6 Speedup Analysis across Models
We evaluate end-to-end decode latency under increasing context
lengths (up to 32k tokens). Figure 5 shows relative speedup against
FullCache baseline across three models. TinyServe achieves 2.1Ã—â€“3.4Ã—
speedup on average, significantly outperforming pruning-based
baselines.MM â€™25, October 27â€“31, 2025, Dublin, Ireland Dong Liu and Yanxuan Yu
Table 1: Comprehensive evaluation across multiple model scales, datasets, and serving scenarios. Results show mean Â±std over
5 runs. Î”indicates relative improvement over FullCache baseline. Best configurations are bolded.
ConfigurationParams
(M)Context
(K)Performance Metrics Efficiency Metrics
LongBench-Avg MMLU LAMBADA Latency
(ms)Memory
(GB)Throughput
(tokens/s)KV Hit
(%)
Accuracy (%) Î” Accuracy (%) Î” Accuracy (%) Î”
TinyLLaMA-125M (Context: 4K)
FullCache 125 4 54.2Â±0.8 - 32.1Â±0.6 - 45.8Â±0.7 - 25.1Â±0.4 2.1Â±0.1 39 .8 100 .0
StreamingLLM 125 4 52.1Â±0.9 -2.1 30.8Â±0.7 -1.3 44.2Â±0.8 -1.6 16.8Â±0.3 1.8Â±0.1 59 .5 87 .3
SoftPrune 125 4 53.5Â±0.7 -0.7 31.5Â±0.6 -0.6 45.1Â±0.7 -0.7 15.3Â±0.3 1.6Â±0.1 65 .4 89 .1
SnapKV 125 4 54.8Â±0.6 +0.6 32.4Â±0.5 +0.3 46.2Â±0.6 +0.4 14.2Â±0.2 1.4Â±0.1 70 .4 91 .7
PyramidKV 125 4 54.4Â±0.5 +0.2 32.0Â±0.4 -0.1 45.9Â±0.5 +0.1 12.8Â±0.2 1.3Â±0.1 78 .1 94 .9
TinyServe 125 4 55.2Â±0.5 +1.0 32.8Â±0.4 +0.7 46.8Â±0.5 +1.0 11.9Â±0.1 1.2Â±0.1 84 .0 96 .2
GPT2-345M (Context: 8K)
FullCache 345 8 61.7Â±0.6 - 38.9Â±0.5 - 52.4Â±0.6 - 45.2Â±0.8 4.8Â±0.2 22 .1 100 .0
StreamingLLM 345 8 58.4Â±0.8 -3.3 36.2Â±0.7 -2.7 50.1Â±0.8 -2.3 28.7Â±0.5 3.2Â±0.2 34 .8 85 .1
SoftPrune 345 8 60.2Â±0.7 -1.5 37.8Â±0.6 -1.1 51.5Â±0.7 -0.9 26.3Â±0.4 2.9Â±0.2 38 .0 88 .2
SnapKV 345 8 61.8Â±0.5 +0.1 38.7Â±0.4 -0.2 52.6Â±0.5 +0.2 24.1Â±0.3 2.6Â±0.2 41 .5 91 .3
PyramidKV 345 8 61.4Â±0.4 -0.3 38.5Â±0.3 -0.4 52.2Â±0.4 -0.2 22.4Â±0.3 2.4Â±0.2 44 .6 93 .8
TinyServe 345 8 62.8Â±0.4 +1.1 39.5Â±0.3 +0.6 53.2Â±0.4 +0.8 20.1Â±0.2 2.1Â±0.1 49 .8 95 .4
OPT-350M (Context: 8K)
FullCache 350 8 62.1Â±0.6 - 39.2Â±0.5 - 52.8Â±0.6 - 46.8Â±0.8 4.9Â±0.2 21 .4 100 .0
StreamingLLM 350 8 58.9Â±0.8 -3.2 36.5Â±0.7 -2.7 50.3Â±0.8 -2.5 29.5Â±0.5 3.3Â±0.2 33 .9 84 .7
SoftPrune 350 8 60.8Â±0.7 -1.3 38.1Â±0.6 -1.1 51.9Â±0.7 -0.9 27.2Â±0.4 3.0Â±0.2 36 .8 87 .9
SnapKV 350 8 62.3Â±0.5 +0.2 39.0Â±0.4 -0.2 53.0Â±0.5 +0.2 25.1Â±0.3 2.7Â±0.2 39 .8 90 .8
PyramidKV 350 8 61.9Â±0.4 -0.2 38.8Â±0.3 -0.4 52.6Â±0.4 -0.2 23.2Â±0.3 2.5Â±0.2 43 .1 92 .6
TinyServe 350 8 63.2Â±0.4 +1.1 39.8Â±0.3 +0.6 53.6Â±0.4 +0.8 21.2Â±0.2 2.2Â±0.1 47 .2 94 .8
GPT2-774M (Context: 16K)
FullCache 774 16 65.8Â±0.5 - 42.1Â±0.4 - 56.3Â±0.5 - 89.2Â±1.2 8.9Â±0.3 11 .2 100 .0
StreamingLLM 774 16 62.4Â±0.7 -3.4 39.8Â±0.6 -2.3 53.8Â±0.7 -2.5 56.8Â±0.8 5.8Â±0.3 17 .6 82 .4
SoftPrune 774 16 64.2Â±0.6 -1.6 41.2Â±0.5 -0.9 55.1Â±0.6 -1.2 52.3Â±0.7 5.2Â±0.3 19 .1 86 .7
SnapKV 774 16 65.9Â±0.4 +0.1 42.0Â±0.3 -0.1 56.4Â±0.4 +0.1 48.1Â±0.6 4.8Â±0.3 20 .8 89 .2
PyramidKV 774 16 65.5Â±0.3 -0.3 41.8Â±0.2 -0.3 56.1Â±0.3 -0.2 44.6Â±0.5 4.5Â±0.3 22 .4 91 .8
TinyServe 774 16 66.8Â±0.3 +1.0 42.8Â±0.2 +0.7 57.1Â±0.3 +0.8 41.2Â±0.4 4.1Â±0.2 24 .3 93 .5
LLaMA-1.3B (Context: 32K)
FullCache 1300 32 68.9Â±0.4 - 45.2Â±0.3 - 59.8Â±0.4 - 156.8Â±2.1 15.8Â±0.5 6 .4 100 .0
StreamingLLM 1300 32 65.2Â±0.6 -3.7 42.8Â±0.5 -2.4 56.9Â±0.6 -2.9 98.4Â±1.3 9.8Â±0.5 10 .2 79 .8
SoftPrune 1300 32 67.1Â±0.5 -1.8 44.1Â±0.4 -1.1 58.2Â±0.5 -1.6 91.2Â±1.1 8.9Â±0.5 11 .0 84 .3
SnapKV 1300 32 69.0Â±0.3 +0.1 45.1Â±0.2 -0.1 59.9Â±0.3 +0.1 84.6Â±0.9 8.2Â±0.5 11 .8 87 .6
PyramidKV 1300 32 68.6Â±0.2 -0.3 44.9Â±0.1 -0.3 59.6Â±0.2 -0.2 78.4Â±0.8 7.8Â±0.5 12 .8 90 .1
TinyServe 1300 32 70.2Â±0.2 +1.3 45.9Â±0.1 +0.7 60.5Â±0.2 +0.7 72.8Â±0.7 7.2Â±0.4 13 .7 92 .8
4.7 Comprehensive Task-Level Evaluation
We present task-specific accuracy and latency on all LongBench
datasets using GPT2-345M and 2048 token budget. Table 4 shows
complete results across all five tasks. TinyServe retains near-full
accuracy while achieving significant latency reduction across all
tasks.
4.8 KV Cache Efficiency and Access Breakdown
We visualize KV cache utilization over time and analyze memory
access patterns. Figure 6 shows cache reuse patterns, while Figure 7
provides detailed access breakdown. TinyServe preserves high-
relevance tokens and avoids cache flushing, resulting in higher
effective reuse rate.
4.9 Serving Synthetic Diagnostics
To validate behavioral consistency and stress-test our system under
serving conditions, we introduce three synthetic diagnostic tasks:
Repetition Tasks: We measure attention reuse efficiency using
prompts with repeated patterns (e.g., "The quick brown fox jumps
over the lazy dog. " repeated 100 times). This tests how well different
methods handle redundant information.
Rare Token Recall: We evaluate performance on low-frequency
tokens by constructing prompts with rare vocabulary items and
measuring prediction accuracy degradation under various KV cache
configurations.Attention Aliasing: We use overlapping contexts with con-
flicting positional information to expose potential confusion in
positional encoding and key conflicts.
Table 5 shows results on these serving synthetic diagnostic tasks.
TinyServe demonstrates consistent behavior across all serving
scenarios, validating its robustness.
4.10 Plugin Ablation Study
We evaluate the contribution of individual system components by
selectively disabling plugins. Table 6 shows the impact of each
component on overall performance.
4.11 System Component Ablation
We study the impact of the KV page size on latency and accuracy. As
expected, larger pages reduce estimation cost but degrade precision.
We use a default page size of 16 for best tradeoff.
4.12 Multi-GPU Scaling
We evaluate TinyServeâ€™s scalability from 1 to 8 A100 GPUs on 128
concurrent prompts. Results show near-linear scaling in throughput,
validating kernel fusion and inter-GPU cache reuse.
4.13 Reproducibility and Implementation
Details
4.13.1 Hyperparameter Search. We conducted extensive hyperpa-
rameter search for optimal configuration:TinyServe: Query-Aware Cache Selection for Efficient LLM Serving MM â€™25, October 27â€“31, 2025, Dublin, Ireland
Table 2: Comprehensive ablation study across multiple configurations, datasets, and model scales. Results show mean Â±std
over 5 runs. Î”indicates relative improvement over baseline. Best configurations are bolded.
ConfigurationParams
(M)FLOPs
(G)Performance Metrics Efficiency Metrics
LongBench-Avg MMLU LAMBADA Latency
(ms)Memory
(GB)Throughput
(tokens/s)
Accuracy (%) Î” Accuracy (%) Î” Accuracy (%) Î”
Component Ablation (TinyServe-345M Base)
Baseline (FullCache) 345 345 61.7Â±0.6 - 38.9Â±0.5 - 52.4Â±0.6 - 45.2Â±0.8 4.8Â±0.2 22 .1
Individual Components
+ Query-Aware Only 345 276 62.1Â±0.5 +0.4 39.2Â±0.4 +0.3 52.8Â±0.5 +0.4 38.4Â±0.6 4.2Â±0.2 26 .0
+ Page-Level Only 345 290 62.3Â±0.4 +0.6 39.4Â±0.3 +0.5 53.0Â±0.4 +0.6 36.8Â±0.5 4.0Â±0.2 27 .2
+ Bounding-Box Only 345 285 62.0Â±0.5 +0.3 39.1Â±0.4 +0.2 52.7Â±0.5 +0.3 39.2Â±0.6 4.3Â±0.2 25 .5
+ Fused Kernel Only 345 280 61.9Â±0.4 +0.2 39.0Â±0.3 +0.1 52.6Â±0.4 +0.2 40.8Â±0.6 4.5Â±0.2 24 .5
Pairwise Component Combinations
Query-Aware + Page-Level 345 265 62.5Â±0.4 +0.8 39.6Â±0.3 +0.7 53.2Â±0.4 +0.8 34.2Â±0.5 3.8Â±0.2 29 .2
Query-Aware + Bounding-Box 345 270 62.4Â±0.3 +0.7 39.5Â±0.2 +0.6 53.1Â±0.3 +0.7 35.6Â±0.5 3.9Â±0.2 28 .1
Query-Aware + Fused Kernel 345 268 62.3Â±0.4 +0.6 39.4Â±0.3 +0.5 53.0Â±0.4 +0.6 36.4Â±0.5 4.0Â±0.2 27 .5
Page-Level + Bounding-Box 345 275 62.6Â±0.3 +0.9 39.7Â±0.2 +0.8 53.3Â±0.3 +0.9 33.8Â±0.4 3.7Â±0.2 29 .6
Page-Level + Fused Kernel 345 272 62.5Â±0.4 +0.8 39.6Â±0.3 +0.7 53.2Â±0.4 +0.8 34.6Â±0.5 3.8Â±0.2 28 .9
Bounding-Box + Fused Kernel 345 278 62.2Â±0.3 +0.5 39.3Â±0.2 +0.4 52.9Â±0.3 +0.5 37.2Â±0.5 4.1Â±0.2 26 .9
Three-Component Combinations
w/o Fused Kernel 345 275 62.7Â±0.3 +1.0 39.8Â±0.2 +0.9 53.4Â±0.3 +1.0 33.2Â±0.4 3.6Â±0.2 30 .1
w/o Bounding-Box 345 270 62.6Â±0.4 +0.9 39.7Â±0.3 +0.8 53.3Â±0.4 +0.9 33.8Â±0.5 3.7Â±0.2 29 .6
w/o Page-Level 345 278 62.3Â±0.3 +0.6 39.4Â±0.2 +0.5 53.0Â±0.3 +0.6 36.4Â±0.5 4.0Â±0.2 27 .5
w/o Query-Aware 345 285 62.1Â±0.4 +0.4 39.2Â±0.3 +0.3 52.8Â±0.4 +0.4 38.8Â±0.6 4.2Â±0.2 25 .8
Full Configuration
FullTinyServe 345 262 62.8Â±0.3 +1.1 39.8Â±0.2 +0.9 53.5Â±0.3 +1.1 32.1Â±0.4 3.5Â±0.2 31 .2
Hyperparameter Ablation (Page Size ð‘†)
ð‘†=8 345 262 62.6Â±0.4 +0.9 39.6Â±0.3 +0.7 53.3Â±0.4 +0.9 30.8Â±0.4 3.2Â±0.2 32 .5
ð‘†=16 345 262 62.8Â±0.3 +1.1 39.8Â±0.2 +0.9 53.5Â±0.3 +1.1 32.1Â±0.4 3.5Â±0.2 31 .2
ð‘†=32 345 262 62.7Â±0.3 +1.0 39.7Â±0.2 +0.8 53.4Â±0.3 +1.0 34.2Â±0.5 3.8Â±0.2 29 .2
ð‘†=64 345 262 62.5Â±0.4 +0.8 39.5Â±0.3 +0.6 53.2Â±0.4 +0.8 36.8Â±0.5 4.2Â±0.2 27 .2
Selection Ratio Ablation (Top-K Ratio)
ð¾/ð‘ƒ=0.1 345 262 62.4Â±0.4 +0.7 39.5Â±0.3 +0.6 53.2Â±0.4 +0.8 28.4Â±0.4 3.1Â±0.2 35 .2
ð¾/ð‘ƒ=0.2 345 262 62.6Â±0.3 +0.9 39.7Â±0.2 +0.8 53.4Â±0.3 +1.0 30.2Â±0.4 3.3Â±0.2 33 .1
ð¾/ð‘ƒ=0.3 345 262 62.8Â±0.3 +1.1 39.8Â±0.2 +0.9 53.5Â±0.3 +1.1 32.1Â±0.4 3.5Â±0.2 31 .2
ð¾/ð‘ƒ=0.5 345 262 62.7Â±0.3 +1.0 39.6Â±0.2 +0.7 53.3Â±0.3 +0.9 35.8Â±0.5 4.0Â±0.2 27 .9
Attention Head Ablation
4 heads 345 262 62.5Â±0.4 +0.8 39.6Â±0.3 +0.7 53.3Â±0.4 +0.9 30.8Â±0.4 3.2Â±0.2 32 .5
8 heads 345 262 62.7Â±0.3 +1.0 39.7Â±0.2 +0.8 53.4Â±0.3 +1.0 31.4Â±0.4 3.4Â±0.2 31 .8
12 heads 345 262 62.8Â±0.3 +1.1 39.8Â±0.2 +0.9 53.5Â±0.3 +1.1 32.1Â±0.4 3.5Â±0.2 31 .2
16 heads 345 262 62.6Â±0.3 +0.9 39.6Â±0.2 +0.7 53.3Â±0.3 +0.9 33.2Â±0.5 3.8Â±0.2 30 .1
Cross-Scale Consistency (Different Model Sizes)
TinyServe-125M (Full) 125 125 55.2Â±0.5 +1.0 32.8Â±0.4 +0.7 46.8Â±0.5 +1.0 11.9Â±0.1 1.2Â±0.1 84 .0
TinyServe-345M (Full) 345 262 62.8Â±0.3 +1.1 39.8Â±0.2 +0.9 53.5Â±0.3 +1.1 32.1Â±0.4 3.5Â±0.2 31 .2
TinyServe-774M (Full) 774 618 66.8Â±0.3 +1.0 42.8Â±0.2 +0.7 57.1Â±0.3 +0.8 41.2Â±0.4 4.1Â±0.2 24 .3
TinyServe-1.3B (Full) 1300 1040 70.2Â±0.2 +1.3 45.9Â±0.1 +0.7 60.5Â±0.2 +0.7 72.8Â±0.7 7.2Â±0.4 13 .7
Table 3: Serving performance comparison: modified vLLM (TinyServe) vs. original vLLM under multi-user workload (GPT2-
345M, 1024 concurrent requests).
System P50 Latency (ms) P99 Latency (ms) Throughput (req/s) GPU Utilization (%)
vLLM 45.2 Â±2.1 128.7 Â±8.3 18.4 Â±0.9 78.3 Â±3.2
TGI 52.8 Â±3.4 156.2 Â±12.1 15.7 Â±1.2 82.1 Â±4.1
TensorRT-LLM 38.9 Â±1.8 112.4 Â±6.7 22.1 Â±1.1 85.7 Â±2.8
TinyServe 32.1 Â±1.5 89.3 Â±4.2 28.6 Â±1.4 91.2 Â±2.1
â€¢Page Size: Tested values [4, 8, 16, 32, 64], selected 16 based
on latency-accuracy trade-off
â€¢Selection Ratio: Evaluated [0.1, 0.2, 0.3, 0.5], chose 0.3 for
best performance
â€¢Batch Timeout: Tested [10ms, 25ms, 50ms, 100ms], selected
50ms for serving scenarios
4.13.2 Environment Configuration. All experiments conducted on:
â€¢Hardware: 8Ã—NVIDIA A100 80GB GPUs
â€¢Software: CUDA 11.8, cuDNN 8.7, PyTorch 2.0.1
â€¢OS:Ubuntu 20.04 LTS
â€¢Random Seeds: Fixed across all experiments (seed=42)4.13.3 Code and Data Availability. Complete implementation, datasets,
and evaluation scripts will be made publicly available upon publi-
cation. The codebase includes:
â€¢TinyServe framework implementation with modular plugin
system
â€¢All baseline implementations (vLLM, TGI, TensorRT-LLM
adapters)
â€¢Evaluation scripts for all experiments
â€¢Preprocessed datasets and model checkpointsMM â€™25, October 27â€“31, 2025, Dublin, Ireland Dong Liu and Yanxuan Yu
AccuracyLatency
Throughput
KV Hit RateTinyLLaMA-125M
FullCache
StreamingLLM
SoftPrune
EntropyStop
TinyServe
AccuracyLatency
Throughput
KV Hit RateGPT2-345M
FullCache
StreamingLLM
SoftPrune
EntropyStop
TinyServe
Figure 4: Radar plot of accuracy, latency, throughput, and KV
hit rate for TinyLLaMA (left) and GPT2-345M (right). Error
bars show 95% confidence intervals. Higher is better for all
metrics.
4.14 Summary
TinyServe consistently improves serving efficiency across diverse
models and tasks. Its query-aware token selection enables aggres-
sive memory reduction with minimal accuracy degradation. The
comprehensive evaluation across all LongBench tasks, serving syn-
thetic diagnostics, and plugin ablation studies validates the ro-
bustness and effectiveness of our approach. When used with tiny
TinyLLaMA OPT-350M GPT2-345M0.00.51.01.52.02.53.0Speedup over FullCacheRelative Decode Speedup @ 32K Prompt
StreamingLLM
SoftPrune
EntropyStop
TinyServeFigure 5: Relative decode latency speedup ( â†“) across different
baselines under 32k prompt length and 2048 token budget.
Error bars represent standard deviation over 5 runs.
0 250 500 750 1000 1250 1500 1750 2000
Decode Steps0.600.650.700.750.800.850.90KV Hit Rate
KV Cache Utilization over Time
StreamingLLM
TinyServe
Figure 6: KV reuse over decode time (context=32k, de-
code=2k). TinyServe maintains higher hit rate and fewer
token evictions.
LLMs, TinyServe allows efficient and interpretable serving profil-
ing, supporting system-level research without relying on full-scale
deployments.
5 Conclusion
We introduced TinyServe , a lightweight and extensible serving
system for efficient inference and training acceleration with tiny
language models. TinyServe bridges system-level bottlenecks in
LLM servingâ€”such as KV cache saturation and decode-time la-
tencyâ€”with modular support for token selection, cache sparsity,
fused attention kernels, and training optimization.
At the core of TinyServe is a query-aware page selection mech-
anism that approximates attention relevance using bounding-box
metadata, enabling selective KV access with minimal overhead.
This approach achieves substantial latency and memory reductionsTinyServe: Query-Aware Cache Selection for Efficient LLM Serving MM â€™25, October 27â€“31, 2025, Dublin, Ireland
Figure 7: KV cache access bandwidth over decode steps
across different caching strategies. FullCache exhibits con-
sistently high memory bandwidth usage, frequently reach-
ing the HBM limit, due to full KV reuse without pruning.
StreamingLLM improves over FullCache by discarding early
tokens, but still exhibits bursty memory loads. TinyServe
shows smoother and significantly lower access patterns, re-
maining well below the HBM threshold, benefiting from
query-aware page-level KV selection. Vertical dotted lines
mark key transitions in token reuse or decoding stages.
Table 6: Plugin ablation study (TinyLLaMA-125M, 16K con-
text, 2K budget).
Configuration Latency (ms) Accuracy (%) KV Hit (%) Memory (GB)
FullTinyServe 9.3Â±0.2 54.0 Â±0.6 91.7 Â±0.8 2.1 Â±0.1
w/o Query Router 12.8 Â±0.3 52.1 Â±0.7 87.3 Â±1.2 2.4 Â±0.2
w/o Page Manager 11.2 Â±0.2 53.5 Â±0.5 89.1 Â±0.9 2.8 Â±0.1
w/o Cache Fusion 10.7 Â±0.3 53.8 Â±0.6 90.2 Â±0.7 2.3 Â±0.1
w/o Multi-GPU 15.6 Â±0.4 54.2 Â±0.5 92.1 Â±0.6 3.2 Â±0.2
Table 7: Effect of KV Page Size on TinyServe latency and
accuracy (TinyLLaMA-125M, seq len = 16K, budget = 2048
tokens).
Page Size Latency (ms) PPL â†“ KV Hit Rate (%)
4 17.6 Â±0.4 24.3 Â±0.2 98.4 Â±0.3
8 12.1 Â±0.3 25.1 Â±0.3 94.9 Â±0.5
16 9.3 Â±0.2 26.0 Â±0.2 91.7 Â±0.4
32 7.8 Â±0.2 28.4 Â±0.4 85.6 Â±0.7
64 6.2 Â±0.1 32.5 Â±0.5 79.3 Â±0.9
Table 8: Multi-GPU throughput scaling for TinyServe (batch
size = 128 prompts, GPT2-345M, seq len = 16K).
#GPUs Tok/ms Speedup ( Ã—) Efficiency (%)
1 0.81 Â±0.02 1.00 Ã— 99.3%
2 1.58 Â±0.03 1.96 Ã— 98.0%
4 3.123 Â±0.05 3.86 Ã— 96.5%
8 6.221 Â±0.08 7.68 Ã— 96.0%Table 4: Complete LongBench evaluation (GPT2-345M, 6K
chunked input, 2K decode). Mean Â±std over 5 runs.
Task Method Acc. â†‘ Lat.â†“ Speedup â†‘
NarrativeQAFullCache 58.3 Â±0.7 25.1 Â±0.4 1.00
StreamingLLM 55.2 Â±0.9 16.8 Â±0.3 1.49
EntropyStop 56.8 Â±0.8 18.2 Â±0.2 1.38
SoftPrune 55.9 Â±0.6 15.3 Â±0.3 1.64
SnapKV 57.1 Â±0.5 14.2 Â±0.2 1.77
PyramidKV 56.4 Â±0.4 12.8 Â±0.1 1.96
TinyServe 57.8 Â±0.5 11.9 Â±0.1 2.11
QasperFullCache 52.4 Â±0.8 26.7 Â±0.3 1.00
StreamingLLM 49.1 Â±1.0 17.1 Â±0.2 1.56
SoftPrune 50.3 Â±0.7 16.2 Â±0.3 1.65
SnapKV 51.2 Â±0.6 14.8 Â±0.2 1.80
PyramidKV 50.7 Â±0.5 13.1 Â±0.2 2.04
TinyServe 51.9 Â±0.6 12.3 Â±0.1 2.17
TriviaQAFullCache 61.7 Â±0.6 23.8 Â±0.3 1.00
StreamingLLM 58.4 Â±0.8 15.2 Â±0.2 1.57
EntropyStop 59.6 Â±0.7 16.8 Â±0.2 1.42
SoftPrune 58.9 Â±0.5 14.6 Â±0.3 1.63
SnapKV 60.2 Â±0.4 13.9 Â±0.2 1.71
PyramidKV 59.5 Â±0.3 12.4 Â±0.1 1.92
TinyServe 60.8 Â±0.4 11.7 Â±0.1 2.03
HotpotQAFullCache 54.7 Â±0.8 24.3 Â±0.3 1.00
StreamingLLM 50.9 Â±1.0 15.9 Â±0.2 1.53
EntropyStop 52.1 Â±0.9 17.4 Â±0.2 1.40
SoftPrune 51.5 Â±0.7 14.1 Â±0.3 1.72
SnapKV 53.0 Â±0.6 13.5 Â±0.2 1.80
PyramidKV 52.3 Â±0.5 12.1 Â±0.1 2.01
TinyServe 54.0 Â±0.6 11.5 Â±0.1 2.11
GovReportFullCache 47.9 Â±0.6 29.1 Â±0.4 1.00
StreamingLLM 44.3 Â±0.8 17.3 Â±0.3 1.68
SoftPrune 45.5 Â±1.0 19.3 Â±0.3 1.51
SnapKV 46.7 Â±0.7 15.8 Â±0.2 1.84
PyramidKV 45.9 Â±0.5 13.2 Â±0.2 2.20
TinyServe 47.0 Â±0.5 12.6 Â±0.2 2.31
Table 5: Serving Synthetic diagnostic task results (GPT2-
345M, 5 runs each).
Task Method Repetition Rare Token Aliasing
FullCache 98.2 Â±0.3 94.7 Â±0.5 89.3 Â±0.7
StreamingLLM 95.1 Â±0.4 91.2 Â±0.6 85.1 Â±0.8
SoftPrune 94.8 Â±0.5 90.8 Â±0.7 84.7 Â±0.9
TinyServe 97.5 Â±0.3 93.8 Â±0.4 88.2 Â±0.6
without compromising accuracy, validated across PG19, LongBench,
and passkey retrieval tasks.
Through its kernel-level optimizations, multi-GPU scaling, and
plug-and-play architecture, TinyServe enables rapid, reproducible
experimentation on resource-constrained hardware. We believe it
offers a practical foundation for LLM serving research, support-
ing both real-time deployment of tiny models and the principled
evaluation of serving mechanisms without the cost of full-scale
models.MM â€™25, October 27â€“31, 2025, Dublin, Ireland Dong Liu and Yanxuan Yu
Acknowledgements
We thank the developers of Hugging Face Transformers and vLLM
for foundational open-source infrastructure.
References
[1]Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang,
Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, et al .2023. Longbench: A
bilingual, multitask benchmark for long context understanding. arXiv preprint
arXiv:2308.14508 (2023).
[2]Zefan Cai, Yichi Zhang, Bofei Gao, Yuliang Liu, Yucheng Li, Tianyu Liu, Kem-
ing Lu, Wayne Xiong, Yue Dong, Junjie Hu, and Wen Xiao. 2025. PyramidKV:
Dynamic KV Cache Compression based on Pyramidal Information Funneling.
arXiv:2406.02069 [cs.CL] https://arxiv.org/abs/2406.02069
[3]Ronen Eldan and Yuanzhi Li. 2023. TinyStories: How Small Can Language
Models Be and Still Speak Coherent English? arXiv:2305.07759 [cs.CL] https:
//arxiv.org/abs/2305.07759
[4]Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn
Song, and Jacob Steinhardt. 2021. Measuring Massive Multitask Language Un-
derstanding. In International Conference on Learning Representations (ICLR) 2021 .
https://arxiv.org/abs/2009.03300
[5]Edward J Hu, yelong shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean
Wang, Lu Wang, and Weizhu Chen. 2022. LoRA: Low-Rank Adaptation of Large
Language Models. In International Conference on Learning Representations . https:
//openreview.net/forum?id=nZeVKeeFYf9
[6]Hugging Face. 2023. Text Generation Inference (TGI). https://github.com/
huggingface/text-generation-inference.
[7]Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng,
Cody Hao Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica. 2023. Efficient
memory management for large language model serving with pagedattention. In
Proceedings of the 29th symposium on operating systems principles . 611â€“626.
[8]Yuhong Li, Yingbing Huang, Bowen Yang, Bharat Venkitesh, Acyr Locatelli,
Hanchen Ye, Tianle Cai, Patrick Lewis, and Deming Chen. 2024. SnapKV: LLM
Knows What You are Looking for Before Generation. arXiv:2404.14469 [cs.CL]
https://arxiv.org/abs/2404.14469
[9]Dong Liu. 2024. Contemporary model compression on large language models
inference. arXiv preprint arXiv:2409.01990 (2024).
[10] Dong Liu and Meng Jiang. 2024. Distance recomputator and topology reconstruc-
tor for graph neural networks. arXiv preprint arXiv:2406.17281 (2024).
[11] Dong Liu, Roger Waleffe, Meng Jiang, and Shivaram Venkataraman. 2024. Graph-
SnapShot: Graph Machine Learning Acceleration with Fast Storage and Retrieval.
CoRR abs/2406.17918 (2024). https://doi.org/10.48550/arXiv.2406.17918
[12] Dong Liu, Roger Waleffe, Meng Jiang, and Shivaram Venkataraman. 2025.
GraphSnapShot: Caching Local Structure for Fast Graph Learning. (2025).
arXiv:2406.17918 [cs.LG] https://arxiv.org/abs/2406.17918
[13] Dong Liu and Yanxuan Yu. 2025. A Data-Centric Safety Framework for Generative
Models: Adversarial Fingerprint Detection and Attribution. In Data in Generative
Models - The Bad, the Ugly, and the Greats . https://openreview.net/forum?id=
y9tP2b7FAP
[14] Dong Liu and Yanxuan Yu. 2025. DRTR: Distance-Aware Graph Representation
Learning. (2025). arXiv:2406.17281 [cs.LG] https://arxiv.org/abs/2406.17281
[15] Dong Liu and Yanxuan Yu. 2025. GraphSnapShot: A System for Graph Machine
Learning Acceleration. In Machine Learning for Computer Architecture and Systems
2025. https://openreview.net/forum?id=KeHes2SVxs
[16] Dong Liu and Yanxuan Yu. 2025. LLMEasyQuant: Scalable Quantization for
Parallel and Distributed LLM Inference. arXiv:2406.19657 [cs.LG] https://arxiv.
org/abs/2406.19657
[17] Dong Liu and Yanxuan Yu. 2025. MT2ST: Adaptive Multi-Task to Single-Task
Learning. (2025). arXiv:2406.18038 [cs.LG] https://arxiv.org/abs/2406.18038
[18] Dong Liu and Yanxuan Yu. 2025. TinyServe: Query-Aware Cache Selection for
Efficient LLM Inference. In ICML 2025 Workshop on Methods and Opportunities at
Small Scale . https://openreview.net/forum?id=sOdtl4jLci
[19] Dong Liu, Yanxuan Yu, Ben Lengerich, Ying Nian Wu, and Xuhong Wang.
2025. PiKV: KV Cache Management System for Mixture of Experts. (2025).arXiv:2508.06526 [cs.DC] https://arxiv.org/abs/2508.06526
[20] Dong Liu, Yanxuan Yu, Ben Lengerich, Ying Nian Wu, and Xuhong Wang. 2025.
PiKV: KV Cache Management System for MoE Architecture. In ES-FoMo III: 3rd
Workshop on Efficient Systems for Foundation Models . https://openreview.net/
forum?id=hHoK1kBPd9
[21] Dong Liu, Yanxuan Yu, Xuhong Wang, Ben Lengerich, and Ying Nian Wu. 2025.
MKA: Memory-Keyed Attention for Efficient Long-Context Reasoning. In ICML
2025 Workshop on Long-Context Foundation Models . https://openreview.net/
forum?id=r1GbqYMJys
[22] Dong Liu, Yanxuan Yu, Yite Wang, Jing Wu, Zhongwei Wan, Sina Alinejad, Ben-
jamin Lengerich, and Ying Nian Wu. 2025. Designing Large Foundation Models
for Efficient Training and Inference: A Survey. (2025). arXiv:2409.01990 [cs.DC]
https://arxiv.org/abs/2409.01990
[23] Dong Liu, Jiayi Zhang, Yifan Li, Yanxuan Yu, Ben Lengerich, and Ying Nian Wu.
2025. FastCache: Fast Caching for Diffusion Transformer Through Learnable
Linear Approximation. arXiv preprint (2025). arXiv:2505.20353 [cs.LG] https:
//arxiv.org/abs/2505.20353
[24] Nelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua,
Fabio Petroni, and Percy Liang. 2023. Lost in the Middle: How Language Models
Use Long Contexts. arXiv:2307.03172 [cs.CL] https://arxiv.org/abs/2307.03172
[25] Neel Nanda, Lawrence Chan, Tom Lieberum, Jess Smith, and Jacob Stein-
hardt. 2023. Progress measures for grokking via mechanistic interpretability.
arXiv:2301.05217 [cs.LG] https://arxiv.org/abs/2301.05217
[26] NVIDIA. 2021. FasterTransformer: NVIDIA Transformer Optimization Toolkit.
https://github.com/NVIDIA/FasterTransformer.
[27] Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma,
Tom Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Con-
erly, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez, Scott
Johnston, Andy Jones, Jackson Kernion, Liane Lovitt, Kamal Ndousse, Dario
Amodei, Tom Brown, Jack Clark, Jared Kaplan, Sam McCandlish, and Chris
Olah. 2022. In-context Learning and Induction Heads. arXiv:2209.11895 [cs.LG]
https://arxiv.org/abs/2209.11895
[28] Denis Paperno, GermÃ¡n Kruszewski, Angeliki Lazaridou, Quan Ngoc Pham,
Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel
FernÃ¡ndez. 2016. The LAMBADA dataset: Word prediction requiring a broad
discourse context. arXiv:1606.06031 [cs.CL] https://arxiv.org/abs/1606.06031
[29] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever,
et al.2019. Language models are unsupervised multitask learners. OpenAI blog
1, 8 (2019), 9.
[30] Jack W. Rae, Anna Potapenko, Siddhant M. Jayakumar, Chloe Hillier, and Timo-
thy P. Lillicrap. 2020. Compressive Transformers for Long-Range Sequence
Modelling. In International Conference on Learning Representations . https:
//openreview.net/forum?id=SylKikSYDH
[31] Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal.
2023. Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-
Intensive Multi-Step Questions. arXiv:2212.10509 [cs.CL] https://arxiv.org/abs/
2212.10509
[32] Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. 2024.
Efficient Streaming Language Models with Attention Sinks. In The Twelfth Inter-
national Conference on Learning Representations . https://openreview.net/forum?
id=NG7sS51zVF
[33] Peiyuan Zhang, Guangtao Zeng, Tianduo Wang, and Wei Lu. 2024. TinyLlama:
An Open-Source Small Language Model. arXiv:2401.02385 [cs.CL] https://arxiv.
org/abs/2401.02385
[34] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui
Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov,
Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali
Sridhar, Tianlu Wang, and Luke Zettlemoyer. 2022. OPT: Open Pre-trained
Transformer Language Models. (2022). arXiv:2205.01068 [cs.CL] https://arxiv.
org/abs/2205.01068
[35] Zhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen, Lianmin Zheng, Ruisi
Cai, Zhao Song, Yuandong Tian, Christopher RÃ©, Clark Barrett, Zhangyang
Wang, and Beidi Chen. 2023. H 2O: Heavy-Hitter Oracle for Efficient Generative
Inference of Large Language Models. (2023). arXiv:2306.14048 [cs.LG] https:
//arxiv.org/abs/2306.14048