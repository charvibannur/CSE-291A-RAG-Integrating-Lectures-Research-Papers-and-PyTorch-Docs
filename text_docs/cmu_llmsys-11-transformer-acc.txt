Lei LiAccelerating Transformer Training and Inference
Natural Language Supervision for Vision (CLIP)
Language Model (BERT, T5, GPT3/4)
Image Classiï¬cation (Vision Transformer, Swin-Transformer)
Speech Recognition (wav2vec, HuBERT)Transformers
Text to Image (Stable Diï¬ƒsion)
Transformer Models as universal architecture
2ğŸ’°
Training Large Models Are Expensive!
3Carbon footprint: Training GPT3 = driving a car for 146 years!
= 1 Car Year CO2Recap Transformer Architecture
4
<BOS>IlikeIlikesingEncoderDecoder
MHA + FFNMHA +FFNMHA + FFNMHA + FFNMHA + FFNMHA + FFNMHA + FFNMHA + FFNMHA + FFNMHA + FFNSoftmaxSoftmaxSoftmaxI like singing and dancing.
Token Embedding Tableæˆ‘â¼€å’Œâ€¦
MHA + FFN
Token Embedding TableIlike youâ€¦
æˆ‘å–œæ¬¢å”±æ­Œå’Œè·³èˆã€‚
Transformer Training Stages
5
Model Replicas
Data Shards
SamplesForwardBackward
GradientsSynchronization
Aggregated GradientsUpdate ParametersThis Lecture Accelerated GPU Computation for Transformer Trainingfor moderate model size (< GPU memory)
based on LightSeq libraryLightSeq: A High Performance Inference Library for Transformers. Wang et al 2021. LightSeq2: Accelerated Training for Transformer-based Models on GPUs. Wang et al 2022. TensorRT-LLM (FasterTransformer), only for inferenceComparison of Acceleration Libraries for Transformers
7Full TransformerTrainingInferencePyTorchTensorï¬‚owTensorRT-LLMâœ”âœ˜âœ”âœ”âœ”âœ”âœ˜âœ”âœ”?âœ”*âœ”âœ”âœ”âœ˜âœ”âœ”âœ”âœ”âœ”
*DeepSpeed implemented Transformer Kernel in Oct 2022 pâ€² 
pâ€² 
p
p
gâ€² 
gâ€² 
g
g
LightSeq/LightSeq2 Optimization Overview
8
x
x
Dropout
x
x
x
Dropout
Computational Graph Optimizations
g1
gâ€² 1
pâ€² 1
p1
Copy FP32
+
Copy FP16
g
p
fp32+
OptimizeOptimize
âˆ‡x
âˆ‡x
âˆ‡w
Ã—
Layer Norm
âˆ‡x
Reuse MemoryTrainer SpeedupMemory Management
Ïƒ(x)
Î¼(x)
Mean
x
x
Î¼(x2)
Dependent Reduction RewritingOptimizeVar
x
Î¼(x)
Mean Square
Mean
Ïƒ(x)
Technique 1: Kernel Fusion
9
3~5Î¼s
=4000 cyclesKernel Fusion ExampleC = A + B
E = C + D
needs two kernel executions.
If we write a custom kernel to add three matrices
E = A + B + C
only needs one kernel. 
Beneï¬ts: 
â€¢reduce overhead
â€¢reduce extra memory access104 load, 2 stores, 2 matrix add.3 load, 1 store, 2 matrix add.MultiHead Attention And Feed Forward Network
11
VxKxQx
Dot Product
Softmax
LinearÃ—
LayerNormyi=wixiâˆ’Î¼(x)Ïƒ(x)+bi
+
LinearReLU
Linear
LayerNorm+MHAFFNBias adding & Dropout & Residual
Y=Yâ‹…Wout
Reshape  Y
Y=Sâ‹…V
Dropout
Softmax
S=Qâ‹…KT/dK
Bias adding & Reshape Q,K,V
Q,K,V=Yâ‹…(WQ,WK,WV)
LayerNorm
Embedding
LayerNorm
Y=Yâ‹…W1
Bias adding & ReLU & Dropout
Y=Yâ‹…W2
Bias adding & Dropout & Residual
Softmax
Y=Yâ‹…Wemb
Cross Entropy
N Layers â€¦
Accelerate non-GEMM Operators via Fusion
12
Self AttentionFFNCriterion
Custom Reduce
cuBLAS GEMM
Custom ElementwiseFused Embedding Forward Operator
13
w
Ew
Lookup
E
Ã—
x
s
+
p
Pp
Lookup
P
x
Dropout
y
Mask
Position
Embedding
Table
Word
Embedding
Table
WordPosition
Scale
(Constant)
Fuse
w
Dropout(sâ‹…Ew+Pp)
E
s
p
P
y
Mask
y=Dropout(sâ‹…Ew+Pp)Less IO for intermediate results Less Kernel launch overhead
5 cuda kernel launches1 cuda kernel launchFused Embedding Backward Operator
14
w
âˆ‡Ew
Reduce Sum
âˆ‡E
Ã—
s
âˆ‡x
âŠ™
âˆ‡y
Mask
w
âˆ‡E
s
âˆ‡y
Mask
  Mask & AtomicAddsâ‹…âˆ‡yâŠ™Aggregate Gradients of same words
Fuseâˆ‡E=ReduceSum(sâ‹…âˆ‡yâŠ™Mask)3 cuda kernel launches1 cuda kernel launchCode Example: Embedding Forward
15
Word Embedding
Lookup Positional Embedding
Lookup 
ScaleDropout mask
Apply dropout
https://github.com/bytedance/lightseq/blob/master/lightseq/csrc/kernels/cuda/embedding_kernels.cuCode Example: Embedding Backward
16Gradient clipping
Gradient accumulationDropout mask
Scale 
Reduce sum
https://github.com/bytedance/lightseq/blob/master/lightseq/csrc/kernels/cuda/embedding_kernels.cuGradient of Criterion Operator
17Smoothed one-hot ground truthSoftmax outputÎ±: smoothing parameter, 0< Î± < 1
V: vocabulary size, length of p, q
Gradient of Softmax
When i  is equal to ground truth token index k:Otherwise 
Therefore p=(1âˆ’Î±)y+Î±Vâ‹…1q=Softmax(h)â„’=âˆ’âˆ‘ipilog(qi)
g=qâˆ’p=>  Fused Criterion Operator
18
q
x
â„’
â„’=âˆ’âˆ‘ipilog(qi)
p
Forward
SoftmaxLog & Inner  ProdFusion
Smoothed One Hot (Constant)
âˆ‡x
q
p+
Backwardâˆ‡x=qâˆ’pWith some calculations: element-wise operator
Layer-Batched Cross Attention
19
BatchÃ—Encoder
x
Decoder LayerDecoder LayerDecoder Layer
Ã—
WL
W2
Ã—
W1
Ã—
â€¦
â€¦
OriginalEncoder
x
Decoder LayerDecoder LayerDecoder Layer
Ã—
â€¦
[W1,â€¦,WL]
Split
 â€¦ y1yL
LightSeq2One Big GEMM
Technique 2: Reduce synchronization
20Rewrite Reduction: LayerNorm Forward
21
Ïƒ(x)
Î¼(x)
Mean
x
Var
Ïƒ(x)=1Nâˆ‘i(xiâˆ’Î¼(x)i)2
x
Î¼(x2)
x
Î¼(x)
Mean Square
Mean
Ïƒ(x)
Ïƒ(x)=Î¼(x2)âˆ’Î¼(x)2
RewriteTwo thread synchronizati
One thread synchronization 
storage: FP16
Calculation: FP32
yi=wixiâˆ’Î¼(x)Ïƒ(x)+biLayerNorm:                          rescales input for stabilityMeanStandard devRewrite Reduction: LayerNorm Backward
22Î±=[xiâˆ’Î¼(x)]Î¼(x)âˆ’Ïƒ(x)mÏƒ(x)3âˆ‡xi=wiâˆ‡yiÏƒ(x)+Î±â‹…âˆ‘jwjâˆ‡yj+Î²â‹…âˆ‘jwjâˆ‡yjxjwhereÎ²=Î¼(x)âˆ’ximÏƒ(x)3
x
w
Inner Prod
âˆ‡y
Inner Prod
âˆ‘wjâˆ‡yj
âˆ‘wjxjâˆ‡yj
Ïƒ(x)
Î¼(x)
Element -wise OP 
âˆ‡x
One thread synchronization 
You will implement LayerNorm in hw3!
Before:Rearrange:Z
x*
Max 
x
Add & SumExp 
y
Exp & Divide 
yi=exiâˆ‘exjRewrite Reduction: Softmax Forward
23Two thread synchronizations
Z=âˆ‘iexiâˆ’x*
{headskeysqueries
Softmaxrow-wise normalization
Two reduce:
Costly!Rewrite Reduction: Softmax Forward
24Two thread synchronizations
Z=âˆ‘iexiâˆ’x*
Parameters (e.g. # of blocks, warps per block) are shape dependent for maximal speedup
{headskeysqueries
Softmaxrow-wise normalization
Z
x*
Max 
x
Add & SumExp 
y
Exp & Divide 
yi=exiâˆ‘exjâ€¦â€¦Row1Row2columnâ‰¤32â€¦Threads1â€¦320311â€¦0031
And Other Shapes â€¦You will implement Softmax in HW3!Threadsâ€¦Row1Row232<columnâ‰¤64â€¦32031132031103110311Code Example: Softmax Forward
25Parameters tuning by using templates
https://github.com/bytedance/lightseq/blob/master/lightseq/csrc/kernels/cuda/softmax_kernels.cuCode Example: Softmax Forward
26Parameters tuning by using templates
Then call with parameters in launch
https://github.com/bytedance/lightseq/blob/master/lightseq/csrc/kernels/cuda/softmax_kernels.cuTechnique 3: Mixed-precision Calculationâ€¢Modern GPU supports half-precision (FP16) or FP8 (on H100)
â€¢Beneï¬ts: 
â€¢lower memory for storing model and data ==> enlarge batch size
â€¢transfer data at a higher rate (with same bandwith) between GPU main memory and SMs
â€¢more FLOPs for FP16 (up to 8x more) compared to FP32.
27Use low-precision for all data?â€¢Forward / Backward could use FP16 or FP8
â€¢Gradient update in Optimizer (or trainer) needs FP32
â€¢Nvidia APEX library provides automatic mixed-precision calculation for many NN layers
â€¢but still miss ï¬ne-grained memory optimization with mixed-precision for LLM.
281632â†’copy
1632â†copy
gâ€² 1gâ€² 2gâ€² 3
pâ€² 1pâ€² 2pâ€² 3ModelTrainer
backward
& forward 
(FP16)
g1g2g3Gradients (FP16)
p1p2p3Parameters (FP16)
Update
(FP32)Original
Modelbackward
& forward 
(FP16)
link
workspace
g1g2g3Gradients (FP16)
FP16
update
linkworkspace
p1p2p3Parameters (FP16)
Trainer
LightSeq2Calculation Precision: FP32
Storage Precision: FP16
Accelerated Mixed-Precision Update
29Continuous space. Only one kernel launch
Dotted lines: 
no actual memory storageâˆ‡out
âˆ‡Y=âˆ‡Dropout(âˆ‡out)
âˆ‡Z=âˆ‡Yğ–³Wout
âˆ‡Y=Reshape(âˆ‡Z)
âˆ‡S=âˆ‡YVğ–³,âˆ‡V=Sğ–³âˆ‡Y
âˆ‡Yâˆ‡inâˆ‡Qâˆ‡Qâˆ‡Kâˆ‡Kâˆ‡Sâˆ‡Sâˆ‡Sâˆ‡ËœKâˆ‡ËœQâˆ‡ËœQâˆ‡ËœVâˆ‡ËœV
âˆ‡V
âˆ‡ËœK
âˆ‡S=âˆ‡Dropout(âˆ‡S)âˆ‡S=âˆ‡Softmax(âˆ‡S)âˆ‡K=Qğ–³âˆ‡S,âˆ‡Q=âˆ‡Sğ–³Kâˆ‡ËœQ,âˆ‡ËœK,âˆ‡ËœV=Reshape(âˆ‡Q,âˆ‡K,âˆ‡V)âˆ‡Y=âˆ‡ËœQğ–³WQ+âˆ‡ËœKğ–³WK+âˆ‡ËœVğ–³WVâˆ‡in=âˆ‡LayerNorm(âˆ‡Y)+âˆ‡outâˆ‡in
Technique 4: Memory Reuse GPU Memory Management for Self Attention Backward
30B:batchsizeL:sequencelengthH:hiddenunitsBÃ—LÃ—H
âˆ‡Y
âˆ‡out
âˆ‡Yâˆ‡Z
BÃ—LÃ—H,âˆ‡out:notusedagain,reuseitsmemory
âˆ‡Z
âˆ‡Y
âˆ‡Yâˆ‡Vâˆ‡S
attentions:BÃ—L2Ã—H
}Bottleneck
BLHmax{BL2N,3BLH}BLHBLHLeast Memory Allocation
BÃ—LÃ—H,reusememoryforâˆ‡Y
Machine Translation Training: 1.4-3.5x Speedup
31DataSetWMT14 English-German Machine TranslationModelTransformer: 24 encoder layers + 24 decoder layersHardware1 Worker with 8x A100BaselineFairSeq (PyTorch) + Apex (optimized operators)
Encoder
Decoder
You and me are like the devil and holy water.Number of people serving as mentorsFor the Player who Refuses to be Played
Du und ich, wir sind wie der Teufel und das Weihwasser.Die Anzahl von Personen, die als Mentoren dienenFÃ¼r den Spieler, der sich weigert ausgespielt zu werden.
Batched Samples
as trainer % 
Experiment with different batch sizes
Transformer ModelMachine Translation Training: 1.4-3.5x Speedup
32
V100: 1.4-2.8xA100: 1.5-3.5x
A100 is more eï¬ƒcient in GEMMModel Size                   Speedup       
33Visualization of Training on GPU
Fairseq: 457ms
LightSeq: 214ms
encoder fw
decoder fw
decoder bw
encoder bw
optimize
output projection 
Training Speedup Breakdown
34Time cost for each training stagesOperatorSpeedupLayerNorm4xSoftmax2.5-3.4xDropout1.1-2.5xTrainer2.3xOperator SpeedupTask: WMT14 Engish German Machine Translation (same for rest pages)
FairseqLightseq2Scalability: 1.12-1.41x Speedup
35Speedup on 1 to 5 workers, each has 8x A100
Speedup of Models with different layersTraining Memory Cost: 6G less
36Transformer BaseTransformer LargeGPT2 Training: 1.6-1.9x Speedup
37DataSetWikiTextModelGPT2Hardware1 Worker with 8x V100/A100BaselineHugging Face (PyTorch) GPT2 Large trained on A100, 1.6-1.9x Speedup
GPT2 Large trained on V100, 1.7-1.8x Speedup
Wouldyoupleasecomeyoupleasecomehere
Decoder
Paraphrase Identification: 1.28-1.44x Speedup
38DataSetMicrosoft Research Paraphrase CorpusModelBERTHardware1 Worker with 8x V100BaselineHugging Face (PyTorch) 
DeepSpeed (Kernel Fusion)
EncoderDifferentEquivalent
For those who got surgery alone, median survival was 41 months.
Those who only had surgery lived an average of 46 months.
1.28x
}}1.44x
LibraryCriterionEmbeddingTrainerâœ˜âœ˜âœ˜âœ”âœ”âœ”
2LightSeq2 vs DeepSpeed Major Differences Image Classification: Vision Transformer (ViT)
39
Vision Transformer for Image classiï¬cation from google AI blogImage Classification: 1.2-1.7x Speedup
40DataSetCIFAR-10ModelVision Transformer (ViT)Hardware1 Worker with 8x V100BaselineHugging Face (PyTorch)
Encoder
BirdCarBallBuildingPatchesVit BaseVit LargeBatch Size (#Patches)               Speedup
Summary for Accelerating Transformer Training
41
Eï¬ƒcient Parameter Update
Memory Management
High performance Kernels for Forward and Backward
âˆ‡x
âˆ‡y
Dropout
g
p
fp32+
âˆ‡z
âˆ‡y
Ã—
Layer Norm
âˆ‡x
Reuse Memory
Kernel Fusion: merge kernels other than matmul
Algebraic Transformation: reduce sync
Mix-precision calculation (use half precision whenever possible)
Memory reuse (dependent on architecture)key techniquesFast Inference for TransformerNAACL 2021LightSeq: A High Performance Inference Library for Transformers Xiaohui Wang, Ying Xiong, Yang Wei, Mingxuan Wang, Lei LiTurboTransformers: An Efï¬cient GPU Serving System For Transformer Models Jiarui Fang, Yang Yu, Chengduo Zhao, Jie ZhouPPoPP 2021Inference: Beam Searchargmax_y P(Y|X)
1.start with empty S
2.at each step, keep k best partial sequences
3.expand them with one more forward generation
4.collect new partial results and keep top-k
43Code Example# 1.compute next token log probabilitylog_token_prob = tf.nn.log_softmax(logit) # [batch_size, beam_size, vocab_size]log_seq_prob += log_token_prob # [batch_size, beam_size, vocab_size]log_seq_prob = tf.reshape(log_seq_prob, [-1, beam_size * vocab_size])# 2. compute the top k sequence probability for each batch sequencetopk_log_probs, topk_indices = tf.nn.top_k(log_seq_prob, k=K)# 3. refresh the cache (decoder key and values) based on beam idrefresh_cache(cache, topk_indices)
44Hierarchical Auto Regressive Search (HARS) for decodingâ€¢Two calculations are needed in one step of beam search:
â€¢Compute the conditional probability of each token in vocab using Softmax
â€¢Select the top- beams by sequential probability.
â€¢need sorting k*V elements!
â€¢Key Idea for acceleration: HARS for decoding
â€¢retrieve and re-rank to reduce complexityk
45HARS - Retrieve stepâ€¢Divide logits into  groups.
â€¢Calculate the maximum of group , denoted as , marked red
â€¢Calculate the minimum of  in each beam, denoted as rough top-th logit .
â€¢Select logits larger than  and write them into GPU memory.kimimikâ„›â„›
46
HARS - Re-rank stepâ€¢Re-rank (sorting) on candidate logits
47
HARS Decoding Exampleâ€¢Original logits, with Beam size = 2 and Vocab size = 8.
482143274431551826HARS Decoding Exampleâ€¢For each beam, divide the eight logits into two groups.
492143274431551826HARS Decoding Exampleâ€¢Calculate the maximum of each group.
502143274431551826HARS Decoding Exampleâ€¢For each beam, calculate the minimum of each groupâ€™s maximum.
512143274431551826HARS Decoding Exampleâ€¢For each beam, select logits larger than the minimum in previous step.
522143274431551826HARS Decoding Exampleâ€¢For each beam, select logits larger than the minimum in previous step.
5347458HARS Decoding Exampleâ€¢Re-rank only on ï¬ve logits
5444578Details in GPU Inference Implementationâ€¢share tensor memory across layers
â€¢mixed precision computation, mostly using FP16 for computation
â€¢Using float4 and half2 to increase bandwidth
â€¢No need to keep intermediate results and gradients during infernece, similar to with torch.no_grad()
55Python API
C++ Operator
CUDA Kernel
LightSeq2 KernelLightSeq Software Architecture
56
Softmax FW
Softmax BW
Dropout BW
â€¦
cuBLAS
CUB
Encoder
Decoder
Softmax
Embedding
Trainer
â€¦
Python API
Encoder
Decoder
Softmax
Embedding
Trainer
â€¦
LightSeq Model Zoo
Transformer
BERT
ViT
GPT2
â€¦
Model Translator
â€¦
Internal APIs
3rd Party
Public APIs
API Example: HuggingFace BERT
57from lightseq.training import LSTransformerEncoderLayer config = LSTransformerEncoderLayer.get_config(             model="bert-base",             max_batch_tokens=4096,             max_seq_len=512,             fp16=True,             local_rank=0) ls_layer = LSTransformerEncoderLayer(config) # replace the 1st Hugging Face layer with LightSeq2 bert_model.layer[0] = ls_layerStep 1: import LightSeq
Step 2: Conï¬g and deï¬ne your model/layer
Step 3: Replace HuggingFace Layer58LightSeq + Fairseq Integrationlightseq-train DATA_SET \     --task translation \     --arch ls_transformer_wmt_en_de_big_t2t \     --optimizer ls_adam \     --criterion ls_label_smoothed_cross_entropy \     --OTHER_PARAMS
â€¢LightSeq can be seamlessly used with Fairseq â€¢Training: lightseq-trainï¼Œusing prefix ls_59LightSeq + Fairseq Integrationâ€¢LightSeq accelerated Transformer embedding / encoder / decoderã€Adam and cross entropy for Fairseq â€¢LightSeq is compatible with Fairseq cache and reorder â€¢LightSeq is compatible with Apex and DeepSpeed together with Fairseqã€‚deepspeed ds_fairseq.py DATA_SET \     â€”-user-dir fs_modules \     --deepspeed_config deepspeed_config.json \     --task translation \     --arch ls_transformer_wmt_en_de_big_t2t \     --optimizer ls_adam \     --criterion ls_label_smoothed_cross_entropy \     --OTHER_PARAMS
GPU Occupationâ€¢LightSeq greatly reduces the proportion of kernels other than GEMM.
60Machine Translation Inference: 14x speedupâ€¢LightSeq outperforms others in most cases, especially in large batch size.
61GPT2 Inference: 6x speedupâ€¢LightSeq outperforms others in most cases
62Summary
63We optimize the training process from 3 aspects
Eï¬ƒcient Parameter Update
via mixed precision
Memory Management
High performance Kernels for Forward and Backward
via Fusion & Algebra trick
âˆ‡x
âˆ‡y
Dropout
g
p
fp32+
âˆ‡z
âˆ‡y
Ã—
Layer Norm
âˆ‡x
Reuse MemoryOperators that can be reused in Other Networks:
Dropout, LayerNorm, Softmax, Cross Entropy
hierachical auto-regressive  search for large vocabulary
converting sorting to parallel operations (max, ï¬lter, re-rank)Accelerating decodingOther Approaches for Acceleration
64
Alternative Model Structures: Linformer, Reformer
Training Strategy: Shallow to Deep, Layer Dropout
Eï¬ƒcient Computation: LAMB, Quantization, Hardware Optimization
65
code is available at 
https://github.com/bytedance/lightseqStay tuned for Deepseekâ€™s FlashMLAâ€¢High-performance decoding kernel optimized for Multi-head Latent Attention (MLA) on Hopper GPUs
â€¢https://github.com/deepseek-ai/FlashMLA
66Reading for Nextâ€¢PyTorch Distributed: Experiences on Accelerating Data Parallel Training
â€¢PyTorch FSDP: Experiences on Scaling Fully Sharded Data Parallel
67