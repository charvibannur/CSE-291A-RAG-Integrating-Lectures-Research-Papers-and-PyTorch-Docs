Published as a conference paper at ICLR 2021
SCORE -BASED GENERATIVE MODELING THROUGH
STOCHASTIC DIFFERENTIAL EQUATIONS
Yang Song
Stanford University
yangsong@cs.stanford.eduJascha Sohl-Dickstein
Google Brain
jaschasd@google.comDiederik P. Kingma
Google Brain
durk@google.com
Abhishek Kumar
Google Brain
abhishk@google.comStefano Ermon
Stanford University
ermon@cs.stanford.eduBen Poole
Google Brain
pooleb@google.com
ABSTRACT
Creating noise from data is easy; creating data from noise is generative modeling.
We present a stochastic differential equation (SDE) that smoothly transforms a com-
plex data distribution to a known prior distribution by slowly injecting noise, and a
corresponding reverse-time SDE that transforms the prior distribution back into the
data distribution by slowly removing the noise. Crucially, the reverse-time SDE
depends only on the time-dependent gradient Ô¨Åeld (a.k.a., score) of the perturbed
data distribution. By leveraging advances in score-based generative modeling, we
can accurately estimate these scores with neural networks, and use numerical SDE
solvers to generate samples. We show that this framework encapsulates previous
approaches in score-based generative modeling and diffusion probabilistic mod-
eling, allowing for new sampling procedures and new modeling capabilities. In
particular, we introduce a predictor-corrector framework to correct errors in the
evolution of the discretized reverse-time SDE. We also derive an equivalent neural
ODE that samples from the same distribution as the SDE, but additionally enables
exact likelihood computation, and improved sampling efÔ¨Åciency. In addition, we
provide a new way to solve inverse problems with score-based models, as demon-
strated with experiments on class-conditional generation, image inpainting, and
colorization. Combined with multiple architectural improvements, we achieve
record-breaking performance for unconditional image generation on CIFAR-10
with an Inception score of 9.89 and FID of 2.20, a competitive likelihood of 2.99
bits/dim, and demonstrate high Ô¨Ådelity generation of 10241024 images for the
Ô¨Årst time from a score-based generative model.
1 I NTRODUCTION
Two successful classes of probabilistic generative models involve sequentially corrupting training
data with slowly increasing noise, and then learning to reverse this corruption in order to form a
generative model of the data. Score matching with Langevin dynamics (SMLD) (Song & Ermon,
2019) estimates the score (i.e., the gradient of the log probability density with respect to data) at each
noise scale, and then uses Langevin dynamics to sample from a sequence of decreasing noise scales
during generation. Denoising diffusion probabilistic modeling (DDPM) (Sohl-Dickstein et al., 2015;
Ho et al., 2020) trains a sequence of probabilistic models to reverse each step of the noise corruption,
using knowledge of the functional form of the reverse distributions to make training tractable. For
continuous state spaces, the DDPM training objective implicitly computes scores at each noise scale.
We therefore refer to these two model classes together as score-based generative models .
Score-based generative models, and related techniques (Bordes et al., 2017; Goyal et al., 2017; Du &
Mordatch, 2019), have proven effective at generation of images (Song & Ermon, 2019; 2020; Ho
et al., 2020), audio (Chen et al., 2020; Kong et al., 2020), graphs (Niu et al., 2020), and shapes (Cai
Work partially done during an internship at Google Brain.
1arXiv:2011.13456v2  [cs.LG]  10 Feb 2021Published as a conference paper at ICLR 2021
¬†
¬†¬†Forward SDE (data¬† ‚Üí  noise)  
Reverse SDE (noise ‚Üí  data)  score function
Figure 1 :Solving a reverse-
time SDE yields a score-based
generative model. Transform-
ing data to a simple noise dis-
tribution can be accomplished
with a continuous-time SDE.
This SDE can be reversed if we
know the score of the distribu-
tion at each intermediate time
step,rxlogptpxq.
et al., 2020). To enable new sampling methods and further extend the capabilities of score-based
generative models, we propose a uniÔ¨Åed framework that generalizes previous approaches through the
lens of stochastic differential equations (SDEs).
SpeciÔ¨Åcally, instead of perturbing data with a Ô¨Ånite number of noise distributions, we consider a
continuum of distributions that evolve over time according to a diffusion process. This process
progressively diffuses a data point into random noise, and is given by a prescribed SDE that does not
depend on the data and has no trainable parameters. By reversing this process, we can smoothly mold
random noise into data for sample generation. Crucially, this reverse process satisÔ¨Åes a reverse-time
SDE (Anderson, 1982), which can be derived from the forward SDE given the score of the marginal
probability densities as a function of time. We can therefore approximate the reverse-time SDE by
training a time-dependent neural network to estimate the scores, and then produce samples using
numerical SDE solvers. Our key idea is summarized in Fig. 1.
Our proposed framework has several theoretical and practical contributions:
Flexible sampling and likelihood computation: We can employ any general-purpose SDE solver
to integrate the reverse-time SDE for sampling. In addition, we propose two special methods not
viable for general SDEs: (i) Predictor-Corrector (PC) samplers that combine numerical SDE solvers
with score-based MCMC approaches, such as Langevin MCMC (Parisi, 1981) and HMC (Neal et al.,
2011); and (ii) deterministic samplers based on the probability Ô¨Çow ordinary differential equation
(ODE). The former uniÔ¨Åes and improves over existing sampling methods for score-based models.
The latter allows for fast adaptive sampling via black-box ODE solvers, Ô¨Çexible data manipulation
via latent codes, a uniquely identiÔ¨Åable encoding , and notably, exact likelihood computation .
Controllable generation: We can modulate the generation process by conditioning on information
not available during training, because the conditional reverse-time SDE can be efÔ¨Åciently estimated
from unconditional scores. This enables applications such as class-conditional generation, image
inpainting, colorization and other inverse problems, all achievable using a single unconditional
score-based model without re-training.
UniÔ¨Åed framework: Our framework provides a uniÔ¨Åed way to explore and tune various SDEs for
improving score-based generative models. The methods of SMLD and DDPM can be amalgamated
into our framework as discretizations of two separate SDEs. Although DDPM (Ho et al., 2020) was
recently reported to achieve higher sample quality than SMLD (Song & Ermon, 2019; 2020), we show
that with better architectures and new sampling algorithms allowed by our framework, the latter can
catch up‚Äîit achieves new state-of-the-art Inception score (9.89) and FID score (2.20) on CIFAR-10,
as well as high-Ô¨Ådelity generation of 10241024 images for the Ô¨Årst time from a score-based model.
In addition, we propose a new SDE under our framework that achieves a likelihood value of 2.99
bits/dim on uniformly dequantized CIFAR-10 images, setting a new record on this task.
2 B ACKGROUND
2.1 D ENOISING SCORE MATCHING WITH LANGEVIN DYNAMICS (SMLD)
Letpp~x|xq:Np~x;x;2Iqbe a perturbation kernel, and pp~xq:¬≥
pdatapxqpp~x|xqdx, where
pdatapxqdenotes the data distribution. Consider a sequence of positive noise scales min1¬†
2¬†¬†Nmax. Typically,minis small enough such that pminpxqpdatapxq, andmaxis
2Published as a conference paper at ICLR 2021
large enough such that pmaxpxqNpx;0;2
maxIq. Song & Ermon (2019) propose to train a Noise
Conditional Score Network (NCSN), denoted by spx;q, with a weighted sum of denoising score
matching (Vincent, 2011) objectives:
arg min
N¬∏
i12
iEpdatapxqEpip~x|xq
ksp~x;iqr~xlogpip~x|xqk2
2
: (1)
Given sufÔ¨Åcient data and model capacity, the optimal score-based model spx;qmatches
rxlogppxqalmost everywhere for PtiuN
i1. For sampling, Song & Ermon (2019) run Msteps
of Langevin MCMC to get a sample for each pipxqsequentially:
xm
ixm1
i ispxm1
i;iq ?
2izm
i; m1;2;;M; (2)
wherei¬°0is the step size, and zm
iis standard normal. The above is repeated for iN;N
1;;1in turn with x0
NNpx|0;2
maxIqandx0
ixM
i 1wheni¬†N. AsM√ë8 andi√ë0
for alli,xM
1becomes an exact sample from pminpxqpdatapxqunder some regularity conditions.
2.2 D ENOISING DIFFUSION PROBABILISTIC MODELS (DDPM)
Sohl-Dickstein et al. (2015); Ho et al. (2020) consider a sequence of positive noise scales
0¬†1;2;;N¬†1. For each training data point x0pdatapxq, a discrete Markov chain
tx0;x1;;xNuis constructed such that ppxi|xi1qNpxi;?1ixi1;iIq, and therefore
pipxi|x0qNpxi;?ix0;p1iqIq, wherei:¬±i
j1p1jq. Similar to SMLD, we can
denote the perturbed data distribution as pip~xq:¬≥
pdatapxqpip~x|xqdx. The noise scales are pre-
scribed such that xNis approximately distributed according to Np0;Iq. A variational Markov chain
in the reverse direction is parameterized with ppxi1|xiqNpxi1;1?1ipxi ispxi;iqq;iIq,
and trained with a re-weighted variant of the evidence lower bound (ELBO):
arg min
N¬∏
i1p1iqEpdatapxqEpip~x|xqrksp~x;iqr~xlogpip~x|xqk2
2s: (3)
After solving Eq. (3) to get the optimal model spx;iq, samples can be generated by starting from
xNNp0;Iqand following the estimated reverse Markov chain as below
xi11?1ipxi ispxi;iqq a
izi; iN;N1;;1: (4)
We call this method ancestral sampling , since it amounts to performing ancestral sampling from
the graphical model¬±N
i1ppxi1|xiq. The objective Eq. (3) described here is Lsimple in Ho et al.
(2020), written in a form to expose more similarity to Eq. (1). Like Eq. (1), Eq. (3) is also a weighted
sum of denoising score matching objectives, which implies that the optimal model, sp~x;iq, matches
the score of the perturbed data distribution, rxlogpipxq. Notably, the weights of the i-th summand
in Eq. (1) and Eq. (3), namely 2
iandp1iq, are related to corresponding perturbation kernels in the
same functional form: 2
i91{Erkrxlogpip~x|xqk2
2sandp1iq91{Erkrxlogpip~x|xqk2
2s.
3 S CORE -BASED GENERATIVE MODELING WITH SDE S
Perturbing data with multiple noise scales is key to the success of previous methods. We propose to
generalize this idea further to an inÔ¨Ånite number of noise scales, such that perturbed data distributions
evolve according to an SDE as the noise intensiÔ¨Åes. An overview of our framework is given in Fig. 2.
3.1 P ERTURBING DATA WITH SDE S
Our goal is to construct a diffusion process txptquT
t0indexed by a continuous time variable tPr0;Ts,
such that xp0qp0, for which we have a dataset of i.i.d. samples, and xpTqpT, for which we
have a tractable form to generate samples efÔ¨Åciently. In other words, p0is the data distribution and
pTis the prior distribution. This diffusion process can be modeled as the solution to an It ÀÜo SDE:
dxfpx;tqdt gptqdw; (5)
3Published as a conference paper at ICLR 2021
¬†
¬†¬†Forward SDE Data Prior Data Reverse SDE
¬† ¬†
Figure 2: Overview of score-based generative modeling through SDEs . We can map data to a
noise distribution (the prior) with an SDE (Section 3.1), and reverse this SDE for generative modeling
(Section 3.2). We can also reverse the associated probability Ô¨Çow ODE (Section 4.3), which yields a
deterministic process that samples from the same distribution as the SDE. Both the reverse-time SDE
and probability Ô¨Çow ODE can be obtained by estimating the score rxlogptpxq(Section 3.3).
where wis the standard Wiener process (a.k.a., Brownian motion), fp;tq:Rd√ëRdis a vector-
valued function called the drift coefÔ¨Åcient of xptq, andgpq:R√ëRis a scalar function known as
thediffusion coefÔ¨Åcient of xptq. For ease of presentation we assume the diffusion coefÔ¨Åcient is a
scalar (instead of a ddmatrix) and does not depend on x, but our theory can be generalized to hold
in those cases (see Appendix A). The SDE has a unique strong solution as long as the coefÔ¨Åcients
are globally Lipschitz in both state and time (√òksendal, 2003). We hereafter denote by ptpxqthe
probability density of xptq, and usepstpxptq|xpsqqto denote the transition kernel from xpsqtoxptq,
where 0¬§s¬†t¬§T.
Typically,pTis an unstructured prior distribution that contains no information of p0, such as a
Gaussian distribution with Ô¨Åxed mean and variance. There are various ways of designing the SDE in
Eq. (5) such that it diffuses the data distribution into a Ô¨Åxed prior distribution. We provide several
examples later in Section 3.4 that are derived from continuous generalizations of SMLD and DDPM.
3.2 G ENERATING SAMPLES BY REVERSING THE SDE
By starting from samples of xpTqpTand reversing the process, we can obtain samples xp0qp0.
A remarkable result from Anderson (1982) states that the reverse of a diffusion process is also a
diffusion process, running backwards in time and given by the reverse-time SDE:
dxrfpx;tqgptq2rxlogptpxqsdt gptqdw; (6)
where wis a standard Wiener process when time Ô¨Çows backwards from Tto0, and dtis an
inÔ¨Ånitesimal negative timestep. Once the score of each marginal distribution, rxlogptpxq, is known
for allt, we can derive the reverse diffusion process from Eq. (6) and simulate it to sample from p0.
3.3 E STIMATING SCORES FOR THE SDE
The score of a distribution can be estimated by training a score-based model on samples with
score matching (Hyv ¬®arinen, 2005; Song et al., 2019a). To estimate rxlogptpxq, we can train a
time-dependent score-based model spx;tqvia a continuous generalization to Eqs. (1) and (3):
arg min
Et!
ptqExp0qExptq|xp0qspxptq;tqrxptqlogp0tpxptq|xp0qq2
2)
:(7)
Here:r0;Ts √ëR¬°0is a positive weighting function, tis uniformly sampled over r0;Ts,
xp0qp0pxqandxptqp0tpxptq|xp0qq. With sufÔ¨Åcient data and model capacity, score matching
ensures that the optimal solution to Eq. (7), denoted by spx;tq, equals rxlogptpxqfor almost all
xandt. As in SMLD and DDPM, we can typically choose 91{Erxptqlogp0tpxptq|xp0qq2
2
.
Note that Eq. (7) uses denoising score matching, but other score matching objectives, such as sliced
4Published as a conference paper at ICLR 2021
score matching (Song et al., 2019a) and Ô¨Ånite-difference score matching (Pang et al., 2020) are also
applicable here.
We typically need to know the transition kernel p0tpxptq|xp0qqto efÔ¨Åciently solve Eq. (7). When
fp;tqis afÔ¨Åne, the transition kernel is always a Gaussian distribution, where the mean and variance are
often known in closed-forms and can be obtained with standard techniques (see Section 5.5 in S ¬®arkk¬®a
& Solin (2019)). For more general SDEs, we may solve Kolmogorov‚Äôs forward equation (√òksendal,
2003) to obtain p0tpxptq|xp0qq. Alternatively, we can simulate the SDE to sample from p0tpxptq|
xp0qqand replace denoising score matching in Eq. (7) with sliced score matching for model training,
which bypasses the computation of rxptqlogp0tpxptq|xp0qq(see Appendix A).
3.4 E XAMPLES : VE, VP SDE S AND BEYOND
The noise perturbations used in SMLD and DDPM can be regarded as discretizations of two different
SDEs. Below we provide a brief discussion and relegate more details to Appendix B.
When using a total of Nnoise scales, each perturbation kernel pipx|x0qof SMLD corresponds to
the distribution of xiin the following Markov chain:
xixi1 b
2
i2
i1zi1; i1;;N; (8)
where zi1Np0;Iq, and we have introduced 00to simplify the notation. In the limit of
N√ë8 ,tiuN
i1becomes a function ptq,zibecomes zptq, and the Markov chain txiuN
i1becomes
a continuous stochastic process txptqu1
t0, where we have used a continuous time variable tPr0;1s
for indexing, rather than an integer i. The processtxptqu1
t0is given by the following SDE
dxc
dr2ptqs
dtdw: (9)
Likewise for the perturbation kernels tpipx|x0quN
i1of DDPM, the discrete Markov chain is
xia
1ixi1 a
izi1; i1;;N: (10)
AsN√ë8 , Eq. (10) converges to the following SDE,
dx1
2ptqxdt a
ptqdw: (11)
Therefore, the noise perturbations used in SMLD and DDPM correspond to discretizations of SDEs
Eqs. (9) and (11). Interestingly, the SDE of Eq. (9) always gives a process with exploding variance
whent√ë8 , whilst the SDE of Eq. (11) yields a process with a Ô¨Åxed variance of one when the initial
distribution has unit variance (proof in Appendix B). Due to this difference, we hereafter refer to
Eq. (9) as the Variance Exploding (VE) SDE, and Eq. (11) the Variance Preserving (VP) SDE.
Inspired by the VP SDE, we propose a new type of SDEs which perform particularly well on
likelihoods (see Section 4.3), given by
dx1
2ptqxdt b
ptqp1e2¬≥t
0psqdsqdw: (12)
When using the same ptqand starting from the same initial distribution, the variance of the stochastic
process induced by Eq. (12) is always bounded by the VP SDE at every intermediate time step (proof
in Appendix B). For this reason, we name Eq. (12) the sub-VP SDE.
Since VE, VP and sub-VP SDEs all have afÔ¨Åne drift coefÔ¨Åcients, their perturbation kernels p0tpxptq|
xp0qqare all Gaussian and can be computed in closed-forms, as discussed in Section 3.3. This makes
training with Eq. (7) particularly efÔ¨Åcient.
4 S OLVING THE REVERSE SDE
After training a time-dependent score-based model s, we can use it to construct the reverse-time
SDE and then simulate it with numerical approaches to generate samples from p0.
5Published as a conference paper at ICLR 2021
Table 1: Comparing different reverse-time SDE solvers on CIFAR-10. Shaded regions are obtained
with the same computation (number of score function evaluations). Mean and standard deviation
are reported over Ô¨Åve sampling runs. ‚ÄúP1000‚Äù or ‚ÄúP2000‚Äù: predictor-only samplers using 1000 or
2000 steps. ‚ÄúC2000‚Äù: corrector-only samplers using 2000 steps. ‚ÄúPC1000‚Äù: Predictor-Corrector (PC)
samplers using 1000 predictor and 1000 corrector steps.
Variance Exploding SDE (SMLD) Variance Preserving SDE (DDPM)
PredictorFID√ì Sampler
P1000 P2000 C2000 PC1000 P1000 P2000 C2000 PC1000
ancestral sampling 4.98.06 4.88.06 3.62.03 3.24.02 3.24.02 3.21.02
reverse diffusion 4.79.07 4.74.08 3.60.02 3.21.02 3.19.02 3.18.01
probability Ô¨Çow 15.41.15 10.54.0820.43.07
3.51.04 3.59.04 3.23.0319.06.06
3.06.03
4.1 G ENERAL -PURPOSE NUMERICAL SDE SOLVERS
Numerical solvers provide approximate trajectories from SDEs. Many general-purpose numerical
methods exist for solving SDEs, such as Euler-Maruyama and stochastic Runge-Kutta methods (Kloe-
den & Platen, 2013), which correspond to different discretizations of the stochastic dynamics. We
can apply any of them to the reverse-time SDE for sample generation.
Ancestral sampling, the sampling method of DDPM (Eq. (4)), actually corresponds to one special
discretization of the reverse-time VP SDE (Eq. (11)) (see Appendix E). Deriving the ancestral
sampling rules for new SDEs, however, can be non-trivial. To remedy this, we propose reverse
diffusion samplers (details in Appendix E), which discretize the reverse-time SDE in the same way
as the forward one, and thus can be readily derived given the forward discretization. As shown in
Table 1, reverse diffusion samplers perform slightly better than ancestral sampling for both SMLD and
DDPM models on CIFAR-10 (DDPM-type ancestral sampling is also applicable to SMLD models,
see Appendix F.)
4.2 P REDICTOR -CORRECTOR SAMPLERS
Unlike generic SDEs, we have additional information that can be used to improve solutions. Since we
have a score-based model spx;tqrxlogptpxq, we can employ score-based MCMC approaches,
such as Langevin MCMC (Parisi, 1981; Grenander & Miller, 1994) or HMC (Neal et al., 2011) to
sample from ptdirectly, and correct the solution of a numerical SDE solver.
SpeciÔ¨Åcally, at each time step, the numerical SDE solver Ô¨Årst gives an estimate of the sample
at the next time step, playing the role of a ‚Äúpredictor‚Äù. Then, the score-based MCMC approach
corrects the marginal distribution of the estimated sample, playing the role of a ‚Äúcorrector‚Äù. The
idea is analogous to Predictor-Corrector methods, a family of numerical continuation techniques for
solving systems of equations (Allgower & Georg, 2012), and we similarly name our hybrid sampling
algorithms Predictor-Corrector (PC) samplers. Please Ô¨Ånd pseudo-code and a complete description
in Appendix G. PC samplers generalize the original sampling methods of SMLD and DDPM: the
former uses an identity function as the predictor and annealed Langevin dynamics as the corrector,
while the latter uses ancestral sampling as the predictor and identity as the corrector.
We test PC samplers on SMLD and DDPM models (see Algorithms 2 and 3 in Appendix G) trained
with original discrete objectives given by Eqs. (1) and (3). This exhibits the compatibility of PC
samplers to score-based models trained with a Ô¨Åxed number of noise scales. We summarize the
performance of different samplers in Table 1, where probability Ô¨Çow is a predictor to be discussed
in Section 4.3. Detailed experimental settings and additional results are given in Appendix G. We
observe that our reverse diffusion sampler always outperform ancestral sampling, and corrector-only
methods (C2000) perform worse than other competitors (P2000, PC1000) with the same computation
(In fact, we need way more corrector steps per noise scale, and thus more computation, to match the
performance of other samplers.) For all predictors, adding one corrector step for each predictor step
(PC1000) doubles computation but always improves sample quality (against P1000). Moreover, it
is typically better than doubling the number of predictor steps without adding a corrector (P2000),
where we have to interpolate between noise scales in an ad hoc manner (detailed in Appendix G) for
SMLD/DDPM models. In Fig. 9 (Appendix G), we additionally provide qualitative comparison for
6Published as a conference paper at ICLR 2021
Table 2: NLLs and FIDs (ODE) on CIFAR-10.
Model NLL Test √ìFID√ì
RealNVP (Dinh et al., 2016) 3.49 -
iResNet (Behrmann et al., 2019) 3.45 -
Glow (Kingma & Dhariwal, 2018) 3.35 -
MintNet (Song et al., 2019b) 3.32 -
Residual Flow (Chen et al., 2019) 3.28 46.37
FFJORD (Grathwohl et al., 2018) 3.40 -
Flow++ (Ho et al., 2019) 3.29 -
DDPM (L) (Ho et al., 2020) ¬§3.70*13.51
DDPM (Lsimple) (Ho et al., 2020) ¬§3.75*3.17
DDPM 3.28 3.37
DDPM cont. (VP) 3.21 3.69
DDPM cont. (sub-VP) 3.05 3.56
DDPM++ cont. (VP) 3.16 3.93
DDPM++ cont. (sub-VP) 3.02 3.16
DDPM++ cont. (deep, VP) 3.13 3.08
DDPM++ cont. (deep, sub-VP) 2.99 2.92Table 3: CIFAR-10 sample quality.
Model FID √ì IS√í
Conditional
BigGAN (Brock et al., 2018) 14.73 9.22
StyleGAN2-ADA (Karras et al., 2020a) 2.42 10.14
Unconditional
StyleGAN2-ADA (Karras et al., 2020a) 2.92 9.83
NCSN (Song & Ermon, 2019) 25.32 8.87 .12
NCSNv2 (Song & Ermon, 2020) 10.87 8.40 .07
DDPM (Ho et al., 2020) 3.17 9.46 .11
DDPM++ 2.78 9.64
DDPM++ cont. (VP) 2.55 9.58
DDPM++ cont. (sub-VP) 2.61 9.56
DDPM++ cont. (deep, VP) 2.41 9.68
DDPM++ cont. (deep, sub-VP) 2.41 9.57
NCSN++ 2.45 9.73
NCSN++ cont. (VE) 2.38 9.83
NCSN++ cont. (deep, VE) 2.20 9.89
models trained with the continuous objective Eq. (7) on 256256LSUN images and the VE SDE,
where PC samplers clearly surpass predictor-only samplers under comparable computation, when
using a proper number of corrector steps.
4.3 P ROBABILITY FLOW AND CONNECTION TO NEURAL ODE S
Score-based models enable another numerical method for solving the reverse-time SDE. For all
diffusion processes, there exists a corresponding deterministic process whose trajectories share the
same marginal probability densities tptpxquT
t0as the SDE. This deterministic process satisÔ¨Åes an
ODE (more details in Appendix D.1):
dx
fpx;tq1
2gptq2rxlogptpxq
dt; (13)
which can be determined from the SDE once scores are known. We name the ODE in Eq. (13) the
probability Ô¨Çow ODE . When the score function is approximated by the time-dependent score-based
model, which is typically a neural network, this is an example of a neural ODE (Chen et al., 2018).
Exact likelihood computation Leveraging the connection to neural ODEs, we can compute the
density deÔ¨Åned by Eq. (13) via the instantaneous change of variables formula (Chen et al., 2018).
This allows us to compute the exact likelihood on any input data (details in Appendix D.2). As an
example, we report negative log-likelihoods (NLLs) measured in bits/dim on the CIFAR-10 dataset
in Table 2. We compute log-likelihoods on uniformly dequantized data, and only compare to models
evaluated in the same way (omitting models evaluated with variational dequantization (Ho et al.,
2019) or discrete data), except for DDPM ( L/Lsimple ) whose ELBO values (annotated with *) are
reported on discrete data. Main results: (i) For the same DDPM model in Ho et al. (2020), we obtain
better bits/dim than ELBO, since our likelihoods are exact; (ii) Using the same architecture, we
trained another DDPM model with the continuous objective in Eq. (7) ( i.e., DDPM cont.), which
further improves the likelihood; (iii) With sub-VP SDEs, we always get higher likelihoods compared
to VP SDEs; (iv) With improved architecture ( i.e., DDPM++ cont., details in Section 4.4) and the
sub-VP SDE, we can set a new record bits/dim of 2.99 on uniformly dequantized CIFAR-10 even
without maximum likelihood training .
Manipulating latent representations By integrating Eq. (13), we can encode any datapoint xp0q
into a latent space xpTq. Decoding can be achieved by integrating a corresponding ODE for the
reverse-time SDE. As is done with other invertible models such as neural ODEs and normalizing
Ô¨Çows (Dinh et al., 2016; Kingma & Dhariwal, 2018), we can manipulate this latent representation for
image editing, such as interpolation, and temperature scaling (see Fig. 3 and Appendix D.4).
Uniquely identiÔ¨Åable encoding Unlike most current invertible models, our encoding is uniquely
identiÔ¨Åable , meaning that with sufÔ¨Åcient training data, model capacity, and optimization accuracy,
the encoding for an input is uniquely determined by the data distribution (Roeder et al., 2020). This
is because our forward SDE, Eq. (5), has no trainable parameters, and its associated probability Ô¨Çow
7Published as a conference paper at ICLR 2021
100101102103
Evaluation number0.00.51.0Evaluation timepoint
ODE Evaluation Points
Precision
1e-1
1e-3
1e-5
NFE=14    NFE=86    NFE=548   
 Interpolation
Figure 3: Probability Ô¨Çow ODE enables fast sampling with adaptive stepsizes as the numerical
precision is varied ( left), and reduces the number of score function evaluations (NFE) without harming
quality ( middle ). The invertible mapping from latents to images allows for interpolations ( right ).
ODE, Eq. (13), provides the same trajectories given perfectly estimated scores. We provide additional
empirical veriÔ¨Åcation on this property in Appendix D.5.
EfÔ¨Åcient sampling As with neural ODEs, we can sample xp0q p0by solving Eq. (13) from
different Ô¨Ånal conditions xpTqpT. Using a Ô¨Åxed discretization strategy we can generate com-
petitive samples, especially when used in conjuction with correctors (Table 1, ‚Äúprobability Ô¨Çow
sampler‚Äù, details in Appendix D.3). Using a black-box ODE solver (Dormand & Prince, 1980) not
only produces high quality samples (Table 2, details in Appendix D.4), but also allows us to explicitly
trade-off accuracy for efÔ¨Åciency. With a larger error tolerance, the number of function evaluations
can be reduced by over 90% without affecting the visual quality of samples (Fig. 3).
4.4 A RCHITECTURE IMPROVEMENTS
We explore several new architecture designs for score-based models using both VE and VP SDEs
(details in Appendix H), where we train models with the same discrete objectives as in SMLD/DDPM.
We directly transfer the architectures for VP SDEs to sub-VP SDEs due to their similarity. Our
optimal architecture for the VE SDE, named NCSN++, achieves an FID of 2.45 on CIFAR-10 with
PC samplers, while our optimal architecture for the VP SDE, called DDPM++, achieves 2.78.
By switching to the continuous training objective in Eq. (7), and increasing the network depth, we can
further improve sample quality for all models. The resulting architectures are denoted as NCSN++
cont. and DDPM++ cont. in Table 3 for VE and VP/sub-VP SDEs respectively. Results reported in
Table 3 are for the checkpoint with the smallest FID over the course of training, where samples are
generated with PC samplers. In contrast, FID scores and NLL values in Table 2 are reported for the
last training checkpoint, and samples are obtained with black-box ODE solvers. As shown in Table 3,
VE SDEs typically provide better sample quality than VP/sub-VP SDEs, but we also empirically
observe that their likelihoods are worse than VP/sub-VP SDE counterparts. This indicates that
practitioners likely need to experiment with different SDEs for varying domains and architectures.
Our best model for sample quality, NCSN++ cont. (deep, VE), doubles the network depth and sets
new records for both inception score and FID on unconditional generation for CIFAR-10. Surprisingly,
we can achieve better FID than the previous best conditional generative model without requiring
labeled data. With all improvements together, we also obtain the Ô¨Årst set of high-Ô¨Ådelity samples
on CelebA-HQ 10241024 from score-based models (see Appendix H.3). Our best model for
likelihoods, DDPM++ cont. (deep, sub-VP), similarly doubles the network depth and achieves a
log-likelihood of 2.99 bits/dim with the continuous objective in Eq. (7). To our best knowledge, this
is the highest likelihood on uniformly dequantized CIFAR-10.
5 C ONTROLLABLE GENERATION
The continuous structure of our framework allows us to not only produce data samples from p0, but
also fromp0pxp0q|yqifptpy|xptqqis known. Given a forward SDE as in Eq. (5), we can sample
8Published as a conference paper at ICLR 2021
Figure 4: Left: Class-conditional samples on 3232CIFAR-10. Top four rows are automobiles and
bottom four rows are horses. Right: Inpainting (top two rows) and colorization (bottom two rows)
results on 256256LSUN. First column is the original image, second column is the masked/gray-
scale image, remaining columns are sampled image completions or colorizations.
fromptpxptq|yqby starting from pTpxpTq|yqand solving a conditional reverse-time SDE:
dxtfpx;tqgptq2rrxlogptpxq rxlogptpy|xqsudt gptqdw: (14)
In general, we can use Eq. (14) to solve a large family of inverse problems with score-based generative
models, once given an estimate of the gradient of the forward process, rxlogptpy|xptqq. In some
cases, it is possible to train a separate model to learn the forward process logptpy|xptqqand
compute its gradient. Otherwise, we may estimate the gradient with heuristics and domain knowledge.
In Appendix I.4, we provide a broadly applicable method for obtaining such an estimate without the
need of training auxiliary models.
We consider three applications of controllable generation with this approach: class-conditional
generation, image imputation and colorization. When yrepresents class labels, we can train a
time-dependent classiÔ¨Åer ptpy|xptqqfor class-conditional sampling. Since the forward SDE
is tractable, we can easily create training data pxptq;yqfor the time-dependent classiÔ¨Åer by Ô¨Årst
samplingpxp0q;yqfrom a dataset, and then sampling xptq p0tpxptq |xp0qq. Afterwards, we
may employ a mixture of cross-entropy losses over different time steps, like Eq. (7), to train the
time-dependent classiÔ¨Åer ptpy|xptqq. We provide class-conditional CIFAR-10 samples in Fig. 4
(left), and relegate more details and results to Appendix I.
Imputation is a special case of conditional sampling. Suppose we have an incomplete data point
ywhere only some subset, 
pyqis known. Imputation amounts to sampling from ppxp0q|
pyqq,
which we can accomplish using an unconditional model (see Appendix I.2). Colorization is a special
case of imputation, except that the known data dimensions are coupled. We can decouple these data
dimensions with an orthogonal linear transformation, and perform imputation in the transformed
space (details in Appendix I.3). Fig. 4 (right) shows results for inpainting and colorization achieved
with unconditional time-dependent score-based models.
6 C ONCLUSION
We presented a framework for score-based generative modeling based on SDEs. Our work enables a
better understanding of existing approaches, new sampling algorithms, exact likelihood computation,
uniquely identiÔ¨Åable encoding, latent code manipulation, and brings new conditional generation
abilities to the family of score-based generative models.
While our proposed sampling approaches improve results and enable more efÔ¨Åcient sampling, they
remain slower at sampling than GANs (Goodfellow et al., 2014) on the same datasets. Identifying
ways of combining the stable learning of score-based generative models with the fast sampling of
implicit models like GANs remains an important research direction. Additionally, the breadth of
samplers one can use when given access to score functions introduces a number of hyper-parameters.
Future work would beneÔ¨Åt from improved methods to automatically select and tune these hyper-
parameters, as well as more extensive investigation on the merits and limitations of various samplers.
9Published as a conference paper at ICLR 2021
ACKNOWLEDGEMENTS
We would like to thank Nanxin Chen, Ruiqi Gao, Jonathan Ho, Kevin Murphy, Tim Salimans and
Han Zhang for their insightful discussions during the course of this project. This research was
partially supported by NSF (#1651565, #1522054, #1733686), ONR (N000141912145), AFOSR
(FA95501910024), and TensorFlow Research Cloud. Yang Song was partially supported by the Apple
PhD Fellowship in AI/ML.
REFERENCES
Eugene L Allgower and Kurt Georg. Numerical continuation methods: an introduction , volume 13.
Springer Science & Business Media, 2012.
Brian D O Anderson. Reverse-time diffusion equation models. Stochastic Process. Appl. , 12(3):
313‚Äì326, May 1982.
Jens Behrmann, Will Grathwohl, Ricky TQ Chen, David Duvenaud, and J ¬®orn-Henrik Jacobsen.
Invertible residual networks. In International Conference on Machine Learning , pp. 573‚Äì582,
2019.
Florian Bordes, Sina Honari, and Pascal Vincent. Learning to generate samples from noise through
infusion training. arXiv preprint arXiv:1703.06975 , 2017.
Andrew Brock, Jeff Donahue, and Karen Simonyan. Large scale gan training for high Ô¨Ådelity natural
image synthesis. In International Conference on Learning Representations , 2018.
Ruojin Cai, Guandao Yang, Hadar Averbuch-Elor, Zekun Hao, Serge Belongie, Noah Snavely, and
Bharath Hariharan. Learning gradient Ô¨Åelds for shape generation. In Proceedings of the European
Conference on Computer Vision (ECCV) , 2020.
Nanxin Chen, Yu Zhang, Heiga Zen, Ron J Weiss, Mohammad Norouzi, and William Chan. Wavegrad:
Estimating gradients for waveform generation. arXiv preprint arXiv:2009.00713 , 2020.
Ricky TQ Chen, Yulia Rubanova, Jesse Bettencourt, and David K Duvenaud. Neural ordinary
differential equations. In Advances in neural information processing systems , pp. 6571‚Äì6583,
2018.
Ricky TQ Chen, Jens Behrmann, David K Duvenaud, and J ¬®orn-Henrik Jacobsen. Residual Ô¨Çows
for invertible generative modeling. In Advances in Neural Information Processing Systems , pp.
9916‚Äì9926, 2019.
Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using real nvp. arXiv
preprint arXiv:1605.08803 , 2016.
John R Dormand and Peter J Prince. A family of embedded runge-kutta formulae. Journal of
computational and applied mathematics , 6(1):19‚Äì26, 1980.
Yilun Du and Igor Mordatch. Implicit generation and modeling with energy based models. In
H. Wallach, H. Larochelle, A. Beygelzimer, F. d 'Alch ¬¥e-Buc, E. Fox, and R. Garnett (eds.), Advances
in Neural Information Processing Systems , volume 32, pp. 3608‚Äì3618. Curran Associates, Inc.,
2019.
Bradley Efron. Tweedie‚Äôs formula and selection bias. Journal of the American Statistical Association ,
106(496):1602‚Äì1614, 2011.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural informa-
tion processing systems , pp. 2672‚Äì2680, 2014.
Anirudh Goyal Alias Parth Goyal, Nan Rosemary Ke, Surya Ganguli, and Yoshua Bengio. Variational
walkback: Learning a transition operator as a stochastic recurrent net. In Advances in Neural
Information Processing Systems , pp. 4392‚Äì4402, 2017.
10Published as a conference paper at ICLR 2021
Will Grathwohl, Ricky TQ Chen, Jesse Bettencourt, Ilya Sutskever, and David Duvenaud. Ffjord:
Free-form continuous dynamics for scalable reversible generative models. In International Confer-
ence on Learning Representations , 2018.
Ulf Grenander and Michael I Miller. Representations of knowledge in complex systems. Journal of
the Royal Statistical Society: Series B (Methodological) , 56(4):549‚Äì581, 1994.
Jonathan Ho, Xi Chen, Aravind Srinivas, Yan Duan, and Pieter Abbeel. Flow++: Improving Ô¨Çow-
based generative models with variational dequantization and architecture design. In International
Conference on Machine Learning , pp. 2722‚Äì2730, 2019.
Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in
Neural Information Processing Systems , 33, 2020.
Michael F Hutchinson. A stochastic estimator of the trace of the inÔ¨Çuence matrix for Laplacian
smoothing splines. Communications in Statistics-Simulation and Computation , 19(2):433‚Äì450,
1990.
Aapo Hyv ¬®arinen. Estimation of non-normalized statistical models by score matching. Journal of
Machine Learning Research , 6(Apr):695‚Äì709, 2005.
Alexia Jolicoeur-Martineau, R ¬¥emi Pich ¬¥e-Taillefer, R ¬¥emi Tachet des Combes, and Ioannis Mitliagkas.
Adversarial score matching and improved sampling for image generation. arXiv preprint
arXiv:2009.05475 , 2020.
Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive growing of gans for
improved quality, stability, and variation. In International Conference on Learning Representations ,
2018.
Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative
adversarial networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition , pp. 4401‚Äì4410, 2019.
Tero Karras, Miika Aittala, Janne Hellsten, Samuli Laine, Jaakko Lehtinen, and Timo Aila. Training
generative adversarial networks with limited data. Advances in Neural Information Processing
Systems , 33, 2020a.
Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Analyzing
and improving the image quality of StyleGAN. In Proc. CVPR , 2020b.
Durk P Kingma and Prafulla Dhariwal. Glow: Generative Ô¨Çow with invertible 1x1 convolutions. In
Advances in Neural Information Processing Systems , pp. 10215‚Äì10224, 2018.
Peter E Kloeden and Eckhard Platen. Numerical solution of stochastic differential equations , vol-
ume 23. Springer Science & Business Media, 2013.
Zhifeng Kong, Wei Ping, Jiaji Huang, Kexin Zhao, and Bryan Catanzaro. Diffwave: A versatile
diffusion model for audio synthesis. arXiv preprint arXiv:2009.09761 , 2020.
Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.
Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild. In
Proceedings of International Conference on Computer Vision (ICCV) , December 2015.
Dimitra Maoutsa, Sebastian Reich, and Manfred Opper. Interacting particle solutions of fokker-planck
equations through gradient-log-density estimation. arXiv preprint arXiv:2006.00702 , 2020.
Radford M Neal et al. Mcmc using hamiltonian dynamics. Handbook of markov chain monte carlo ,
2(11):2, 2011.
Chenhao Niu, Yang Song, Jiaming Song, Shengjia Zhao, Aditya Grover, and Stefano Ermon. Permu-
tation invariant graph generation via score-based generative modeling. volume 108 of Proceedings
of Machine Learning Research , pp. 4474‚Äì4484, Online, 26‚Äì28 Aug 2020. PMLR.
11Published as a conference paper at ICLR 2021
Bernt √òksendal. Stochastic differential equations. In Stochastic differential equations , pp. 65‚Äì84.
Springer, 2003.
Tianyu Pang, Kun Xu, Chongxuan Li, Yang Song, Stefano Ermon, and Jun Zhu. EfÔ¨Åcient learning of
generative models via Ô¨Ånite-difference score matching. arXiv preprint arXiv:2007.03317 , 2020.
Giorgio Parisi. Correlation functions and computer simulations. Nuclear Physics B , 180(3):378‚Äì384,
1981.
Ali Razavi, Aaron van den Oord, and Oriol Vinyals. Generating diverse high-Ô¨Ådelity images with
vq-vae-2. In Advances in Neural Information Processing Systems , pp. 14837‚Äì14847, 2019.
Geoffrey Roeder, Luke Metz, and Diederik P Kingma. On linear identiÔ¨Åability of learned representa-
tions. arXiv preprint arXiv:2007.00810 , 2020.
Simo S ¬®arkk¬®a and Arno Solin. Applied stochastic differential equations , volume 10. Cambridge
University Press, 2019.
John Skilling. The eigenvalues of mega-dimensional matrices. In Maximum Entropy and Bayesian
Methods , pp. 455‚Äì466. Springer, 1989.
Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised
learning using nonequilibrium thermodynamics. In International Conference on Machine Learning ,
pp. 2256‚Äì2265, 2015.
Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution.
InAdvances in Neural Information Processing Systems , pp. 11895‚Äì11907, 2019.
Yang Song and Stefano Ermon. Improved techniques for training score-based generative models.
Advances in Neural Information Processing Systems , 33, 2020.
Yang Song, Sahaj Garg, Jiaxin Shi, and Stefano Ermon. Sliced score matching: A scalable approach
to density and score estimation. In Proceedings of the Thirty-Fifth Conference on Uncertainty in
ArtiÔ¨Åcial Intelligence, UAI 2019, Tel Aviv, Israel, July 22-25, 2019 , pp. 204, 2019a.
Yang Song, Chenlin Meng, and Stefano Ermon. Mintnet: Building invertible neural networks with
masked convolutions. In Advances in Neural Information Processing Systems , pp. 11002‚Äì11012,
2019b.
Matthew Tancik, Pratul P. Srinivasan, Ben Mildenhall, Sara Fridovich-Keil, Nithin Raghavan, Utkarsh
Singhal, Ravi Ramamoorthi, Jonathan T. Barron, and Ren Ng. Fourier features let networks learn
high frequency functions in low dimensional domains. NeurIPS , 2020.
Pascal Vincent. A connection between score matching and denoising autoencoders. Neural computa-
tion, 23(7):1661‚Äì1674, 2011.
Fisher Yu, Ari Seff, Yinda Zhang, Shuran Song, Thomas Funkhouser, and Jianxiong Xiao. Lsun:
Construction of a large-scale image dataset using deep learning with humans in the loop. arXiv
preprint arXiv:1506.03365 , 2015.
Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. arXiv preprint arXiv:1605.07146 ,
2016.
Richard Zhang. Making convolutional networks shift-invariant again. In ICML , 2019.
12Published as a conference paper at ICLR 2021
APPENDIX
We include several appendices with additional details, derivations, and results. Our framework
allows general SDEs with matrix-valued diffusion coefÔ¨Åcients that depend on the state, for which
we provide a detailed discussion in Appendix A. We give a full derivation of VE, VP and sub-VP
SDEs in Appendix B, and discuss how to use them from a practitioner‚Äôs perspective in Appendix C.
We elaborate on the probability Ô¨Çow formulation of our framework in Appendix D, including a
derivation of the probability Ô¨Çow ODE (Appendix D.1), exact likelihood computation (Appendix D.2),
probability Ô¨Çow sampling with a Ô¨Åxed discretization strategy (Appendix D.3), sampling with black-
box ODE solvers (Appendix D.4), and experimental veriÔ¨Åcation on uniquely identiÔ¨Åable encoding
(Appendix D.5). We give a full description of the reverse diffusion sampler in Appendix E, the
DDPM-type ancestral sampler for SMLD models in Appendix F, and Predictor-Corrector samplers in
Appendix G. We explain our model architectures and detailed experimental settings in Appendix H,
with10241024 CelebA-HQ samples therein. Finally, we detail on the algorithms for controllable
generation in Appendix I, and include extended results for class-conditional generation (Appendix I.1),
image inpainting (Appendix I.2), colorization (Appendix I.3), and a strategy for solving general
inverse problems (Appendix I.4).
A T HE FRAMEWORK FOR MORE GENERAL SDE S
In the main text, we introduced our framework based on a simpliÔ¨Åed SDE Eq. (5) where the diffusion
coefÔ¨Åcient is independent of xptq. It turns out that our framework can be extended to hold for more
general diffusion coefÔ¨Åcients. We can consider SDEs in the following form:
dxfpx;tqdt Gpx;tqdw; (15)
where fp;tq:Rd√ëRdandGp;tq:Rd√ëRdd. We follow the It ÀÜo interpretation of SDEs
throughout this paper.
According to (Anderson, 1982), the reverse-time SDE is given by ( cf., Eq. (6))
dxtfpx;tqrrGpx;tqGpx;tqTsGpx;tqGpx;tqTrxlogptpxqudt Gpx;tqdw;(16)
where we deÔ¨Åne rFpxq:prf1pxq;rf2pxq;;rfdpxqqTfor a matrix-valued function
Fpxq:pf1pxq;f2pxq;;fdpxqqTthroughout the paper.
The probability Ô¨Çow ODE corresponding to Eq. (15) has the following form ( cf., Eq. (13), see a
detailed derivation in Appendix D.1):
dx"
fpx;tq1
2rrGpx;tqGpx;tqTs1
2Gpx;tqGpx;tqTrxlogptpxq*
dt: (17)
Finally for conditional generation with the general SDE Eq. (15), we can solve the conditional
reverse-time SDE below ( cf., Eq. (14), details in Appendix I):
dxtfpx;tqrrGpx;tqGpx;tqTsGpx;tqGpx;tqTrxlogptpxq
Gpx;tqGpx;tqTrxlogptpy|xqudt Gpx;tqdw:(18)
When the drift and diffusion coefÔ¨Åcient of an SDE are not afÔ¨Åne, it can be difÔ¨Åcult to compute the
transition kernel p0tpxptq|xp0qqin closed form. This hinders the training of score-based models,
because Eq. (7) requires knowing rxptqlogp0tpxptq|xp0qq. To overcome this difÔ¨Åculty, we can
replace denoising score matching in Eq. (7) with other efÔ¨Åcient variants of score matching that do not
require computing rxptqlogp0tpxptq|xp0qq. For example, when using sliced score matching (Song
et al., 2019a), our training objective Eq. (7) becomes
arg min
Et"
ptqExp0qExptqEvpv1
2kspxptq;tqk2
2 vTspxptq;tqv*
; (19)
where:r0;Ts√ëR is a positive weighting function, tUp0;Tq,Ervs0, and CovrvsI.
We can always simulate the SDE to sample from p0tpxptq|xp0qq, and solve Eq. (19) to train the
time-dependent score-based model spx;tq.
13Published as a conference paper at ICLR 2021
B VE, VP AND SUB -VP SDE S
Below we provide detailed derivations to show that the noise perturbations of SMLD and DDPM
are discretizations of the Variance Exploding (VE) and Variance Preserving (VP) SDEs respectively.
We additionally introduce sub-VP SDEs, a modiÔ¨Åcation to VP SDEs that often achieves better
performance in both sample quality and likelihoods.
First, when using a total of Nnoise scales, each perturbation kernel pipx|x0qof SMLD can be
derived from the following Markov chain:
xixi1 b
2
i2
i1zi1; i1;;N; (20)
where zi1Np0;Iq,x0pdata, and we have introduced 00to simplify the notation. In the
limit ofN√ë8 , the Markov chain txiuN
i1becomes a continuous stochastic process txptqu1
t0,
tiuN
i1becomes a function ptq, andzibecomes zptq, where we have used a continuous time variable
tP r0;1sfor indexing, rather than an integer iP t1;2;;Nu. Let x i
N
xi, i
N
i,
andz i
N
zifori1;2;;N. We can rewrite Eq. (20) as follows with t1
Nand
tP 
0;1
N;;N1
N(
:
xpt tqxptq a
2pt tq2ptqzptqxptq c
dr2ptqs
dttzptq;
where the approximate equality holds when t!1. In the limit of t√ë0, this converges to
dxc
dr2ptqs
dtdw; (21)
which is the VE SDE.
For the perturbation kernels tpipx|x0quN
i1used in DDPM, the discrete Markov chain is
xia
1ixi1 a
izi1; i1;;N; (22)
where zi1Np0;Iq. To obtain the limit of this Markov chain when N√ë 8 , we deÔ¨Åne an
auxiliary set of noise scales tiNiuN
i1, and re-write Eq. (22) as below
xic
1i
Nxi1 ci
Nzi1; i1;;N: (23)
In the limit of N√ë8 ,tiuN
i1becomes a function ptqindexed bytPr0;1s. Let i
N
i,
xpi
Nqxi,zpi
Nqzi. We can rewrite the Markov chain Eq. (23) as the following with t1
N
andtPt0;1;;N1
Nu:
xpt tqa
1pt tqtxptq a
pt tqtzptq
xptq1
2pt tqtxptq a
pt tqtzptq
xptq1
2ptqtxptq a
ptqtzptq; (24)
where the approximate equality holds when t!1. Therefore, in the limit of t√ë0, Eq. (24)
converges to the following VP SDE:
dx1
2ptqxdt a
ptqdw: (25)
So far, we have demonstrated that the noise perturbations used in SMLD and DDPM correspond to
discretizations of VE and VP SDEs respectively. The VE SDE always yields a process with exploding
variance when t√ë8 . In contrast, the VP SDE yields a process with bounded variance. In addition,
the process has a constant unit variance for all tPr0;8qwhenppxp0qqhas a unit variance. Since the
VP SDE has afÔ¨Åne drift and diffusion coefÔ¨Åcients, we can use Eq. (5.51) in S ¬®arkk¬®a & Solin (2019) to
obtain an ODE that governs the evolution of variance
dVPptq
dtptqpIVPptqq;
14Published as a conference paper at ICLR 2021
where VPptq:Covrxptqsfortxptqu1
t0obeying a VP SDE. Solving this ODE, we obtain
VPptqI e¬≥t
0psqdspVPp0qIq; (26)
from which it is clear that the variance VPptqis always bounded given VPp0q. Moreover, VPptq
IifVPp0qI. Due to this difference, we name Eq. (9) as the Variance Exploding (VE) SDE , and
Eq. (11) the Variance Preserving (VP) SDE .
Inspired by the VP SDE, we propose a new SDE called the sub-VP SDE , namely
dx1
2ptqxdt b
ptqp1e2¬≥t
0psqdsqdw: (27)
Following standard derivations, it is straightforward to show that Erxptqsis the same for both VP and
sub-VP SDEs; the variance function of sub-VP SDEs is different, given by
sub-VPptqI e2¬≥t
0psqdsI e¬≥t
0psqdspsub-VPp0q2Iq; (28)
where sub-VPptq:Covrxptqsfor a processtxptqu1
t0obtained by solving Eq. (27). In addition,
we observe that (i) sub-VPptq¬§VPptqfor allt¬•0withsub-VPp0qVPp0qand sharedpsq;
and (ii) limt√ë8sub-VPptqlimt√ë8VPptqIiflimt√ë8¬≥t
0psqds8 . The former is why we
name Eq. (27) the sub-VP SDE‚Äîits variance is always upper bounded by the corresponding VP
SDE. The latter justiÔ¨Åes the use of sub-VP SDEs for score-based generative modeling, since they can
perturb any data distribution to standard Gaussian under suitable conditions, just like VP SDEs.
VE, VP and sub-VP SDEs all have afÔ¨Åne drift coefÔ¨Åcients. Therefore, their perturbation kernels
p0tpxptq|xp0qqare all Gaussian and can be computed with Eqs. (5.50) and (5.51) in S ¬®arkk¬®a & Solin
(2019):
p0tpxptq|xp0qq$
'&
'%N 
xptq;xp0q;r2ptq2p0qsI
; (VE SDE)
N 
xptq;xp0qe1
2¬≥t
0psqds;IIe¬≥t
0psqds
(VP SDE)
N 
xptq;xp0qe1
2¬≥t
0psqds;r1e¬≥t
0psqdss2I
(sub-VP SDE):(29)
As a result, all SDEs introduced here can be efÔ¨Åciently trained with the objective in Eq. (7).
C SDE S IN THE WILD
Below we discuss concrete instantiations of VE and VP SDEs whose discretizations yield SMLD
and DDPM models, and the speciÔ¨Åc sub-VP SDE used in our experiments. In SMLD, the noise
scalestiuN
i1is typically a geometric sequence where minis Ô¨Åxed to 0:01andmaxis chosen
according to Technique 1 in Song & Ermon (2020). Usually, SMLD models normalize image inputs
to the ranger0;1s. SincetiuN
i1is a geometric sequence, we have pi
Nqimin
max
min	i1
N1
fori1;2;;N. In the limit of N√ë 8 , we haveptq min
max
min	t
fortP p0;1s. The
corresponding VE SDE is
dxminmax
min
tc
2 logmax
mindw; tPp0;1s; (30)
and the perturbation kernel can be derived via Eq. (29):
p0tpxptq|xp0qqN
xptq;xp0q;2
minmax
min	2t
I
; tPp0;1s: (31)
There is one subtlety when t0: by deÔ¨Ånition, p0q00(following the convention in Eq. (20)),
butp0 q:limt√ë0 ptqmin0. In other words, ptqfor SMLD is not differentiable since
p0qp0 q, causing the VE SDE in Eq. (21) undeÔ¨Åned for t0. In practice, we bypass this issue
by always solving the SDE and its associated probability Ô¨Çow ODE in the range tPr;1sfor some
small constant ¬°0, and we use 105in our VE SDE experiments.
15Published as a conference paper at ICLR 2021
0.0 0.2 0.4 0.6 0.8 1.0
t05001000150020002500VarianceVariance of Perturbation Kernels
SMLD original
VE SDE
(a) SMLD
0.0 0.2 0.4 0.6 0.8 1.0
t0.00.20.40.60.81.0Scaling Factor of MeansMean of Perturbation Kernels
DDPM original
VP SDE (b) DDPM (mean)
0.0 0.2 0.4 0.6 0.8 1.0
t0.00.20.40.60.81.0VarianceVariance of Perturbation Kernels
DDPM original
VP SDE (c) DDPM (variance)
Figure 5: Discrete-time perturbation kernels and our continuous generalizations match each other
almost exactly. (a) compares the variance of perturbation kernels for SMLD and VE SDE; (b)
compares the scaling factors of means of perturbation kernels for DDPM and VP SDE; and (c)
compares the variance of perturbation kernels for DDPM and VP SDE.
For DDPM models, tiuN
i1is typically an arithmetic sequence where imin
N i1
NpN1qpmax
minqfori1;2;;N. Therefore, ptqmin tpmaxminqfortPr0;1sin the limit of
N√ë8 . This corresponds to the following instantiation of the VP SDE:
dx1
2pmin tpmaxminqqxdt b
min tpmaxminqdw; tPr0;1s; (32)
where xp0qpdatapxq. In our experiments, we let min0:1andmax20to match the settings in
Ho et al. (2020). The perturbation kernel is given by
p0tpxptq|xp0qq
N
xptq;e1
4t2pmaxminq1
2tminxp0q;IIe1
2t2pmaxminqtmin	
; tPr0;1s:(33)
For DDPM, there is no discontinuity issue with the corresponding VP SDE; yet, there are numerical
instability issues for training and sampling at t0, due to the vanishing variance of xptqast√ë0.
Therefore, same as the VE SDE, we restrict computation to tPr;1sfor a small¬°0. For sampling,
we choose103so that the variance of xpqin VP SDE matches the variance of x1in DDPM;
for training and likelihood computation, we adopt 105which empirically gives better results.
As a sanity check for our SDE generalizations to SMLD and DDPM, we compare the perturbation
kernels of SDEs and original discrete Markov chains in Fig. 5. The SMLD and DDPM models both
useN1000 noise scales. For SMLD, we only need to compare the variances of perturbation
kernels since means are the same by deÔ¨Ånition. For DDPM, we compare the scaling factors of means
and the variances. As demonstrated in Fig. 5, the discrete perturbation kernels of original SMLD and
DDPM models align well with perturbation kernels derived from VE and VP SDEs.
For sub-VP SDEs, we use exactly the same ptqas VP SDEs. This leads to the following perturbation
kernel
p0tpxptq|xp0qq
N
xptq;e1
4t2pmaxminq1
2tminxp0q;r1e1
2t2pmaxminqtmins2I	
; tPr0;1s:(34)
We also restrict numerical computation to the same interval of r;1sas VP SDEs.
Empirically, we observe that smaller generally yields better likelihood values for all SDEs. For
sampling, it is important to use an appropriate for better Inception scores and FIDs, although
samples across different look visually the same to human eyes.
D P ROBABILITY FLOW ODE
D.1 D ERIVATION
The idea of probability Ô¨Çow ODE is inspired by Maoutsa et al. (2020), and one can Ô¨Ånd the derivation
of a simpliÔ¨Åed case therein. Below we provide a derivation for the fully general ODE in Eq. (17). We
16Published as a conference paper at ICLR 2021
consider the SDE in Eq. (15), which possesses the following form:
dxfpx;tqdt Gpx;tqdw;
where fp;tq:Rd√ëRdandGp;tq:Rd√ëRdd. The marginal probability density ptpxptqq
evolves according to Kolmogorov‚Äôs forward equation (Fokker-Planck equation) (√òksendal, 2003)
Bptpxq
Btd¬∏
i1B
Bxirfipx;tqptpxqs 1
2d¬∏
i1d¬∏
j1B2
BxiBxjd¬∏
k1Gikpx;tqGjkpx;tqptpxq
:(35)
We can easily rewrite Eq. (35) to obtain
Bptpxq
Btd¬∏
i1B
Bxirfipx;tqptpxqs 1
2d¬∏
i1d¬∏
j1B2
BxiBxjd¬∏
k1Gikpx;tqGjkpx;tqptpxq
d¬∏
i1B
Bxirfipx;tqptpxqs 1
2d¬∏
i1B
Bxid¬∏
j1B
Bxjd¬∏
k1Gikpx;tqGjkpx;tqptpxq
:(36)
Note that
d¬∏
j1B
Bxjd¬∏
k1Gikpx;tqGjkpx;tqptpxq
d¬∏
j1B
Bxjd¬∏
k1Gikpx;tqGjkpx;tq
ptpxq d¬∏
j1d¬∏
k1Gikpx;tqGjkpx;tqptpxqB
Bxjlogptpxq
ptpxqrrGpx;tqGpx;tqTs ptpxqGpx;tqGpx;tqTrxlogptpxq;
based on which we can continue the rewriting of Eq. (36) to obtain
Bptpxq
Btd¬∏
i1B
Bxirfipx;tqptpxqs 1
2d¬∏
i1B
Bxid¬∏
j1B
Bxjd¬∏
k1Gikpx;tqGjkpx;tqptpxq
d¬∏
i1B
Bxirfipx;tqptpxqs
 1
2d¬∏
i1B
Bxi
ptpxqrrGpx;tqGpx;tqTs ptpxqGpx;tqGpx;tqTrxlogptpxq
d¬∏
i1B
Bxi!
fipx;tqptpxq
1
2
rrGpx;tqGpx;tqTs Gpx;tqGpx;tqTrxlogptpxq
ptpxq)
d¬∏
i1B
Bxir~fipx;tqptpxqs; (37)
where we deÔ¨Åne
~fpx;tq:fpx;tq1
2rrGpx;tqGpx;tqTs1
2Gpx;tqGpx;tqTrxlogptpxq:
Inspecting Eq. (37), we observe that it equals Kolmogorov‚Äôs forward equation of the following
SDE with ~Gpx;tq:0(Kolmogorov‚Äôs forward equation in this case is also known as the Liouville
equation.)
dx~fpx;tqdt ~Gpx;tqdw;
which is essentially an ODE:
dx~fpx;tqdt;
same as the probability Ô¨Çow ODE given by Eq. (17). Therefore, we have shown that the probability
Ô¨Çow ODE Eq. (17) induces the same marginal probability density ptpxqas the SDE in Eq. (15).
17Published as a conference paper at ICLR 2021
D.2 L IKELIHOOD COMPUTATION
The probability Ô¨Çow ODE in Eq. (17) has the following form when we replace the score rxlogptpxq
with the time-dependent score-based model spx;tq:
dx"
fpx;tq1
2rrGpx;tqGpx;tqTs1
2Gpx;tqGpx;tqTspx;tq*
loooooooooooooooooooooooooooooooooooooooooomoooooooooooooooooooooooooooooooooooooooooon
:~fpx;tqdt: (38)
With the instantaneous change of variables formula (Chen et al., 2018), we can compute the log-
likelihood of p0pxqusing
logp0pxp0qq logpTpxpTqq ¬ªT
0r~fpxptq;tqdt; (39)
where the random variable xptqas a function of tcan be obtained by solving the probability Ô¨Çow
ODE in Eq. (38). In many cases computing r~fpx;tqis expensive, so we follow Grathwohl et al.
(2018) to estimate it with the Skilling-Hutchinson trace estimator (Skilling, 1989; Hutchinson, 1990).
In particular, we have
r~fpx;tqEppqrTr~fpx;tqs; (40)
where r~fdenotes the Jacobian of ~fp;tq, and the random variable satisÔ¨Åes Eppqrs0and
CovppqrsI. The vector-Jacobian product Tr~fpx;tqcan be efÔ¨Åciently computed using reverse-
mode automatic differentiation, at approximately the same cost as evaluating ~fpx;tq. As a result,
we can sample ppqand then compute an efÔ¨Åcient unbiased estimate to r~fpx;tqusing
Tr~fpx;tq. Since this estimator is unbiased, we can attain an arbitrarily small error by averaging
over a sufÔ¨Åcient number of runs. Therefore, by applying the Skilling-Hutchinson estimator Eq. (40)
to Eq. (39), we can compute the log-likelihood to any accuracy.
In our experiments, we use the RK45 ODE solver (Dormand & Prince, 1980) provided by
scipy.integrate.solve_ivp in all cases. The bits/dim values in Table 2 are computed
withatol=1e-5 andrtol=1e-5 , same as Grathwohl et al. (2018). To give the likelihood results
of our models in Table 2, we average the bits/dim obtained on the test dataset over Ô¨Åve different runs
with105(see deÔ¨Ånition of in Appendix C).
D.3 P ROBABILITY FLOW SAMPLING
Suppose we have a forward SDE
dxfpx;tqdt Gptqdw;
and one of its discretization
xi 1xi fipxiq Gizi; i0;1;;N1; (41)
where ziNp0;Iq. We assume the discretization schedule of time is Ô¨Åxed beforehand, and thus
we absorb the dependency on tinto the notations of fiandGi. Using Eq. (17), we can obtain the
following probability Ô¨Çow ODE:
dx"
fpx;tq1
2GptqGptqTrxlogptpxq*
dt: (42)
We may employ any numerical method to integrate the probability Ô¨Çow ODE backwards in time for
sample generation. In particular, we propose a discretization in a similar functional form to Eq. (41):
xixi 1fi 1pxi 1q 1
2Gi 1GT
i 1spxi 1;i 1q; i0;1;;N1;
where the score-based model spxi;iqis conditioned on the iteration number i. This is a determin-
istic iteration rule. Unlike reverse diffusion samplers or ancestral sampling, there is no additional
randomness once the initial sample xNis obtained from the prior distribution. When applied to
SMLD models, we can get the following iteration rule for probability Ô¨Çow sampling:
xixi 1 1
2p2
i 12
iqspxi 1;i 1q; i0;1;;N1: (43)
Similarly, for DDPM models, we have
xip2a
1i 1qxi 1 1
2i 1spxi 1;i 1q; i0;1;;N1: (44)
18Published as a conference paper at ICLR 2021
D.4 S AMPLING WITH BLACK -BOX ODE SOLVERS
For producing Ô¨Ågures in Fig. 3, we use a DDPM model trained on 256256CelebA-HQ with the
same settings in Ho et al. (2020). All FID scores of our models in Table 2 are computed on samples
from the RK45 ODE solver implemented in scipy.integrate.solve_ivp withatol=1e-5
andrtol=1e-5 . We use105for VE SDEs and 103for VP SDEs (see also Appendix C).
Aside from the interpolation results in Fig. 3, we demonstrate more examples of latent space
manipulation in Fig. 6, including interpolation and temperature scaling. The model tested here is a
DDPM model trained with the same settings in Ho et al. (2020).
Although solvers for the probability Ô¨Çow ODE allow fast sampling, their samples typically have
higher (worse) FID scores than those from SDE solvers if no corrector is used. We have this
empirical observation for both the discretization strategy in Appendix D.3, and black-box ODE
solvers introduced above. Moreover, the performance of probability Ô¨Çow ODE samplers depends on
the choice of the SDE‚Äîtheir sample quality for VE SDEs is much worse than VP SDEs especially
for high-dimensional data.
D.5 U NIQUELY IDENTIFIABLE ENCODING
As a sanity check, we train two models (denoted as ‚ÄúModel A‚Äù and ‚ÄúModel B‚Äù) with different
architectures using the VE SDE on CIFAR-10. Here Model A is an NCSN++ model with 4 layers per
resolution trained using the continuous objective in Eq. (7), and Model B is all the same except that it
uses 8 layers per resolution. Model deÔ¨Ånitions are in Appendix H.
We report the latent codes obtained by Model A and Model B for a random CIFAR-10 image in
Fig. 7. In Fig. 8, we show the dimension-wise differences and correlation coefÔ¨Åcients between latent
encodings on a total of 16 CIFAR-10 images. Our results demonstrate that for the same inputs, Model
A and Model B provide encodings that are close in every dimension, despite having different model
architectures and training runs.
E R EVERSE DIFFUSION SAMPLING
Given a forward SDE
dxfpx;tqdt Gptqdw;
and suppose the following iteration rule is a discretization of it:
xi 1xi fipxiq Gizi; i0;1;;N1 (45)
where ziNp0;Iq. Here we assume the discretization schedule of time is Ô¨Åxed beforehand, and
thus we can absorb it into the notations of fiandGi.
Based on Eq. (45), we propose to discretize the reverse-time SDE
dxrfpx;tqGptqGptqTrxlogptpxqsdt Gptqdw;
with a similar functional form, which gives the following iteration rule for iPt0;1;;N1u:
xixi 1fi 1pxi 1q Gi 1GT
i 1spxi 1;i 1q Gi 1zi 1; (46)
where our trained score-based model spxi;iqis conditioned on iteration number i.
When applying Eq. (46) to Eqs. (10) and (20), we obtain a new set of numerical solvers for the
reverse-time VE and VP SDEs, resulting in sampling algorithms as shown in the ‚Äúpredictor‚Äù part of
Algorithms 2 and 3. We name these sampling methods (that are based on the discretization strategy
in Eq. (46)) reverse diffusion samplers .
As expected, the ancestral sampling of DDPM (Ho et al., 2020) (Eq. (4)) matches its reverse diffusion
counterpart when i√ë0for alli(which happens when t√ë0sinceiit, see Appendix B),
19Published as a conference paper at ICLR 2021
Figure 6: Samples from the probability Ô¨Çow ODE for VP SDE on 256256CelebA-HQ. Top:
spherical interpolations between random samples. Bottom: temperature rescaling (reducing norm of
embedding).
20Published as a conference paper at ICLR 2021
0 20 40 60 80 100
Dimension100
0100Latent value
Model A
Model B
Figure 7: Comparing the Ô¨Årst 100 dimensions of the latent code obtained for a random CIFAR-10
image. ‚ÄúModel A‚Äù and ‚ÄúModel B‚Äù are separately trained with different architectures.
0 100 200 300
Difference in encodingsModel A 
 vs.
 Model B
Model A (shuffled) 
 vs. 
 Model B (shuffled)
0.00 0.25 0.50 0.75 1.00
Correlation Coefficient0200400600Count
Model AModel B
r=0.96x1(T)
Figure 8: Left: The dimension-wise difference between encodings obtained by Model A and B. As a
baseline, we also report the difference between shufÔ¨Çed representations of these two models. Right :
The dimension-wise correlation coefÔ¨Åcients of encodings obtained by Model A and Model B.
because
xi1a
1i 1pxi 1 i 1spxi 1;i 1qq a
i 1zi 1

1 1
2i 1 opi 1q
pxi 1 i 1spxi 1;i 1qq a
i 1zi 1

1 1
2i 1
pxi 1 i 1spxi 1;i 1qq a
i 1zi 1

1 1
2i 1
xi 1 i 1spxi 1;i 1q 1
22
i 1spxi 1;i 1q a
i 1zi 1

1 1
2i 1
xi 1 i 1spxi 1;i 1q a
i 1zi 1

2
11
2i 1

xi 1 i 1spxi 1;i 1q a
i 1zi 1

2
11
2i 1
 opi 1q
xi 1 i 1spxi 1;i 1q a
i 1zi 1
p2a
1i 1qxi 1 i 1spxi 1;i 1q a
i 1zi 1:
Therefore, the original ancestral sampler of Eq. (4) is essentially a different discretization to the same
reverse-time SDE. This uniÔ¨Åes the sampling method in Ho et al. (2020) as a numerical solver to the
reverse-time VP SDE in our continuous framework.
F A NCESTRAL SAMPLING FOR SMLD MODELS
The ancestral sampling method for DDPM models can also be adapted to SMLD models. Consider a
sequence of noise scales 1¬†2¬†¬†Nas in SMLD. By perturbing a data point x0with these
noise scales sequentially, we obtain a Markov chain x0√ëx1√ë√ë xN, where
ppxi|xi1qNpxi;xi1;p2
i2
i1qIq; i1;2;;N:
21Published as a conference paper at ICLR 2021
Algorithm 1 Predictor-Corrector (PC) sampling
Require:
N: Number of discretization steps for the reverse-time SDE
M: Number of corrector steps
1:Initialize xNpTpxq
2:foriN1to0do
3: xi√êPredictorpxi 1q
4: forj1toMdo
5: xi√êCorrectorpxiq
6:return x0
Here we assume 00to simplify notations. Following Ho et al. (2020), we can compute
qpxi1|xi;x0qN
xi1;2
i1
2
ixi 
12
i1
2
i	
x0;2
i1p2
i2
i1q
2
iI
:
If we parameterize the reverse transition kernel as ppxi1|xiqNpxi1;pxi;iq;2
iIq, then
Lt1EqrDKLpqpxi1|xi;x0qq}ppxi1|xiqs
Eq
1
22
i2
i1
2
ixi 
12
i1
2
i	
x0pxi;iq2
2
 C
Ex0;z
1
22
ixipx0;zq2
i2
i1
izpxipx0;zq;iq2
2
 C;
whereLt1is one representative term in the ELBO objective (see Eq. (8) in Ho et al. (2020)), Cis
a constant that does not depend on ,zNp0;Iq, andxipx0;zqx0 iz. We can therefore
parameterize pxi;iqvia
pxi;iqxi p2
i2
i1qspxi;iq;
where spxi;iqis to estimate z{i. As in Ho et al. (2020), we let ic
2
i1p2
i2
i1q
2
i. Through
ancestral sampling on¬±N
i1ppxi1|xiq, we obtain the following iteration rule
xi1xi p2
i2
i1qspxi;iq d
2
i1p2
i2
i1q
2
izi;i1;2;;N; (47)
where xNNp0;2
NIq,denotes the optimal parameter of s, and ziNp0;Iq. We call
Eq. (47) the ancestral sampling method for SMLD models.
G P REDICTOR -CORRECTOR SAMPLERS
Predictor-Corrector (PC) sampling The predictor can be any numerical solver for the reverse-
time SDE with a Ô¨Åxed discretization strategy. The corrector can be any score-based MCMC approach.
In PC sampling, we alternate between the predictor and corrector, as described in Algorithm 1. For
example, when using the reverse diffusion SDE solver (Appendix E) as the predictor, and annealed
Langevin dynamics (Song & Ermon, 2019) as the corrector, we have Algorithms 2 and 3 for VE and
VP SDEs respectively, where tiuN1
i0are step sizes for Langevin dynamics as speciÔ¨Åed below.
The corrector algorithms We take the schedule of annealed Langevin dynamics in Song & Ermon
(2019), but re-frame it with slight modiÔ¨Åcations in order to get better interpretability and empirical
performance. We provide the corrector algorithms in Algorithms 4 and 5 respectively, where we call
rthe ‚Äúsignal-to-noise‚Äù ratio. We determine the step size using the norm of the Gaussian noise kzk2,
norm of the score-based model ksk2and the signal-to-noise ratio r. When sampling a large batch
of samples together, we replace the norm kk2with the average norm across the mini-batch. When
the batch size is small, we suggest replacing kzk2with?
d, wheredis the dimensionality of z.
22Published as a conference paper at ICLR 2021
Algorithm 2 PC sampling (VE SDE)
1:xNNp0;2
maxIq
2:foriN1to0do
3:x1
i√êxi 1 p2
i 12
iqspxi 1;i 1q
4:zNp0;Iq
5:xi√êx1
i b
2
i 12
iz
6: forj1toMdo
7: zNp0;Iq
8: xi√êxi ispxi;iq ?2iz
9:return x0Algorithm 3 PC sampling (VP SDE)
1:xNNp0;Iq
2:foriN1to0do
3:x1
i√êp2?1i 1qxi 1 i 1spxi 1;i 1q
4:zNp0;Iq
5:xi√êx1
i ?i 1z
6: forj1toMdo
7: zNp0;Iq
8: xi√êxi ispxi;iq ?2iz
9:return x0
Algorithm 4 Corrector algorithm (VE SDE).
Require:tiuN
i1;r;N;M .
1:x0
NNp0;2
maxIq
2:fori√êNto1do
3: forj√ê1toMdo
4: zNp0;Iq
5: g√êspxj1
i;iq
6:√ê2prkzk2{kgk2q2
7: xj
i√êxj1
i g ?
2z
8:x0
i1√êxM
i
return x0
0Algorithm 5 Corrector algorithm (VP SDE).
Require:tiuN
i1;tiuN
i1;r;N;M .
1:x0
NNp0;Iq
2:fori√êNto1do
3: forj√ê1toMdo
4: zNp0;Iq
5: g√êspxj1
i;iq
6:√ê2iprkzk2{kgk2q2
7: xj
i√êxj1
i g ?
2z
8:x0
i1√êxM
i
return x0
0
Denoising For both SMLD and DDPM models, the generated samples typically contain small
noise that is hard to detect by humans. As noted by Jolicoeur-Martineau et al. (2020), FIDs can be
signiÔ¨Åcantly worse without removing this noise. This unfortunate sensitivity to noise is also part of
the reason why NCSN models trained with SMLD has been performing worse than DDPM models
in terms of FID, because the former does not use a denoising step at the end of sampling, while the
latter does. In all experiments of this paper we ensure there is a single denoising step at the end of
sampling, using Tweedie‚Äôs formula (Efron, 2011).
Figure 9: PC sampling for LSUN bedroom and church. The vertical axis corresponds to the total
computation, and the horizontal axis represents the amount of computation allocated to the corrector.
Samples are the best when computation is split between the predictor and corrector.
Training We use the same architecture in Ho et al. (2020) for our score-based models. For the
VE SDE, we train a model with the original SMLD objective in Eq. (1); similarly for the VP SDE,
we use the original DDPM objective in Eq. (3). We apply a total number of 1000 noise scales for
training both models. For results in Fig. 9, we train an NCSN++ model (deÔ¨Ånition in Appendix H) on
23Published as a conference paper at ICLR 2021
Table 4: Comparing different samplers on CIFAR-10, where ‚ÄúP2000‚Äù uses the rounding interpolation
between noise scales. Shaded regions are obtained with the same computation (number of score
function evaluations). Mean and standard deviation are reported over Ô¨Åve sampling runs.
Variance Exploding SDE (SMLD) Variance Preserving SDE (DDPM)
PredictorFID√ì Sampler
P1000 P2000 C2000 PC1000 P1000 P2000 C2000 PC1000
ancestral sampling 4.98.06 4.92.02 3.62.03 3.24.02 3.11.03 3.21.02
reverse diffusion 4.79.07 4.72.07 3.60.02 3.21.02 3.10.03 3.18.01
probability Ô¨Çow 15.41.15 12.87.0920.43.07
3.51.04 3.59.04 3.25.0419.06.06
3.06.03
Table 5: Optimal signal-to-noise ratios of different samplers. ‚ÄúP1000‚Äù or ‚ÄúP2000‚Äù: predictor-only
samplers using 1000 or 2000 steps. ‚ÄúC2000‚Äù: corrector-only samplers using 2000 steps. ‚ÄúPC1000‚Äù:
PC samplers using 1000 predictor and 1000 corrector steps.
VE SDE (SMLD) VP SDE (DDPM)
Predictorr Sampler
P1000 P2000 C2000 PC1000 P1000 P2000 C2000 PC1000
ancestral sampling - - 0.17 - - 0.01
reverse diffusion - - 0.16 - - 0.01
probability Ô¨Çow - -0.22
0.17 - -0.27
0.04
256256LSUN bedroom and church outdoor (Yu et al., 2015) datasets with the VE SDE and our
continuous objective Eq. (7). The batch size is Ô¨Åxed to 128 on CIFAR-10 and 64 on LSUN.
Ad-hoc interpolation methods for noise scales Models in this experiment are all trained with
1000 noise scales. To get results for P2000 (predictor-only sampler using 2000 steps) which requires
2000 noise scales, we need to interpolate between 1000 noise scales at test time. The speciÔ¨Åc
architecture of the noise-conditional score-based model in Ho et al. (2020) uses sinusoidal positional
embeddings for conditioning on integer time steps. This allows us to interpolate between noise scales
at test time in an ad-hoc way (while it is hard to do so for other architectures like the one in Song &
Ermon (2019)). SpeciÔ¨Åcally, for SMLD models, we keep minandmaxÔ¨Åxed and double the number
of time steps. For DDPM models, we halve minandmaxbefore doubling the number of time steps.
Supposetspx;iquN1
i0is a score-based model trained on Ntime steps, and let ts1
px;iqu2N1
i0
denote the corresponding interpolated score-based model at 2Ntime steps. We test two different
interpolation strategies for time steps: linear interpolation where s1
px;iqspx;i{2qand rounding
interpolation where s1
px;iqspx;ti{2uq. We provide results with linear interpolation in Table 1,
and give results of rounding interpolation in Table 4. We observe that different interpolation methods
result in performance differences but maintain the general trend of predictor-corrector methods
performing on par or better than predictor-only or corrector-only samplers.
Hyper-parameters of the samplers For Predictor-Corrector and corrector-only samplers on
CIFAR-10, we search for the best signal-to-noise ratio ( r) over a grid that increments at 0.01.
We report the best rin Table 5. For LSUN bedroom/church outdoor, we Ô¨Åx rto 0.075. Unless
otherwise noted, we use one corrector step per noise scale for all PC samplers. We use two corrector
steps per noise scale for corrector-only samplers on CIFAR-10. For sample generation, the batch size
is 1024 on CIFAR-10 and 8 on LSUN bedroom/church outdoor.
H A RCHITECTURE IMPROVEMENTS
We explored several architecture designs to improve score-based models for both VE and VP SDEs.
Our endeavor gives rise to new state-of-the-art sample quality on CIFAR-10, new state-of-the-art
likelihood on uniformly dequantized CIFAR-10, and enables the Ô¨Årst high-Ô¨Ådelity image samples of
resolution 10241024 from score-based generative models. Code and checkpoints are open-sourced
at https://github.com/yang-song/score sde.
24Published as a conference paper at ICLR 2021
H.1 S ETTINGS FOR ARCHITECTURE EXPLORATION
Unless otherwise noted, all models are trained for 1.3M iterations, and we save one checkpoint per
50k iterations. For VE SDEs, we consider two datasets: 3232CIFAR-10 (Krizhevsky et al., 2009)
and6464CelebA (Liu et al., 2015), pre-processed following Song & Ermon (2020). We compare
different conÔ¨Ågurations based on their FID scores averaged over checkpoints after 0.5M iterations.
For VP SDEs, we only consider the CIFAR-10 dataset to save computation, and compare models
based on the average FID scores over checkpoints obtained between 0.25M and 0.5M iterations,
because FIDs turn to increase after 0.5M iterations for VP SDEs.
All FIDs are computed on 50k samples with tensorflow gan. For sampling, we use the PC
sampler discretized at 1000 time steps. We choose reverse diffusion (see Appendix E) as the predictor.
We use one corrector step per update of the predictor for VE SDEs with a signal-to-noise ratio of
0.16, but save the corrector step for VP SDEs since correctors there only give slightly better results
but require double computation. We follow Ho et al. (2020) for optimization, including the learning
rate, gradient clipping, and learning rate warm-up schedules. Unless otherwise noted, models are
trained with the original discrete SMLD and DDPM objectives in Eqs. (1) and (3) and use a batch
size of 128. The optimal architectures found under these settings are subsequently transferred to
continuous objectives and deeper models. We also directly transfer the best architecture for VP SDEs
to sub-VP SDEs, given the similarity of these two SDEs.
CIFAR-10 CelebA
dataset2.53.03.54.04.5FIDFIR
False
True
CIFAR-10 CelebA
dataset2.53.03.54.04.5FIDskip_rescale
False
True
CIFAR-10 CelebA
dataset2.53.03.54.04.5FID
resblock_type
ddpm
biggan
CIFAR-10 CelebA
dataset2.53.03.54.04.5FID
num_res_blocks
2
4
CIFAR-10 CelebA
dataset2.53.03.54.04.5FID
Progressive Arch. (input, output)
none, none
input_skip, none
residual, none
none, output_skip
input_skip, output_skip
residual, output_skip
none, residual
input_skip, residual
residual, residual
Figure 10: The effects of different architecture components for score-based models trained with VE
perturbations.
Our architecture is mostly based on Ho et al. (2020). We additionally introduce the following
components to maximize the potential improvement of score-based models.
1.Upsampling and downsampling images with anti-aliasing based on Finite Impulse Re-
sponse (FIR) (Zhang, 2019). We follow the same implementation and hyper-parameters in
StyleGAN-2 (Karras et al., 2020b).
2.Rescaling all skip connections by 1{?
2. This has been demonstrated effective in several best-
in-class GAN models, including ProgressiveGAN (Karras et al., 2018), StyleGAN (Karras
et al., 2019) and StyleGAN-2 (Karras et al., 2020b).
3.Replacing the original residual blocks in DDPM with residual blocks from BigGAN (Brock
et al., 2018).
4. Increasing the number of residual blocks per resolution from 2to4.
25Published as a conference paper at ICLR 2021
5.Incorporating progressive growing architectures. We consider two progressive architectures
for input: ‚Äúinput skip‚Äù and ‚Äúresidual‚Äù, and two progressive architectures for output: ‚Äúoutput
skip‚Äù and ‚Äúresidual‚Äù. These progressive architectures are deÔ¨Åned and implemented according
to StyleGAN-2.
We also tested equalized learning rates, a trick used in very successful models like Progressive-
GAN (Karras et al., 2018) and StyleGAN (Karras et al., 2019). However, we found it harmful at an
early stage of our experiments, and therefore decided not to explore more on it.
The exponential moving average (EMA) rate has a signiÔ¨Åcant impact on performance. For models
trained with VE perturbations, we notice that 0.999 works better than 0.9999, whereas for models
trained with VP perturbations it is the opposite. We therefore use an EMA rate of 0.999 and 0.9999
for VE and VP models respectively.
H.2 R ESULTS ON CIFAR-10
All architecture components introduced above can improve the performance of score-based models
trained with VE SDEs, as shown in Fig. 10. The box plots demonstrate the importance of each
component when other components can vary freely. On both CIFAR-10 and CelebA, the additional
components that we explored always improve the performance on average for VE SDEs. For
progressive growing, it is not clear which combination of conÔ¨Ågurations consistently performs the
best, but the results are typically better than when no progressive growing architecture is used.
Our best score-based model for VE SDEs 1) uses FIR upsampling/downsampling, 2) rescales skip
connections, 3) employs BigGAN-type residual blocks, 4) uses 4 residual blocks per resolution
instead of 2, and 5) uses ‚Äúresidual‚Äù for input and no progressive growing architecture for output. We
name this model ‚ÄúNCSN++‚Äù, following the naming convention of previous SMLD models (Song &
Ermon, 2019; 2020).
We followed a similar procedure to examine these architecture components for VP SDEs, except that
we skipped experiments on CelebA due to limited computing resources. The NCSN++ architecture
worked decently well for VP SDEs, ranked 4th place over all 144 possible conÔ¨Ågurations. The top con-
Ô¨Åguration, however, has a slightly different structure, which uses no FIR upsampling/downsampling
and no progressive growing architecture compared to NCSN++. We name this model ‚ÄúDDPM++‚Äù,
following the naming convention of Ho et al. (2020).
The basic NCSN++ model with 4 residual blocks per resolution achieves an FID of 2.45 on CIFAR-10,
whereas the basic DDPM++ model achieves an FID of 2.78. Here in order to match the convention
used in Karras et al. (2018); Song & Ermon (2019) and Ho et al. (2020), we report the lowest FID
value over the course of training, rather than the average FID value over checkpoints after 0.5M
iterations (used for comparing different models of VE SDEs) or between 0.25M and 0.5M iterations
(used for comparing VP SDE models) in our architecture exploration.
Switching from discrete training objectives to continuous ones in Eq. (7) further improves the FID
values for all SDEs. To condition the NCSN++ model on continuous time variables, we change
positional embeddings, the layers in Ho et al. (2020) for conditioning on discrete time steps, to
random Fourier feature embeddings (Tancik et al., 2020). The scale parameter of these random
Fourier feature embeddings is Ô¨Åxed to 16. We also reduce the number of training iterations to 0.95M
to suppress overÔ¨Åtting. These changes improve the FID on CIFAR-10 from 2.45 to 2.38 for NCSN++
trained with the VE SDE, resulting in a model called ‚ÄúNCSN++ cont.‚Äù. In addition, we can further
improve the FID from 2.38 to 2.20 by doubling the number of residual blocks per resolution for
NCSN++ cont., resulting in the model denoted as ‚ÄúNCSN++ cont. (deep)‚Äù. All quantitative results
are summarized in Table 3, and we provide random samples from our best model in Fig. 11.
Similarly, we can also condition the DDPM++ model on continuous time steps, resulting in a model
‚ÄúDDPM++ cont.‚Äù. When trained with the VP SDE, it improves the FID of 2.78 from DDPM++ to
2.55. When trained with the sub-VP SDE, it achieves an FID of 2.61. To get better performance,
we used the Euler-Maruyama solver as the predictor for continuously-trained models, instead of the
ancestral sampling predictor or the reverse diffusion predictor. This is because the discretization
strategy of the original DDPM method does not match the variance of the continuous process well
whent√ë0, which signiÔ¨Åcantly hurts FID scores. As shown in Table 2, the likelihood values are
3.21 and 3.05 bits/dim for VP and sub-VP SDEs respectively. Doubling the depth, and trainin with
26Published as a conference paper at ICLR 2021
Figure 11: Unconditional CIFAR-10 samples from NCSN++ cont. (deep, VE).
27Published as a conference paper at ICLR 2021
Figure 12: Samples on 10241024 CelebA-HQ from a modiÔ¨Åed NCSN++ model trained with the
VE SDE.
28Published as a conference paper at ICLR 2021
0.95M iterations, we can improve both FID and bits/dim for both VP and sub-VP SDEs, leading to a
model ‚ÄúDDPM++ cont. (deep)‚Äù. Its FID score is 2.41, same for both VP and sub-VP SDEs. When
trained with the sub-VP SDE, it can achieve a likelihood of 2.99 bits/dim. Here all likelihood values
are reported for the last checkpoint during training.
H.3 H IGH RESOLUTION IMAGES
Encouraged by the success of NCSN++ on CIFAR-10, we proceed to test it on 10241024 CelebA-
HQ (Karras et al., 2018), a task that was previously only achievable by some GAN models and
VQ-V AE-2 (Razavi et al., 2019). We used a batch size of 8, increased the EMA rate to 0.9999, and
trained a model similar to NCSN++ with the continuous objective (Eq. (7)) for around 2.4M iterations
(please Ô¨Ånd the detailed architecture in our code release.) We use the PC sampler discretized at 2000
steps with the reverse diffusion predictor, one Langevin step per predictor update and a signal-to-noise
ratio of 0.15. The scale parameter for the random Fourier feature embeddings is Ô¨Åxed to 16. We use
the ‚Äúinput skip‚Äù progressive architecture for the input, and ‚Äúoutput skip‚Äù progressive architecture for
the output. We provide samples in Fig. 12. Although these samples are not perfect ( e.g., there are
visible Ô¨Çaws on facial symmetry), we believe these results are encouraging and can demonstrate the
scalability of our approach. Future work on more effective architectures are likely to signiÔ¨Åcantly
advance the performance of score-based generative models on this task.
I C ONTROLLABLE GENERATION
Consider a forward SDE with the following general form
dxfpx;tqdt Gpx;tqdw;
and suppose the initial state distribution is p0pxp0q|yq. The density at time tisptpxptq|yqwhen
conditioned on y. Therefore, using Anderson (1982), the reverse-time SDE is given by
dxtfpx;tqrrGpx;tqGpx;tqTsGpx;tqGpx;tqTrxlogptpx|yqudt Gpx;tqdw:(48)
Sinceptpxptq|yq9ptpxptqqppy|xptqq, the score rxlogptpxptq|yqcan be computed easily by
rxlogptpxptq|yqrxlogptpxptqq rxlogppy|xptqq: (49)
This subsumes the conditional reverse-time SDE in Eq. (14) as a special case. All sampling methods
we have discussed so far can be applied to the conditional reverse-time SDE for sample generation.
I.1 C LASS -CONDITIONAL SAMPLING
When yrepresents class labels, we can train a time-dependent classiÔ¨Åer ptpy|xptqqfor class-
conditional sampling. Since the forward SDE is tractable, we can easily create a pair of training
datapxptq;yqby Ô¨Årst sampling pxp0q;yqfrom a dataset and then obtaining xptqp0tpxptq|xp0qq.
Afterwards, we may employ a mixture of cross-entropy losses over different time steps, like Eq. (7),
to train the time-dependent classiÔ¨Åer ptpy|xptqq.
To test this idea, we trained a Wide ResNet (Zagoruyko & Komodakis, 2016)
(Wide-ResNet-28-10 ) on CIFAR-10 with VE perturbations. The classiÔ¨Åer is condi-
tioned on logiusing random Fourier features (Tancik et al., 2020), and the training objective is
a simple sum of cross-entropy losses sampled at different scales. We provide a plot to show the
accuracy of this classiÔ¨Åer over noise scales in Fig. 13. The score-based model is an unconditional
NCSN++ (4 blocks/resolution) in Table 3, and we generate samples using the PC algorithm with
2000 discretization steps. The class-conditional samples are provided in Fig. 4, and an extended set
of conditional samples is given in Fig. 13.
I.2 I MPUTATION
Imputation is a special case of conditional sampling. Denote by 
pxqand
pxqthe known and un-
known dimensions of xrespectively, and let f
p;tqandG
p;tqdenote fp;tqandGp;tqrestricted
to the unknown dimensions. For VE/VP SDEs, the drift coefÔ¨Åcient fp;tqis element-wise, and the
diffusion coefÔ¨Åcient Gp;tqis diagonal. When fp;tqis element-wise, f
p;tqdenotes the same
29Published as a conference paper at ICLR 2021
102
101
100101
0.20.40.60.8AccuracyAccuracy vs. noise scale
Figure 13: Class-conditional image generation by solving the conditional reverse-time SDE with PC.
The curve shows the accuracy of our noise-conditional classiÔ¨Åer over different noise scales.
30Published as a conference paper at ICLR 2021
element-wise function applied only to the unknown dimensions. When Gp;tqis diagonal, G
p;tq
denotes the sub-matrix restricted to unknown dimensions.
For imputation, our goal is to sample from pp
pxp0qq |
pxp0qq yq. DeÔ¨Åne a new diffusion
process zptq
pxptqq, and note that the SDE for zptqcan be written as
dzf
pz;tqdt G
pz;tqdw:
The reverse-time SDE, conditioned on 
pxp0qqy, is given by
dz 
f
pz;tqrrG
pz;tqG
pz;tqTs
G
pz;tqG
pz;tqTrzlogptpz|
pzp0qqyq(
dt G
pz;tqdw:
Althoughptpzptq|
pxp0qqyqis in general intractable, it can be approximated. Let Adenote the
event 
pxp0qqy. We have
ptpzptq|
pxp0qqyqptpzptq|Aq¬ª
ptpzptq|
pxptqq;Aqptp
pxptqq|Aqd
pxptqq
Eptp
pxptqq|Aqrptpzptq|
pxptqq;Aqs
Eptp
pxptqq|Aqrptpzptq|
pxptqqqs
ptpzptq|^
pxptqqq;
where ^
pxptqqis a random sample from ptp
pxptqq|Aq, which is typically a tractable distribution.
Therefore,
rzlogptpzptq|
pxp0qqyqrzlogptpzptq|^
pxptqqq
rzlogptprzptq;^
pxptqqsq;
whererzptq;^
pxptqqsdenotes a vector uptqsuch that 
puptqq  ^
pxptqqand
puptqq 
zptq, and the identity holds because rzlogptprzptq;^
pxptqqsq  rzlogptpzptq |^
pxptqqq 
rzlogpp^
pxptqqq rzlogptpzptq|^
pxptqqq.
We provided an extended set of inpainting results in Figs. 14 and 15.
I.3 C OLORIZATION
Colorization is a special case of imputation, except that the known data dimensions are coupled.
We can decouple these data dimensions by using an orthogonal linear transformation to map the
gray-scale image to a separate channel in a different space, and then perform imputation to complete
the other channels before transforming everything back to the original image space. The orthogonal
matrix we used to decouple color channels is
0:5770:816 0
0:577 0:408 0:707
0:577 0:4080:707
:
Because the transformations are all orthogonal matrices, the standard Wiener process wptqwill still
be a standard Wiener process in the transformed space, allowing us to build an SDE and use the same
imputation method in Appendix I.2. We provide an extended set of colorization results in Figs. 16
and 17.
I.4 S OLVING GENERAL INVERSE PROBLEMS
Suppose we have two random variables xandy, and we know the forward process of generating y
fromx, given byppy|xq. The inverse problem is to obtain xfromy, that is, generating samples
fromppx|yq. In principle, we can estimate the prior distribution ppxqand obtainppx|yqusing
Bayes‚Äô rule: ppx|yqppxqppy|xq{ppyq. In practice, however, both estimating the prior and
performing Bayesian inference are non-trivial.
Leveraging Eq. (48), score-based generative models provide one way to solve the inverse problem.
Suppose we have a diffusion process txptquT
t0generated by perturbing xwith an SDE, and a
31Published as a conference paper at ICLR 2021
time-dependent score-based model spxptq;tqtrained to approximate rxlogptpxptqq. Once we
have an estimate of rxlogptpxptq|yq, we can simulate the reverse-time SDE in Eq. (48) to sample
fromp0pxp0q|yqppx|yq. To obtain this estimate, we Ô¨Årst observe that
rxlogptpxptq|yqrxlog¬ª
ptpxptq|yptq;yqppyptq|yqdyptq;
where yptqis deÔ¨Åned via xptqand the forward process ppyptq|xptqq. Now assume two conditions:
‚Ä¢ppyptq|yqis tractable. We can often derive this distribution from the interaction between
the forward process and the SDE, like in the case of image imputation and colorization.
‚Ä¢ptpxptq |yptq;yq ptpxptq |yptqq. For small t,yptqis almost the same as yso the
approximation holds. For large t,ybecomes further away from xptqin the Markov chain,
and thus have smaller impact on xptq. Moreover, the approximation error for large tmatter
less for the Ô¨Ånal sample, since it is used early in the sampling process.
Given these two assumptions, we have
rxlogptpxptq|yqrxlog¬ª
ptpxptq|yptqqppyptq|yqdy
rxlogptpxptq|^yptqq
rxlogptpxptqq rxlogptp^yptq|xptqq
spxptq;tq rxlogptp^yptq|xptqq; (50)
where ^yptqis a sample from ppyptq|yq. Now we can plug Eq. (50) into Eq. (48) and solve the
resulting reverse-time SDE to generate samples from ppx|yq.
32Published as a conference paper at ICLR 2021
Figure 14: Extended inpainting results for 256256bedroom images.
33Published as a conference paper at ICLR 2021
Figure 15: Extended inpainting results for 256256church images.
34Published as a conference paper at ICLR 2021
Figure 16: Extended colorization results for 256256bedroom images.
35Published as a conference paper at ICLR 2021
Figure 17: Extended colorization results for 256256church images.
36