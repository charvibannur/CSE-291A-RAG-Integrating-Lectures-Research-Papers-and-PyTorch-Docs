torch.nn.attention
Created On: Jan 24, 2024 | Last Updated On: Oct 29, 2024
This module contains functions and classes that alter the behavior of
torch.nn.functional.scaled_dot_product_attention
Utils
sdpa_kernelContext manager to select which backend to use for scaled dot product
attention.
SDPBackendAn enum-like class that contains the different backends for scaled dot product
attention.
Submodules
flex_attention This module implements the user facing API for flex_attention in PyTorch.
bias Defines bias subclasses that work with scaled_dot_product_attention
experimental
Previous
torch.nn.initNext
torch.nn.attention.sdpa_kernelRate this Page★★★★★
© Copyright PyTorch Contributors.
Built with the PyData Sphinx Theme 0.15.4.Send Feedback
To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or
navigating, you agree to allow our usage of cookies. As the current maintainers of this site,
Facebookʼs Cookies Policy applies. Learn more, including about available controls: Cookies
Policy.10/10/25, 3:06 PM torch.nn.attention — PyTorch 2.8 documentation
https://docs.pytorch.org/docs/stable/nn.attention.html 1/2Docs
Access comprehensive
developer documentation
for PyTorchTutorials
Get in-depth tutorials for
beginners and advanced
developersResources
Find development
resources and get your
questions answered
View Docs View Tutorials View Resources
Stay in touch for updates, event info, and the latest news
First Name* Last Name* Email*
Select Country* SUBMIT
By submitting this form, I consent to receive marketing emails from the LF and its projects
regarding their events, training, research, developments, and related announcements. I understand
that I can unsubscribe at any time using the links in the footers of the emails I receive. Privacy
Policy.
© PyTorch. Copyright © The Linux Foundation®. All rights reserved. The Linux Foundation has
registered trademarks and uses trademarks. For more information, including terms of use, privacy
policy, and trademark usage, please see our Policies page. Trademark Usage. Privacy Policy. To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or
navigating, you agree to allow our usage of cookies. As the current maintainers of this site,
Facebookʼs Cookies Policy applies. Learn more, including about available controls: Cookies
Policy.10/10/25, 3:06 PM torch.nn.attention — PyTorch 2.8 documentation
https://docs.pytorch.org/docs/stable/nn.attention.html 2/2