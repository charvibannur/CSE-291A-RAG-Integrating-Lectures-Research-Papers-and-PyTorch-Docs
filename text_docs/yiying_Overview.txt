CSE291A-Fall25  Systems for LLM and AI Agents - Course OverviewYiying ZhangBefore we start•This course is about AI agent and LLM infrastructure. You will learn the fundamentals about AI agent and LLM backend, including distributed training, LLM inference, reasoning models, augmented LLM, foundations of AI agents, agent tooling, agent memory and context, agent hosting and infra, agent evaluation and observability, security and trustworthy, and agent use cases. 
•This course is not about AI/ML theory or agent frameworks/vibe coding. It will focus on the backend of LLMs and AI agents, with real use cases as the motivation of why backends are designed in certain ways. If you are looking for resources to learn how to build AI agents, this is not the right course for you. 
•Class website: https://cseweb.ucsd.edu/~yiying/cse291a-fall25
•Class time: MWF 9:00pm - 9:50pm over ZoomWho Am I•Associate prof at UCSD CSE
•Working on systems and AI/ML
•AI agent systems, LLM serving, ML for systems
•Leads WukLab (mlsys.wuklab.io) 
•Founder and CEO of GenseeAI (gensee.ai)
•AI agent and tooling infrastructure (we are hiring part-time intern and FTE)
* Generated by Nano Banana based on my headshotCourse Structure•Most are lectures given by me or paper discussion led by me
•Rest are paper discussion led by guest speakers and student volunteers
•Require paper reading and online material learning
•Need to submit paper summaries and answer questions before each class
•One group project (open-ended, research oriented)
•No exam, quizzes, or homeworkWARNING: This Is A New Course•Lecture content is all newly developed
•Project topics are experimental
•We may run into hiccups here and there throughout the quarter
•Take it with your own discretion 
•But.. it could be useful for your job hunting, future work, future research,..Sample Turn-in of Paper Summary Upload your answer in PDF to Canvas
PipeDream: Generalized Pipeline Parallelism for DNN Training
Summary and your overall feeling of the paper: 2-5 sentences
Q1: By making the pipeline more smooth (less pipeline bubbles), what tradeoﬀ does PipeDream make? i.e., in what aspect is GPipe better than PipeDream?
A: 1-3 sentences
Q2: What type of parallelism do you think is most widely adopted in practice? Why?
A: 2-3 sentencesPaper Discussion LeadTo gain bonus points, you can volunteer to lead or co-lead paper discussion. Here are what you need to do for discussion lead: 
•Prepare slides that you will use to lead discussion. You can use slides that are available online, build slides yourself, or extend existing online slides with your own content. 
•Your should not only just explain the paper (using existing online slides) but also prepare questions that will stimulate class discussion. 
•Send draft slides to me at least 48 hours before the corresponding class start time. I will make comments and suggestions, which you should incorporate in the ﬁnal version. The sooner you send your draft slides to me, the more helpful feedback I can give you.Course Project•One research-oriented, open-ended project in groups of 3-5
•Be prepared to do substantial self-learning and hands-on work, start early! 
•You can use LLM and other tools, as long as they are not human help
•but cite what models/tools you use (this will not be viewed as a -, rather a +)
•Form your group by 10/6!Course Project Topics•Option 1: Eﬃcient Computer-Use Agent (ECUA) Competition
•Build an eﬃcient computer-use agent (CUA), compete on class leaderboard
•Option 2: Domain-Speciﬁc RAG Systems
•Build a RAG system specializing in a domain you pick
•Option 3: TBD, collecting feedback in this lecture
•Research with WukLab
•Work with WukLab PhD students and me on research projectMore research opportunities at WukLab and intern/FTE opportunities at GenseeAI!Option 1: Edge Computer-Use Agent (ECUA)•Build a computer-use agent (CUA) targeting edge devices (PC, handheld); the CUA should run fully locally and achieve high quality and low latency
•Phase 1: Create Evaluation Set
•At least 10 tasks with diﬀerent diﬃculty + your human reference solutions
•Phase 2: Build Your ECUA
•Build and evaluate your ECUA, write a ﬁnal report
•At the end of the course, we will build a leaderboard for all your ECUAsOption 2: Domain-Speciﬁc RAG Systems•Build a RAG system for a domain you choose
•Phase 1:
•Select a domain (e.g., ﬁntech, accounting, legal, healthcare, retail, manufacturing, and education)
•Create 10+ requests from 100+ data sources for your chosen domain
•Phase 2:
•Build the actual RAG and evaluate itOption 3: TBD•Context/memory management
•Runtime agent/workﬂow optimization
•Better web search
•Or only Option 1 and Option 2
•Voting time nowCourse Grading Scheme•25% Paper reading and class participation
•75% Project
•Phase 1: 15%    Oct 24
•Phase 1 writeup: 10%    Oct 24
•Phase 2: 25% based on agent/RAG evaluation score    Dec 10
•Final project report: 25%   Dec 10
•If you choose research track, grading will be primarily based on the ﬁnal report and presentation
•Up to 5pts Bonus (discussion lead)Tentative Schedule•1 week: intro and distributed training
•3 weeks: basic and advanced LLM inference and serving
•1 weeks: AI agent overview and use cases
•3 weeks: AI agent infrastructure and advanced AI agents
•2 weeks: other related materialsDiversity and Inclusion•Respect for Diversity: It is my intent that students from all diverse backgrounds and perspectives be well served by this course, that students’ learning needs be addressed both in and out of class, and that the diversity that students bring to this class be viewed as a resource, strength and beneﬁt.
•All people have the right to be addressed and referred to in accordance with their personal identity. In this class, we will have the chance to indicate the name that we prefer to be called and, if we choose, to identify pronouns with which we would like to be addressed...I will do my best to address and refer to all students accordingly and support classmates in doing so as well.Academic IntegrityAs a student at UCSD you are subject to the UCSD POLICY ON INTEGRITY OF SCHOLARSHIP , which enjoins you to respect the highest standards of honesty and integrity. All work that you submit in this course must be your own; unauthorized group eﬀorts are considered academic dishonesty. Academic dishonesty is a serious oﬀense which may result in suspension or expulsion from the university. Students are encouraged to report academic dishonesty to the instructor directly, or to the Academic Integrity Oﬃce.
Note: You are allowed to and in fact, encouraged to use AI assistance. The criteria in this course is to not use other human assistance outside of your group for your project, but feel free to use AI assistants and LLMs (e.g., cursor, ChatGPT). You should clearly say what AI tools and models you use for what tasks. This will be viewed as a + not -.Another Chance To Leave Now•If not, then your ﬁrst assignment is 
•First Day Survey: Prior Knowledge #FinAid
•Available as a quiz (ungraded survey) on Canvas 
•Only one question: what’s your name
•Due Oct 3rdWhat is LLM (Large Language Model)?•Language models 
•Text to text generation (i.e., text as input and text as output)
•that are large
•Massive number of model parameters (weights)
•Trained on huge amounts of data
How Do LLMs Learn?•Via a model architecture (think of a giant graph of interconnected nodes)
•e.g., Transformer, Mamba
•Model weights “trained” using a massive dataset
•e.g., Internet, books, codeWhat can LLMs do?•Text Generation: Writing emails, articles, poems, and even code.
•Translation: Translating languages with impressive accuracy.
•Summarization: Condensing long documents into key points.
•Question Answering: Providing answers to a wide range of questions.
•Conversational AI: Powering chatbots and virtual assistants.What can’t LLM do (well)?•Hallucination: can generate seemingly correct but incorrect information.
•Bias: can reﬂect biases present in their training data.
•Real-time info: no access to data present after their training
•Real-time info: no access to data present after their training
•Privacy: your personal info can be part of training data
•Security: LLMs can be manipulated to give/not give certain answersWhat is AI Agent?•From outside: an autonomous entity that can operate on its own
•What’s inside: a program involving model calls and often tool calls
•Diﬀerence from an LLM: can perceive env, make decisions, take actions
•Diﬀerence from an AI workﬂow: Autonomous and can “morph”Here is a famous picture from Lilian Weng (from OpenAI)What is LLM Agentshttps://gptpluginz.com/llm-agents/
https://lilianweng.github.io/posts/2023-06-23-agent/ƔLanguage Mastery:Their inherent capability to both comprehend and produce language ensures seamless user interaction.ƔDecision-making:LLMs are equipped to reason and decide, making them adept at solving intricate issues.ƔFlexibility:Their adaptability ensures they can be molded for diverse applications.ƔCollaborative Interactions:They can collaborate with humans or other agents, paving the way for multifaceted interactions.Why LLM Agents stand out?
Equipping Agents: The Power of Tooling•Tools: external functions, APIs, or even another (utility) agent (e.g., MCP server)
•Agents can often decide when to call tools and what tools to call
•Common tools
•Web search + crawling
•Browser
•Social media, email hooks
•Code + CLI executionAgent Memory: Knowledge, History, State•LLMs only have short-term memory (i.e., context window)
•Many agents needs long-term memory and/or internal/external knowledge
•Retrieving information for agents (Retrieval-Augemented Generation, RAG)
•Eﬀectiveness depends on both data (memory) and their retrievalDemo Time: Eigent Computer-Use Agent performing a Discord summarization taskWhat is Infrastructure (Systems)?•The backbone that powers applications
•Traditional infra (systems)
•OS, database, networking, hardware, compiler, security, virtualization, DevOps, etc.
•for data centers, cloud computing, edge computing, etc.
•Check my other CSE291 courses (virtualization and cloud computing)What is LLM Infra?•LLM training
•Pre-training (foundational model training)
•Post-training (ﬁne-tuning, RLHF)
•LLM serving
•Single-GPU/CPU LLM inference
•Distributed model serving
•LLMOps
•Training data collection, preparation, and synthesize
•Experimental tracking, model registry
•Monitoring and logging of LLM servingSource data from lifearchitect.ai, plotted by GPT-5Source data from lifearchitect.ai, plotted by GPT-5Source data from lifearchitect.ai, plotted by Gemini w/ Google Sheet, after multiple rounds of promptingWhat is AI Agent Infra?•Agent testing and evaluation
•Unit + e2e test, metrics, benchmarks, human-in-the-loop
•Agent autotuning and optimization
•Automated prompt tuning, model selection, tool selection, workﬂow optimization
•Agent hosting
•Serverless, long-running, stateful, stateless
•Tooling, memory, dataWhy Studying LLM and AI Agent Infra?•LLMs and agents are resource-hungry → training takes thousands of GPUs, inference is costly at scale.
•Infrastructure is the enabler of progress: without eﬃcient systems, even the best algorithms cannot be deployed.
•Opportunities for research breakthroughs: performance, scalability, security, energy eﬃciency.
•Unique edge for your future job hunting: cloud providers, enterprises, and startups all depend on robust AI/ML infraNext time…•AI agents overview
•No questions or summary for next time, will start from next Wed
•Work on the FinAid quiz (it will only take a couple of min)!
•Start looking for project teammates!!