11868/11968 LLM Systems
Distributed Training â€“
Model Parallelism
Lei Li
â€¢Model Parallel Training
â€¢Pipeline Parallelism
â€¢Tensor Parallelism
2Todayâ€™s TopicModel Parallelism
3Motivation: The size of models increases exponentially fast and large. 
It is no longer possible to fit these large models into the memory of a 
single GPU. 
TransformerGPT1GPT2GPT3GopherPALMGPT4
LLaMA3.1DeepSeek -v3
Qwen3
001101001,00010,000
2016 2017 2018 2019 2020 2021 2022 2023 2024 2025size(B)Model Parallel Training
4computation (forward/backward/update) of a model is 
distributed across multiple workers.
Distributed layer -wise computation Distributed tensor computation
F0F1F2F3
B0B1B2B3loss
device 3, layer 3
device 2, layer 2
device 1, layer 1
device 0, layer 0
grad updatePipeline Parallelism
5NaÃ¯ve Model Parallel: The model is distributed across multiple 
GPUs over layers. 
Any disadvantage?
all but one GPU is idle at any given moment!F0F1F2F3
B0B1B2B3loss
dev3, layer 3
dev2, layer 2
dev1, layer 1
dev0, layer 0
grad updateF0F1F2F3
B0B1B2B3
timedev3
dev2
dev1
dev0 updateupdateupdateupdatenccl send/ recv6Pipeline Parallelism Illustration
each device needs to calculate forward/backward, cache 
activations for the layers stored on the device.
https:// siboehm.com /articles/22/pipeline -parallel -training7
nccl send/ recvâ€¢Low GPU utilization
oat any point of time, only one device is working. others are idle. 
â€¢No interleaving of computation and communication
oWhile sending intermediate results to next device, GPUs are idle.
â€¢High memory demand
o1st GPU needs to store all activations until the whole batch 
completes. 
8Limitations of NaÃ¯ve Pipeline Parallelâ€¢Key idea: Divides input data batches into smaller micro -
batches, pipelining microbatches .
â€¢A batch is usually decided by GPU memory size and 
memory needed for one data sampleâ€™s forward/backward
oas large as possible to fill the GPU memory
â€¢A mciro -batch can be smaller
9Pipeline Parallel: GPipe
[1] Huang, Yanping , et al. " Gpipe : Efficient training of giant neural networks using pipeline parallelism." Advances in neural information processing systems 32 (2019).10Pipeline Parallelism â€“ Micro -batching
GPipe : Divides input data mini -batches into smaller micro -batches.
Finishing all forward before starting backward for micro -batches.
(i)the number of model partitions K (i.e. number of devices)
(ii)the number of micro -batches M
(iii)the number of model layers: L
[1] Huang, Yanping , et al. " Gpipe : Efficient training of giant neural networks using pipeline parallelism." Advances in neural information processing systems 32 (2019).11Pipeline Parallelism: Microbatch  Pipelining
GPipe : Divides input mini -batches into smaller micro -batches.
During backward, recomputes forward 
Bubble overhead: ğ‘‚(ğ¾âˆ’1
ğ‘€+ğ¾âˆ’1) could be negligible when ğ‘€>4Ã—ğ¾
Communication overhead: transfer activation tensors at the partition boundaries
Peak activation memory: ğ‘‚(ğ‘Ã—ğ¿) â” ğ‘‚(ğ‘+ğ¿
ğ¾Ã—ğ‘
ğ‘€) with gradient checkpoint (later)
12GPipe  Performance
Normalized training throughput using Gpipe  with different # 
of partitions K and different # of micro -batches M on TPUs 
and GPUs without high -speed interconnect.
13Reduce PP Memory Cost: Gradient Checkpointing
Re-materialization
â€¢Forward pass: each accelerator only stores output activations
â€¢Backward pass: the k â€“th accelerator recomputes the composite 
forward function Fk
Vanilla backprop                                         Memory poor backprop
[1] Chen, Tianqi, et al. "Training deep nets with sublinear memory cost." arXiv  preprint arXiv:1604.06174 (2016).
[2] https://github.com/cybertronai/gradient -checkpointing
â–ªMemory for activations: O(n)
â–ªNode computation: O(n)â–ªMemory for activations: O(1)
â–ªNode computation: O(n2)14Gradient Checkpointing
Gradient checkpoint
â€¢Cash the activations of every sqrt(n) layers
â€¢Memory for activations: O(n)
â€¢Node computation: O(sqrt(n) * sqrt(n)) = O(n)
[1] Chen, Tianqi, et al. "Training deep nets with sublinear memory cost." arXiv  preprint arXiv:1604.06174 (2016).
[2] https://github.com/cybertronai/gradient -checkpointing
15Limitations of Pipeline Parallel
finishing all forward before backward on micro -batches.
number of micro -batches in -flight (completed forward but not 
backward): 8 in this example
need to store all the activations for these micro -batches16Improving Pipeline Parallel with 1F1B flush
â€¢PipeDream -Flush 1F1B â€“ start backward as soon as possible
â€¢benefit: reduce memory cost for storing the activations
â€¢number of micro -batches in flight (completed forward but not 
backward): at most 4 (assuming 1B=2F) vs 8 in GPipe .
PipeDream :generalized pipeline parallelism for DNN training. Narayanan et al. SOSP 2019.17Further Improving Pipeline Parallel by Chunking 
Model Layers and Interleaving Stages
Efficient Large -Scale Language Model Training on GPU Clusters Using Megatron -LM. Narayanan et al SC 2021.Chunk 1 Layers Chunk 2 Layers
Device 1 1, 2 9, 10
Device 2 3, 4 11, 12
Device 3 5, 6 13, 14
Device 4 7, 8 15, 16
Complete smaller computation for each chunk and 2 stages of F/BImplementing GPipe  Parallelism 
18def minibatch_steps (self):
    yield  [ZeroGrad ()]
    # STAGE 1: First, we FWD all microbatches
    for microbatch_id  in range (self.num_micro_batches ):
        yield  self.steps_FWD_microbatch (microbatch_id )
    # at this position, all microbatches  are in flight and
    # memory demand is highest
    # STAGE 2: Then, we BWD all microbatches
    for microbatch_id  in reversed (range (self.num_micro_batches )):
        yield from  self.steps_BWD_microbatch (microbatch_id )
    # updating the weights is the last step of processing any batch
    yield  [OptimizerStep ()]
full code in https:// github.com /siboehm /shallowspeed19def steps_FWD_microbatch (self, microbatch_id ):
    cmds  = []
    if self.is_first_stage :
        # first pipeline stage loads data from disk
        cmds .append (LoadMicroBatchInput (microbatch_id =microbatch_id ))
    else:
        # all other stages receive activations from prev  pipeline stage
        cmds .append (RecvActivations ())
    cmds .append (Forward( microbatch_id =microbatch_id ))
    if not self.is_last_stage :
        # all but the last pipeline stage send their output to next stage
        cmds .append (SendActivations ())
    return  cmds20def steps_BWD_microbatch (self, microbatch_id ):
    cmds  = []
    if self.is_last_stage :
        # last pipeline stage loads data from disk
        cmds .append (LoadMicroBatchTarget (microbatch_id =microbatch_id ))
    else:
        # all other stages wait to receive grad from prev  stage
        cmds .append (RecvOutputGrad ())
    # the first microBatch  is the lasted one that goes through backward pass
    if self.is_first_microbatch (microbatch_id ):
        # interleaved backprop and AllReduce  during last microBatch  of BWD
        cmds .append (BackwardGradAllReduce (microbatch_id =microbatch_id ))
    else:
        cmds .append (BackwardGradAcc (microbatch_id =microbatch_id ))
    if not self.is_first_stage :
        # all but last pipeline stage send their input grad to prev  stage
        cmds .append (SendInputGrad ())
    yield  cmdstorch.distributed.pipelining
â€¢It consists of two stages
obuild PipelineStage
â–ªmanually splitting the model
â–ªsplitting model automatically
ousePipelineSchedule for execution
21Pipeline Parallelism in pytorch22class  Transformer (nn.Module ): 
  def __init__(self, model_args : ModelArgs ): 
    super ().__init__() 
    self.tok_embeddings  = nn.Embedding (...) 
    # Using a ModuleDict  lets us delete layers without affecting names,  ensuring checkpoints will correctly save and 
load.  
    self.layers  = torch.nn.ModuleDict () 
    for layer_id  in range (model_args.n_layers ):  
      self.layers [str(layer_id )] = TransformerBlock (...) 
    self.output  = nn.Linear (...) 
  def forward (self, tokens:  torch.Tensor ): 
    # Handling layers being 'None' at runtime enables easy pipeline splitting  
    h = self.tok_embeddings (tokens)  if self.tok_embeddings  else tokens  
    for layer  in self.layers.values (): 
      h = layer(h,  self.freqs_cis ) 
    h = self.norm (h) if self.norm  else h 
    output  = self.output (h).float()  if self.output  else h 
    return  output
https://pytorch.org/docs/main/distributed.pipelining.html  23from  torch.distributed.pipelining  import  PipelineStage  
with  torch.device ("meta" ): 
  assert  num_stages  == 2, "This is a simple 2 -stage example"  
  # we construct the entire model, then delete the parts we do not need for this stage  # in practice, this can 
be done using a helper function that automatically divides up layers across stages.  
  model  = Transformer()  
  if stage_index  == 0: # prepare the first stage model  
    del model.layers ["1"] 
    model.norm  = None  
    model.output  = None  
  elif stage_index  == 1: # prepare the second stage model    
    model.tok_embeddings  = None  
    del model.layers ["0"] 
  stage  = PipelineStage (model,  stage_index , num_stages , device)  24from  torch.distributed.pipelining  import  ScheduleGPipe  
# Create a schedule  
schedule  = ScheduleGPipe (stage,  n_microbatches ) 
# Input data (whole batch)  
x = torch.randn (batch_size , in_dim , device=device)  
# Run the pipeline with input `x`  # `x` will be divided into microbatches  automatically  
if rank  == 0: 
  schedule.step (x) 
else:  
  output  = schedule.step ()Tensor Parallelism
2627Tensor Parallelism â€“ spliting  the matrix 
computation
28Tensor Parallelism for FFN (big mat mul)
All-reduce (to compute XA) is needed ï¼
Splitting X and A29Tensor Parallelism for FFN
All-reduce is not needed (for Y) ï¼
Splitting weight A
ğ´=ğ´1,ğ´2, weight B
ğµ=ğµ1
ğµ2, 
[ğ‘Œ1,ğ‘Œ2]=GeLU (ğ‘‹âˆ™ğ´1),GeLU (ğ‘‹âˆ™ğ´2), 
ğ‘=ğ‘Œ1âˆ™ğµ1+ğ‘Œ2âˆ™ğµ230Tensor Parallelism for Self -Attention
â€¢Split weights over columns (heads)
â€¢All-reduce is not needed ï¼
â€¢Input embedding
â€¢Split over columns
â€¢all-reduce is required
â€¢Output embedding
â€¢Split over columns
â€¢Fuse outputs with 
cross -entropy loss 
(huge reduction in 
communication)
â€¢all-gather is needed
31Tensor Parallelism - Embeddings
â€¢Layer normalization, dropout, 
residual connections
â€¢Duplicate across GPUs
â€¢Each model parallel worker 
optimizes its own set of 
parameters
32Tensor Parallelism
33Combination of Pipeline and Tensor 
Model Parallelism
#PP and #TP depend on model architecture and 
GPU server config34â€¢Takeaway #1: When considering different forms of model parallelism, tensor 
model parallelism should generally be used up to degree ğ‘” when using ğ‘”-GPU 
servers, and then pipeline model parallelism can be used to scale up to larger 
models across servers â” TP for in -node parallel computingCombination of Pipeline and Tensor 
Model Parallelism
35â€¢Takeaway #2: When using data and model parallelism, a total 
model -parallel size of ğ‘€ = ğ‘¡ Â· ğ‘ should be used so that the modelâ€™s 
parameters and intermediate metadata fit in GPU memory; data 
parallelism can be used to scale up training to more GPUs.Model Parallel + Data Parallel
â€¢Pipeline Parallelism
osplit by layers (horizonal split) 
oeliminate the bubbles (idle)
ointerleaving forward/backward
â€¢Tensor Parallelism
osplit the matrix computation
38Summary of Model Parallel Training