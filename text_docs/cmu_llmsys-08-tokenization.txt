11868 LLM Systems
Tokenization
Lei Li
â€¢Pretraining: Mask -labeled recovery of random spans + next 
token prediction
â€¢Multitask supervised fine -tuning with instruction templates
â€¢Relative position instead of absolute position: Rotary 
positional embedding
â€¢Smooth activation: SwishGLU
â€¢Sparse attention patterns
2Recap â€“ Key Ideas in Modern Pre -trained 
LLMsâ€¢Subword  tokenization: Byte -Pair-Encoding 
oCode walk through
â€¢Information -theoretic vocabulary (VOLT)
â€¢Practical Considerations in LLM
â€¢Vocabulary sharing and impact on multilingual performance
â€¢Tokenizer -free model (BLT)
3OutlineMany words donâ€™t map to one token: indivisible.
tokenize rTokenizer â€“ split text into basic units
embedding table lookup
4â€¢Word -level Tokenization
oBreak by space and punctuation.
oEnglish, French, German, Spanish
oSpecial treatment: numbers replaced by special token [number]
oHow large is the Vocabulary? Cut -off by frequency, the rest 
replaced by [UNK]Simple Tokenization â€“ Word -level
The  most  eager  is  Oregon  which  is  enlisting  5,000  drivers  in  the  countryâ€™s  biggest  experiment. 
5from collections import defaultdict
def build_word_dict (file_path ):
    word_dict  = defaultdict (int)
    with open( file_path , 'r', encoding='utf -8') as file:
        for line in file:
            words = line.split ()  # Split by spaces
            for word in words:
                word_dict [word] += 1
    
    return word_dict
6Vocabulary â€“ simple exampleHow many words?
Bobâ€™s handyman isado-it-yourself kinda guy, isnâ€™the?What is a word?
Clitic
noun -noun 
compound
multi -word 
expression
contraction
7â€¢Orthographic definition
ostrings separated by white spaces
ospoken language: units corresponding to written word separated 
by pause 
oproblem: Bobâ€™s handy man  is a do-it-yourself  kinda  guy, isnâ€™t he?
â€¢What about languages that do not use white spaces?
ä»–æ˜¨å¤©æ™šä¸Šå»çœ‹äº†æ¶ˆå¤±çš„å¥¹
he yesterday night watched lost in starsWords
8â€¢Easy to implement
â€¢Cons:
oOut-of-vocabulary (OOV) or unknown tokens, e.g. Covid
oTradeoff between parameters size and unknown chances. 
â–ªSmaller vocab => fewer parameters to learn, easier to generate (deciding 
one word from smaller dictionary), more OOV
â–ªLarger vocab => more parameters to learn, harder to generate, less OOV
oHard for certain languages with continuous script: Japanese, 
Chinese, Korean, Khmer, etc. Need separate word segmentation 
tool (can be neural networks)Pros and Cons of Word -level Tokenization
9â€¢Each letter and punctuation is a token
â€¢Pros:
oVery small vocabulary (except for some languages, e.g. Chinese)
oNo Out -of-Vocabulary token
â€¢Cons:
oA sentence can be longer sequence
oTokens do not representing semantic meaningCharacter -level Tokenization
T
 h
 e
 m
 o
 s
 t
 e
 a
 g
 e
 r
 i
 s
 O
 r
 e
 g
 â€¦
10â€¢Goal:
omoderate size vocabulary
ono OOV
â€¢Idea: 
orepresent rare words (OOV) by sequence of subwords
â€¢Byte Pair Encoding (BPE)
onot necessarily semantic meaningful
oOriginally for data compressionSubword -level Tokenization
The  most  eager  is  Oregon  which  is  enlisting  5,000  driver s  in  the  country â€™s  big g estexperiment. 
Philip Gage. A New Algorithm for Data Compression, 1994
111.Initialize vocabulary with all characters as tokens (also add 
end-of-word symbol) and frequencies
2.Loop until vocabulary size reaches capacity
1)Count successive pairs of tokens in corpus
2)Rank and select the top frequent pair
3)Combine the pair to form a new token, add to vocabulary
3.Output final vocabularyByte Pair Encoding (BPE) for Text
Building Vocabulary
Rico Sennrich et al. Neural Machine Translation of Rare Words with Subword Units. 2016121.starting from chars
2.repeatedly, merge most frequent pairs to form new tokens
3.until reaching a fixed size. Byte-Pair-Encoding Tokenization
cat 90
catch 50
rat 80
rattle 40freq. raw word a
c
e
h
l
t
ata
c
e
h
l
t
at
cata
c
e
h
l
t
at
cat
rata
c
e
h
l
t
at
cat
rat
catc
Neural Machine Translation of Rare Words with Subword Units. Sennrich et al. ACL 2016merge(
â€˜aâ€™, â€™tâ€™)merge
(â€˜câ€™, â€™atâ€™)merge
(â€˜râ€™, â€™atâ€™)merge
(â€˜catâ€™, â€™câ€™)a
c
e
h
l
t
13â€¢Split text by space or other delimiters 
â€¢Repeat
ogreedy find the longest prefix that matches a token in BPE 
dictionary
osplit and process the remaining parts until no more text left
14BPE Tokenizationâ€¢https://github.com/llmsystem/llmsys_code_examples/blob/m
ain/tokenization/tokenization.ipynb  
15Code Exampleâ€¢Subword  tokenization: Byte -Pair-Encoding 
oCode walk through
â€¢Information -theoretic vocabulary (VOLT)
â€¢Practical Considerations in LLM
â€¢Vocabulary sharing and impact on multilingual performance
â€¢Tokenizer -free model (BLT)
16Outline
â€¢Compression
oaverage number of bytes per token
ğµğ‘ƒğ‘‡=#utf8bytes
#tokens
onormalized sequence length
ğ‘ğ‘†ğ¿=#ğ‘¡ğ‘œğ‘˜ğ‘’ğ‘›ğ‘ 
#ğ‘¡ğ‘œğ‘˜ğ‘’ğ‘›ğ‘ ğ‘–ğ‘›ğ¿ğ¿ğ‘ğ‘€ğ´
onormalized entropy (next)
17Measuring VocabularyFind Optimal Vocabulary
Normalized Entropy
Which one leads to better NLG/MT performance? 
Repeated full training and testing are required to find the optimal vocabulary!(BPE -
Search)Numerous possible vocabularies at the sub -word level. 
Size 
Vocab 
1k tokens
V ocab 
10k tokens
V ocab 
30k tokens
Xu, Zhou, Gan, Zheng, Li. Vocabulary Learning via Optimal Transport for Neural Machine Translation. ACL 2021a.
18â€¢Normalized Entropy â„‹ğ‘£=âˆ’1
ğ‘™ğ‘£Ïƒğ‘–âˆˆğ‘£ğ‘ƒğ‘–logğ‘ƒ(ğ‘–)
ğ‘™ğ‘£: average number of chars for vâ€™s all tokens
â€¢It measures semantic -information -per-char
oSmaller is favorable. Less ambiguity and easy to generate
Token count
a 200
e 90
c 30
t 30
s 90Token count
a 100
aes 90
cat 30
â„‹ğ‘£=0.14 â„‹ğ‘£=1.37
token prob.VOLT: Using entropy to learn vocabulary
19â€¢Value: Normalized Entropy 
â€¢Cost: Size 
â€¢Marginal Utility of information for Vocabulary (MUV)
oğ‘€ğ‘£ğ‘˜â†’ğ‘£ğ‘˜+ğ‘š=âˆ’ğ»(ğ‘£ğ‘˜)âˆ’ğ»(ğ‘£ğ‘˜+ğ‘š)
ğ‘š
oNegative gradients  of normalized entropy to size
oHow much value each token bringsMUV: Utility of Information for Adding 
Tokens
Xu, Zhou, Gan, Zheng, LeiLi. Vocabulary Learning via Optimal Transport for Neural Machine Translation. ACL 2021a20â€¢Cost -effective point in MUV curve 
omaximum MUV => best BLEUMUV is good indicator for MT performance 
Xu, Zhou, Gan, Zheng, LeiLi. Vocabulary Learning via Optimal Transport for Neural Machine Translation. ACL 2021a21â€¢MUV and BLEU are correlated on two -thirds of tasks
â€¢A good coarse -grained evaluation metricMUV Indicates MT Performance
MUVMUV
Xu, Zhou, Gan, Zheng, LeiLi. Vocabulary Learning via Optimal Transport for Neural Machine Translation. ACL 2021a22â€¢Transport character occurrences to token occurrences
â€¢Maximizing MUV for vocabulary
oğ‘šğ‘ğ‘¥âˆ’(ğ»(ğ‘‰ğ‘¡+1)âˆ’ğ»(ğ‘‰ğ‘¡))
â€¢Instead, maximizing the lower bound ==> Optimal Transport
oğ‘šğ‘ğ‘¥
ğ‘¡(ğ‘šğ‘ğ‘¥ğ»(ğ‘‰ğ‘¡)âˆ’ğ‘šğ‘ğ‘¥ğ»(ğ‘‰ğ‘¡+1))VOLT: Vocabulary Building via 
Transportation
all candidate tokens (e.g. 100k)
Xu, Zhou, Gan, Zheng, LeiLi. Vocabulary Learning via Optimal Transport for Neural Machine Translation. ACL 2021a23â€¢The vocabulary with the maximum MUV
oMaximum gap between IPC of a vocabulary (with size t) 
and that of a smaller vocabulary (with size <t)
oğ‘šğ‘ğ‘¥âˆ’(ğ»(ğ‘‰ğ‘¡+1)âˆ’ğ»(ğ‘‰ğ‘¡))
â€¢Intractable, instead to maximize lower -bound
â€¢==> ğ‘šğ‘ğ‘¥
ğ‘¡(ğ‘šğ‘ğ‘¥ğ»(ğ‘‰ğ‘¡)âˆ’ğ‘šğ‘ğ‘¥ğ»(ğ‘‰ğ‘¡+1))
â€¢Finding ğ‘šğ‘ğ‘¥
ğ‘£ğ»(ğ‘£)==> Optimal TransportReducing MUV Optimization to OT
Xu, Zhou, Gan, Zheng, Li. V ocabulary Learning via Optimal Transport for Neural Machine Translation. ACL 2021.24â€¢Entropy -regularized Optimal Transport
â€¢Sinkhornâ€™s  algorithm (from [ Sinkhorn  1967])VOLT: Finding the Optimal Vocabulary
Pa,a Pa,ab Pa,bc
Pb,a Pb,ab Pb,bc
Pc,a Pc,ab Pc,bca
c
Cost matrix DTransportation matrix ğ‘ƒ
babc ab
0 âˆ
âˆ
âˆ âˆa
cbabc abğ‘šğ‘–ğ‘›
ğ‘ƒâˆˆâ„ğ‘šÃ—ğ‘›âŸ¨ğ·,ğ‘ƒâŸ©âˆ’ğ»(ğ‘ƒ)
âˆ€ğ‘–âˆˆChar,Ïƒ
ğ‘—âˆˆğ‘‰ğ‘›ğ‘ƒğ‘–,ğ‘—=ğ‘ƒÌ‚
(ğ‘–)
âˆ€ğ‘—âˆˆğ‘‰ğ‘›,|Ïƒ
ğ‘–âˆˆCharğ‘ƒğ‘–,ğ‘—âˆ’ğ‘ƒÌ‚
(ğ‘—)|=ğœ–subject to
ln2ln2
ln2
ln2TokChar
TokChar
Xu, Zhou, Gan, Zheng, LeiLi. Vocabulary Learning via Optimal Transport for Neural Machine Translation. ACL 2021a25â€¢VOLT uses a greedy strategy to encode text with a 
constructed sub -word level vocabulary similar to BPE.
â€¢The vocabulary includes all basic characters.
oTo encode text, it first splits sentences into character -level tokens. 
oThen, we merge two consecutive tokens into one token if the 
merged one is in the vocabulary solved by OT. 
oThis process keeps running until no tokens can be merged. 
oOut-of-vocabulary tokens will be split into smaller tokens. Encoding and Decoding with VOLT
26â€¢Subword  tokenization: Byte -Pair-Encoding 
oCode walk through
â€¢Information -theoretic vocabulary (VOLT)
â€¢Practical Considerations in LLM
â€¢Vocabulary sharing and impact on multilingual performance
â€¢Tokenizer -free model (BLT)
27Outline
â€¢deduplication
â€¢sentencepiece
â€¢Code (programming languages)
â€¢Numbers
â€¢multilingual
28Practical Consideration in LLMsâ€¢LLaMA  3 deduplicate at 
ourl level dedup
odocument level dedup  using minHash
oline level dedup  using SHA -1 64 bit hash code for every 30m 
docs
â–ªto remove boilerplate, e.g. navigation menu, cookie warning, contact info
â€¢Filter
on-gram repeats in one line
oâ€œdirty word â€ counting
otoken distribution KL divergence too different from corpus29Corpus Deduplication and Filteringâ€¢BBPE: byte -level BPE (universal for all languages)
â€¢Wordpiece : 
olike BPE
obut instead of merge with most frequent pairs, merge a and b, if 
p(b|a) will be maximized
â€¢SentencePiece :
oUniform way to treat space, punctuation 
oUse the raw sentence, replacing space â€˜ â€™ with _ (U+ 2581 )
oThen split character and do BPEMore Subword  Tokenization
Kudo and Richardson, SentencePiece , 2018 
3031Handling Code: Pre -tokenization
Using regular expression to split the sequences 
example: .append  => a single token
Dagan et al. Getting the most out of your tokenizer for pre -training and domain adaptation. ICML 2024 . 32Handling numbers: Enable math in LLM
xVal: A Continuous Numerical Tokenization for Scientific Language Models. Golkar  et al 2023.â€¢32k ( LLaMA  2) â” 128k tokens ( LLaMA  3.1)
o100k from openAIâ€™s  tiktoken  (from original 200k)
o28k allocated to multilingual
33LLaMA3â€™s multilingual vocabulary
https://www.icodeformybhasa.com/p/exploring -multilingual -aspects -and â€¢combining documents from multiple ( 176 in LLaMA 3) 
languages, about 8% of total text, then apply BPE on the 
joint corpus
â€¢obtain the same amount of BPE token for each language 
and then merge. 
â€¢allocating the capacity of each language by balancing the 
average log probability (ALP)
34How to construct multilingual vocabulary?
Zheng et al. Allocating large vocabulary capacity for crosslingual  language model pre -training. EMNLP 2021.
Liang et al. XLM -V: Overcoming the V ocabulary Bottleneck in  Multilingual Masked Language Models. EMNLP2023.https://belladoreai.github.io/llama -tokenizer -js/example -
demo/build/
https://koala.sh/tools/free -gpt-tokenizer
35Demoâ€¢Subword  tokenization: Byte -Pair-Encoding 
oCode walk through
â€¢Information -theoretic vocabulary (VOLT)
â€¢Practical Considerations in LLM
â€¢Vocabulary sharing and impact on multilingual performance
â€¢Tokenizer -free model (BLT)
36Outline
37Vocabulary Sharing
English: television       Spanish: televisiÃ³n
French: television         Italian: television
Dutch: televisie             Portuguese: televisÃ£o
Swedish: television Finnish: televisioâ€¢Construct a small instruction -finetuning dataset using 10k 
bilingual parallel data
â€¢Finetune LLaMA -7B
â€¢Examine the translation performance of
oThe supervision bilingual direction (bilingual)
oAll other directions (multilingual)
38Embedding Finetuning for LLM
How Vocabulary Sharing Facilitates Multilingualism in LLaMA ? Yuan et al, ACL 
202439Does embedding FT promote bilingual & 
multilingual translation performance? 
QuadrantPerformance
Case Languages
Bilingual Multilingual
Reciprocal â†‘ â†‘ cs, da, fr, de
Altruistic â†“ â†‘ ar, vi, zh, ko
Stagnant â†“ â†“ Km, lo, gu, te
Selfish â†‘ â†“ hi
How V ocabulary Sharing Facilitates Multilingualism in LLaMA ? Y uan et al, ACL 2024Fine-tuning 
on bilingual 
data does 
not always 
bring 
benefits to 
supervised 
direction!
40
ar, vi, zh, ko
How Vocabulary Sharing Facilitates Multilingualism in LLaMA ? Yuan et al, ACL 
2024â€¢Byte-BPE (BBPE) produces longer byte level token 
sequence than the number of characters
â€¢é¥• [tÄo] (gluttonous) â” three tokens [227, 234, 260]
â€¢Implication for improvement:
oshortening: remove the common prefix 227
41Stagnant Quadrant â€“ Over -tokenization
How V ocabulary Sharing Facilitates Multilingualism in LLaMA ? Y uan et al, ACL 202442Stagnant Quadrant: expanding vocab
  
shortening 
05101520
enâ†’km enâ†’lo enâ†’gu enâ†’te
Full Tuning Extend Vocab Shorten
How V ocabulary Sharing Facilitates Multilingualism in LLaMA ? Y uan et al, ACL 202443Tokenizer -free Model â€“ Byte Latent 
Transformer
Pagnoni  et al. Byte Latent Transformer: Patches Scale Better Than Tokens. 2024.â€¢Subword  tokenization: Byte -Pair-Encoding
oiteratively merging most frequent pairs of tokens
â€¢Information -theoretic vocabulary (VOLT)
osolving entropy constrained optimal transport problem
â€¢Pre-tokenization through regex
â€¢Number treatment
â€¢Vocab sharing impact multilingual performance
ohow to solve languages in stagnant quad 44Summaryhttps://canvas.cmu.edu/courses/44373/quizzes/140013  
45Quiz 5