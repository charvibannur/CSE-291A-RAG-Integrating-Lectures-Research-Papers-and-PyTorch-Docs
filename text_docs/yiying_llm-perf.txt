Understanding LLM PerformanceYiying ZhangTool use: Æ”The agent learns to call external APIsfor extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.What is LLM Agentshttps://gptpluginz.com/llm-agents/
https://lilianweng.github.io/posts/2023-06-23-agent/
MCP (Model Context Protocol)â—Connecting (N) LLMs to (M) external tools/resources used to be a NxM problem â—MCP standardizes the LLM-tool communication into a N->1->M process â—Build with a client-server model â—MCP client: the agent that needs to call tool/data â—MCP server: a service to expose external tools and data sources
AI Agent/Workflow Frameworksâ—Frameworks initially proposed to standardize AI workflows, provide some out-of-box design patterns and abstractions â—Some examples â—LangChain: Came out the earliest, probably the most popular and hardest to use â—LlamaIndex: Good RAG support â—CrewAI and Camel: multi-agent framework for more complex tasks â—But a lot of unnecessary, added complexity for agents, harder to customize â—My experience of whatâ€™s the easiest and sufficient for many tasks â—No framework (pure Python) â—No MCP (can just write your own functions or hooks) â—No A2A (no need for multi-agent)What is AI Agent Infra?â—Agent testing and evaluation â—‹Unit + e2e test, metrics, benchmarks, human-in-the-loop â—Agent autotuning and optimization â—‹Automated prompt tuning, model selection, tool selection, workflow optimization â—Agent hosting â—‹Serverless or long-running?  â—‹Stateful or stateless? â—Tooling, memory, dataDemo Time: Eigent Computer-Use Agent performing a Discord summarization taskOutlineâ—Transformer primer â—Introduction oriented for LLM infra (perf problems), not the theory â—LLM performanceSelf Attention
source: https://jalammar.github.io/illustrated-transformer/
Multi-headed Attention
source: https://jalammar.github.io/illustrated-transformer/Transformer Model
source: https://jalammar.github.io/illustrated-transformer/Inference process of LLMs
Layer 1Layer N
Artificialthe
thefuture
Layer 1Layer NLayer 1Layer N
futureof
11IntelligenceisInputOutputâ€¦â€¦â€¦Repeat until the sequence â—Reaches its pre-defined maximum length (e.g., 2048 tokens) â—Generates certain tokens (e.g., â€œ<|end of sequence|>â€)//0LQIHUHQFHEDFNJURXQG
/HJHQGÆ”<HOORZSURPSWWRNHQÆ”%OXHJHQHUDWHGWRNHQÆ”5HGHQGRIVHTXHQFHWRNHQ,WHUDWLYHHDFKIRUZDUGSDVVJHQHUDWHVDVLQJOHWRNHQ$XWRUHJUHVVLYHJHQHUDWLRQFRQVXPHVSURPSWWRNHQVSUHYLRXVO\JHQHUDWHGWRNHQV&RPSOHWLRQSRWHQWLDOO\GHFLGHGE\PRGHO$JHQHUDWHGWRNHQFDQEHWKHHQGRIVHTXHQFHWRNHQ+RZGRHVWH[WJHQHUDWLRQZRUN"
Preï¬ll and Decoding Stagesâ€¢Preï¬ll: processing model input (all in one forward pass)
â€¢Decode: generating output token, one at a time
source: LLM Inference Serving: Survey of Recent Advances and OpportunitiesKV Cacheâ€¢KV Cache: stores Key (K) and Value (V) tensors computed at each transformer layer during inference to avoid recompilation
â€¢Preï¬ll: store computed KVs of input sequence in KV cache
â€¢Each iteration in decode phase: each new token only needs to attend to cached KV states + the latest token
 Question: what makes KV cache big?
source: https://developer.nvidia.com/blog/mastering-llm-techniques-inference-optimization[recap] What is LLM (Large Language Model)?â€¢Language models 
â€¢Text to text generation (i.e., text as input and text as output)
â€¢that are large
â€¢Massive number of model parameters (weights)
â€¢Trained on huge amounts of data
Source data from lifearchitect.ai, plotted by GPT-5Source data from lifearchitect.ai, plotted by GPT-5Popular LLM-powered services
18ProgrammingDev toolsChatSales&MarketingCustomer Service
LLM Endpoints or Hosted LLM servers
What is LLM Infra?â€¢LLM training
â€¢Pre-training (foundational model training)
â€¢Post-training (ï¬ne-tuning, RLHF)
â€¢LLM serving
â€¢Single-GPU/CPU LLM inference
â€¢Distributed model serving
â€¢LLMOps
â€¢Training data collection, preparation, and synthesize
â€¢Experimental tracking, model registry
â€¢Monitoring and logging of LLM serving6\VWHPVFKDOOHQJHVWKDWLQFUHDVHFRVWÆ”6L]HRI//0SDUDPHWHUV!!VL]HRI//0GDWDÅ¼/ODPD%a*%WRVWRUHIORDWSDUDPHWHUVÅ¼[$*%WRVWRUH[$*%WRPD[LPL]HWKURXJKSXWÆ”0HPRU\,2KXJHIDFWRULQODWHQF\Å¼)RUDVLQJOHWRNHQKDYHWRORDG*%WRFRPSXWHFRUHVÅ¼&38PHPRU\,2a *%VÅ¼*38PHPRU\,2a *%V$*%Æ”+LJKWKURXJKSXWUHTXLUHVPDQ\)/236Å¼&38FDQGRUHDOWLPHJHQHUDWLRQRIDVLQJOHVHTXHQFHÅ¼*38FDQGRUHDOWLPHJHQHUDWLRQIRUPDQ\VHTXHQFHV
)URPWKH)ODVK$WWHQWLRQSDSHUKWWSVDU[LYRUJSGISGIWhat does â€œPerformanceâ€ mean in LLM serving?â€¢Not quality â€œperformanceâ€: in this class and in systems context in general, performance means temporal performance, not quality or accuracy
â€¢ Time To First Token (TTFT): Queueing time + reï¬ll time + one token decoding: How quickly users start seeing a response, i.e., response time
â€¢Time Per Output Token (TPOT): Time to generate each additional output token. Average TPOT x output token length is the time users see all the response after seeing the ï¬rst token â€¢Latency: The overall request time it takes for the model to generate the full response for a user. latency =Â (TTFT)Â +Â (TPOT)Â * (the number of tokens to be generated)
â€¢Throughput: The number of output tokens per second an inference server can generate across all users and requestsQuestionsâ€¢In what scenario/use cases is TTFT (TPOT) more important? â€¢What is the latency-throughput tradeoï¬€? â€¢What are the factors that aï¬€ect diï¬€erent LLM performance metrics?(Nvidia) GPU and CUDA Primer
source: https://developer.nvidia.com/blog/cuda-refresher-cuda-programming-model/ 
(Nvidia) GPU and CUDA Primer
source: https://jonathan-hui.medium.com/ai-chips-a100-gpu-with-nvidia-ampere-architecture-3034ed685e6e Preï¬ll vs. Decode Resource Demandâ€¢GPU has two general types of computing resources:
â€¢GPU kernel and GPU memory
â€¢Is preï¬ll compute or memory bound?
â€¢Is decoding compute or memory bound?Common Techniques to Improve LLM Inference Perfâ€¢KV Cacheâ€¢Operator Fusion:Â Combining different adjacent operatorsâ€¢Quantization:Â Use fewer bits for weights and activationsâ€¢Compression:Â Sparsity or distillationâ€¢Parallelization:Â Tensor parallelism across multiple devices or pipeline parallelism for larger models.â€¢More later this quarterModel Bandwidth Utilization (MBU)â€¢MBU = (achieved memory bandwidth) / (peak memory bandwidth) 
â€¢achieved memory bandwidth is ((total model parameter size + KV cache size) / TPOT)
â€¢Model FLOPs Utilization (MFU) = (observed throughput (TPOT)) / (peak GPU FLOPs)
â€¢MBU important for  memory-bound computation, MFU important for compute-bound computation. Ideally, want both to be 100%
â€¢batch size: how many number of requests are sent to GPU for model forwarding at the same time (in parallel)
source: https://www.databricks.com/blog/llm-inference-performance-engineering-best-practices 
source: https://huggingface.co/blog/tngtech/llm-performance-prefill-decode-concurrent-requests source: https://www.databricks.com/blog/llm-inference-performance-engineering-best-practices Figure 2: Empirically observed MBU for different degrees of tensor parallelism with TensorRT-LLM on A100-40G GPUs. Requests: sequences of 512 input tokens with a batch size of 1.source: https://www.databricks.com/blog/llm-inference-performance-engineering-best-practices 
Figure 3: Empirically observed MBU for different batch sizes and tensor parallelism modes on H100-80G GPUs. Requests: sequences of 512 input tokenssource: https://www.databricks.com/blog/llm-inference-performance-engineering-best-practices Figure 7: Throughput latency curve for the MPT-7B model. This allows users to pick a hardware conï¬guration that meets their throughput requirements under a latency constraint.Backup Slide: Googleâ€™s TPU (Tensor Processing Unit)
Source: Jouppi etal. â€œIn-Datacenter Performance Analysis of a Tensor Processing Unitâ€ [ISCAâ€™17]